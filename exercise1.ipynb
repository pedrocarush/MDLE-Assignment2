{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType, DoubleType, IntegerType, StructField, StructType\n",
    "from itertools import combinations\n",
    "from typing import Iterable, Any, List, Set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 19:09:52 WARN Utils: Your hostname, martinho-MS-7B86 resolves to a loopback address: 127.0.1.1; using 192.168.1.67 instead (on interface enp34s0)\n",
      "23/04/28 19:09:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/28 19:09:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('exercise1') \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/28 19:09:57 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "|track_id|album-comments| album-date_created|album-date_released|album-engineer|album-favorites|album-id|   album-information|album-listens|      album-producer|album-tags|         album-title|album-tracks|      album-type|artist-active_year_begin|artist-active_year_end|artist-associated_labels|          artist-bio|artist-comments|artist-date_created|artist-favorites|artist-id|artist-latitude|   artist-location|artist-longitude|      artist-members|         artist-name|artist-related_projects|         artist-tags|      artist-website|artist-wikipedia_page|set-split|set-subset|track-bit_rate|track-comments|track-composer| track-date_created|track-date_recorded|track-duration|track-favorites|track-genre_top|track-genres| track-genres_all|   track-information|track-interest|track-language_code|       track-license|track-listens|track-lyricist|track-number|track-publisher|track-tags|         track-title|\n",
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "|       2|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|     small|        256000|             0|          null|2008-11-26 01:48:12|2008-11-26 00:00:00|           168|              2|        Hip-Hop|        [21]|             [21]|                null|          4656|                 en|Attribution-NonCo...|         1293|          null|           3|           null|        []|                Food|\n",
      "|       3|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|    medium|        256000|             0|          null|2008-11-26 01:48:14|2008-11-26 00:00:00|           237|              1|        Hip-Hop|        [21]|             [21]|                null|          1470|                 en|Attribution-NonCo...|          514|          null|           4|           null|        []|        Electric Ave|\n",
      "|       5|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|     small|        256000|             0|          null|2008-11-26 01:48:20|2008-11-26 00:00:00|           206|              6|        Hip-Hop|        [21]|             [21]|                null|          1933|                 en|Attribution-NonCo...|         1151|          null|           6|           null|        []|          This World|\n",
      "|      10|             0|2008-11-26 01:45:08|2008-02-06 00:00:00|          null|              4|       6|                null|        47632|                null|        []|   Constant Hitmaker|           2|           Album|                    null|                  null|    Mexican Summer, R...|<p><span style=\"f...|              3|2008-11-26 01:42:55|              74|        6|           null|              null|            null|Kurt Vile, the Vi...|           Kurt Vile|                   null|['philly', 'kurt ...| http://kurtvile.com|                 null| training|     small|        192000|             0|     Kurt Vile|2008-11-25 17:49:06|2008-11-26 00:00:00|           161|            178|            Pop|        [10]|             [10]|                null|         54881|                 en|Attribution-NonCo...|        50135|          null|           1|           null|        []|             Freeway|\n",
      "|      20|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:48:56|2008-01-01 00:00:00|           311|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           978|                 en|Attribution-NonCo...|          361|          null|           3|           null|        []|     Spiritual Level|\n",
      "|      26|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:05|2008-01-01 00:00:00|           181|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|          1060|                 en|Attribution-NonCo...|          193|          null|           4|           null|        []| Where is your Love?|\n",
      "|      30|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:11|2008-01-01 00:00:00|           174|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           718|                 en|Attribution-NonCo...|          612|          null|           5|           null|        []|           Too Happy|\n",
      "|      46|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:53|2008-01-01 00:00:00|           104|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           252|                 en|Attribution-NonCo...|          171|          null|           8|           null|        []|            Yosemite|\n",
      "|      48|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:56|2008-01-01 00:00:00|           205|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           247|                 en|Attribution-NonCo...|          173|          null|           9|           null|        []|      Light of Light|\n",
      "|     134|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|    medium|        256000|             0|          null|2008-11-26 01:43:19|2008-11-26 00:00:00|           207|              3|        Hip-Hop|        [21]|             [21]|                null|          1126|                 en|Attribution-NonCo...|          943|          null|           5|           null|        []|        Street Music|\n",
      "|     135|             1|2008-11-26 01:49:19|2009-01-07 00:00:00|          null|              0|      58|<p>A couple of un...|         3331|                null|        []|                 mp3|           4|   Single Tracks|                    null|                  null|                    null|                null|              1|2008-11-26 01:47:07|               0|       52|           null|              null|            null|                null|            Abominog|                   null|        ['abominog']|http://myspace.co...|                 null| training|     large|        256000|             1|          null|2008-11-26 01:43:26|2008-11-26 00:00:00|           837|              0|           Rock|    [45, 58]|     [58, 12, 45]|                null|          2484|                 en|Attribution-NonCo...|         1832|          null|           0|           null|        []|        Father's Day|\n",
      "|     136|             1|2008-11-26 01:49:19|2009-01-07 00:00:00|          null|              0|      58|<p>A couple of un...|         3331|                null|        []|                 mp3|           4|   Single Tracks|                    null|                  null|                    null|                null|              1|2008-11-26 01:47:07|               0|       52|           null|              null|            null|                null|            Abominog|                   null|        ['abominog']|http://myspace.co...|                 null| training|    medium|        256000|             1|          null|2008-11-26 01:43:35|2008-11-26 00:00:00|           509|              0|           Rock|    [45, 58]|     [58, 12, 45]|                null|          1948|                 en|Attribution-NonCo...|         1498|          null|           0|           null|        []|Peel Back The Mou...|\n",
      "|     137|             1|2008-11-26 01:49:35|2006-12-01 00:00:00|          null|              2|      59|<p>Here's the pro...|         1681|                null| ['lafms']|        Live at LACE|           2|Live Performance|     1978-01-01 00:00:00|   1998-01-01 00:00:00|    Los Angeles Free ...|<p>Airway was a m...|              0|2008-11-26 01:47:22|               5|       53|     34.0522342|   Los Angeles, CA|    -118.2436849|Rick Potts, Juan ...|              Airway|   Los Angeles Free ...|          ['airway']|http://www.lafms....| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:43:42|1978-04-27 00:00:00|          1233|              2|   Experimental|     [1, 32]|      [32, 1, 38]|<p>Recorded live ...|          2559|                 en|Attribution-NonCo...|         1278|          null|           1|           null| ['lafms']|              Side A|\n",
      "|     138|             1|2008-11-26 01:49:35|2006-12-01 00:00:00|          null|              2|      59|<p>Here's the pro...|         1681|                null| ['lafms']|        Live at LACE|           2|Live Performance|     1978-01-01 00:00:00|   1998-01-01 00:00:00|    Los Angeles Free ...|<p>Airway was a m...|              0|2008-11-26 01:47:22|               5|       53|     34.0522342|   Los Angeles, CA|    -118.2436849|Rick Potts, Juan ...|              Airway|   Los Angeles Free ...|          ['airway']|http://www.lafms....| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:43:56|1978-04-27 00:00:00|          1231|              2|   Experimental|     [1, 32]|      [32, 1, 38]|<p>Recorded live ...|          1909|                 en|Attribution-NonCo...|          489|          null|           2|           null| ['lafms']|              Side B|\n",
      "|     139|             0|2008-11-26 01:49:57|2009-01-16 00:00:00|          null|              1|      60|<p>A full ensambl...|         1304|                null|        []|Every Man For Him...|           2|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|    medium|        128000|             0|          null|2008-11-26 01:44:05|2008-11-26 00:00:00|           296|              3|           Folk|        [17]|             [17]|                null|           702|                 en|Attribution-Nonco...|          582|          null|           2|           null|        []|            CandyAss|\n",
      "|     140|             1|2008-11-26 01:49:59|2007-05-22 00:00:00|          null|              1|      61|<p>Alec K. Redfea...|         1300|Alec K. Refearn, ...|        []|      The Blind Spot|           1|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     small|        128000|             0|          null|2008-11-26 01:44:07|2008-11-26 00:00:00|           253|              5|           Folk|        [17]|             [17]|                null|          1593|                 en|Attribution-Nonco...|         1299|          null|           2|           null|        []|  Queen Of The Wires|\n",
      "|     141|             0|2008-11-26 01:49:57|2009-01-16 00:00:00|          null|              1|      60|<p>A full ensambl...|         1304|                null|        []|Every Man For Him...|           2|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     small|        128000|             0|          null|2008-11-26 01:44:10|2008-11-26 00:00:00|           182|              1|           Folk|        [17]|             [17]|                null|           839|                 en|Attribution-Nonco...|          725|          null|           4|           null|        []|                Ohio|\n",
      "|     142|             0|2008-11-26 01:50:03|2005-01-25 00:00:00|          null|              1|      62|<p>Recorded at So...|          845|                null|        []|      The Quiet Room|           1|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     large|        128000|             0|          null|2008-11-26 01:44:11|2008-11-26 00:00:00|           470|              6|           Folk|        [17]|             [17]|                null|          1223|                 en|Attribution-Nonco...|          848|          null|           5|           null|        []|Punjabi Watery Grave|\n",
      "|     144|             0|2008-11-26 01:50:07|2009-01-06 00:00:00|          null|              0|      64|<p><em>A</em>ltho...|         2014|        Tom Buckland|        []|          Amoebiasis|           0|           Album|     1992-01-01 00:00:00|   1999-01-01 00:00:00|                    null|<p>The obscure, b...|              1|2008-11-26 01:47:54|               7|       56|     41.8239891|    Providence, RI|     -71.4128343|Alec K. Redfearn\\...|    Amoebic Ensemble|             Septimania|['providence', 'a...|http://www.myspac...| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:44:15|1998-11-26 00:00:00|            82|              1|           Jazz|         [4]|              [4]|                null|          1146|                 en|Attribution-Nonco...|         1143|          null|           1|           null|        []|             Wire Up|\n",
      "|     145|             0|2008-11-26 01:50:07|2009-01-06 00:00:00|          null|              0|      64|<p><em>A</em>ltho...|         2014|        Tom Buckland|        []|          Amoebiasis|           0|           Album|     1992-01-01 00:00:00|   1999-01-01 00:00:00|                    null|<p>The obscure, b...|              1|2008-11-26 01:47:54|               7|       56|     41.8239891|    Providence, RI|     -71.4128343|Alec K. Redfearn\\...|    Amoebic Ensemble|             Septimania|['providence', 'a...|http://www.myspac...| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:44:18|1998-11-26 00:00:00|           326|              1|           Jazz|         [4]|              [4]|                null|           968|                 en|Attribution-Nonco...|          883|          null|           3|           null|        []|          Amoebiasis|\n",
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tracks_df = (spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"quote\", '\"')\n",
    "    .option(\"escape\", '\"')\n",
    "    .csv('data/tracks.csv')\n",
    ")\n",
    "\n",
    "# rename columns with row values from first row to second row\n",
    "column_categories = list(zip(*tracks_df.take(2)))\n",
    "columns = tracks_df.columns\n",
    "tracks_df = tracks_df.select(F.col(columns[0]).alias('track_id'),\n",
    "    *(F.col(column).alias(\"-\".join(map(str, categories)))\n",
    "    for column, categories in zip(columns[1:], column_categories[1:]))\n",
    ")\n",
    "\n",
    "tracks_df = (tracks_df\n",
    "    .filter(F.col(\"track_id\").isNotNull()) \n",
    "    .filter(F.col(\"track_id\") != \"track_id\")\n",
    ")\n",
    "\n",
    "tracks_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: feature selection? not needed necessarily but...\n",
    "features_df = (spark.read\n",
    "    .csv('data/features.csv')\n",
    ")\n",
    "\n",
    "# rename columns with row values from first row to second row\n",
    "column_categories = list(zip(*features_df.take(3)))\n",
    "columns = features_df.columns\n",
    "features_df = features_df.select(F.col(columns[0]).alias('track_id'),\n",
    "    *(F.col(column).cast(DoubleType()).alias(\"-\".join(map(str, categories)))\n",
    "    for column, categories in zip(columns[1:], column_categories[1:]))\n",
    ")\n",
    "\n",
    "features_df = (features_df\n",
    "    .filter(F.col(\"track_id\") != \"feature\")\n",
    "    .filter(F.col(\"track_id\") != \"statistics\")\n",
    "    .filter(F.col(\"track_id\") != \"number\")\n",
    "    .filter(F.col(\"track_id\") != \"track_id\")\n",
    ")\n",
    "\n",
    "features_music_columns = features_df.columns.copy()\n",
    "features_music_columns.remove('track_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative clustering (in-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_subset_name = \"small\"\n",
    "\n",
    "small_tracks_df = tracks_df.filter(F.col(\"set-subset\") == dataset_subset_name)\n",
    "small_features_df = (features_df\n",
    "    .join(small_tracks_df, \"track_id\", \"left\")\n",
    "    .filter(F.col(\"set-subset\").isNotNull())\n",
    "    .select(features_df.columns)\n",
    ")\n",
    "\n",
    "small_features_pd = small_features_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the metrics (radius, diameter, density_r, density_d) for each cluster\n",
    "def calculate_metrics(pd_df, centroids):\n",
    "    \n",
    "    cluster = pd_df[\"cluster\"].values[0]\n",
    "    metrics = pd.DataFrame({'radius': [0], 'diameter': [0],'density_r': [0],'density_d': [0]}, columns=['radius', 'diameter','density_r','density_d'])\n",
    "    centroid = centroids[cluster].reshape(1,-1)\n",
    "\n",
    "    matrix = pd_df.drop(columns=[\"cluster\", \"track_id\"]).to_numpy()\n",
    "    \n",
    "    matrix_radius = np.sqrt(np.sum((matrix - centroid)**2, axis=1))\n",
    "    metrics.loc[0,'radius'] = np.max(matrix_radius)\n",
    "    # calculate density with radius\n",
    "    metrics.loc[0,'density_r'] = len(pd_df) / metrics.loc[0,'radius']**2\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        matrix_diameter = np.sqrt(np.sum((matrix[i:,:] - matrix[i,:])**2, axis=1))\n",
    "\n",
    "        max_diameter = np.max(matrix_diameter)\n",
    "        if max_diameter > metrics.loc[0,'diameter']:\n",
    "            metrics.loc[0,'diameter'] = max_diameter\n",
    "        \n",
    "    # calculate density with diameter\n",
    "    metrics.loc[0,'density_d'] = len(pd_df) / metrics.loc[0,'diameter']**2\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "import numpy.typing as npt\n",
    "\n",
    "n_clusters_range = range(8, 17)\n",
    "\n",
    "metrics_pd_array = []\n",
    "\n",
    "small_features_pd = pd.concat(objs=[small_features_pd, pd.DataFrame(data=np.zeros((len(small_features_pd), 1), dtype=np.int32), columns=[\"cluster\"])], axis=1)\n",
    "\n",
    "def cluster_agglomeratively(data: pd.DataFrame, n_clusters: int) -> npt.NDArray[np.float32]:\n",
    "    if \"cluster\" not in data.columns:\n",
    "        raise ValueError(\"The 'cluster' column should be present in the dataframe!\")\n",
    "    \n",
    "    data_features_only = data.drop(columns=[\"cluster\", \"track_id\"])\n",
    "\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    clusterer.fit(data_features_only)\n",
    "    \n",
    "    centroid_calculator = NearestCentroid()\n",
    "    centroid_calculator.fit(data_features_only, clusterer.labels_)\n",
    "\n",
    "    data[\"cluster\"] = clusterer.labels_\n",
    "    return centroid_calculator.centroids_\n",
    "\n",
    "if os.path.exists(\"./results/metrics_pd_array_pickle.pkl\"):\n",
    "    with open(\"./results/metrics_pd_array_pickle.pkl\",'rb') as f:\n",
    "        metrics_pd_array = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    # i = 8 until 16\n",
    "    for n_clusters in n_clusters_range:\n",
    "        print(f\"n_clusters: {n_clusters}\")\n",
    "        centroids = cluster_agglomeratively(small_features_pd, n_clusters)\n",
    "        metrics_pd_array.append(small_features_pd.groupby(\"cluster\").apply(calculate_metrics, centroids))\n",
    "\n",
    "    # TODO: weird column at 0s without name??\n",
    "    with open(\"./results/metrics_pd_array_pickle.pkl\",'wb') as f:\n",
    "        pickle.dump(metrics_pd_array, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the different number of clusters $k$, we printed the average and variance of each of the calculated density metrics, since it's those we are concerned with when assessing the quality of the clustering.\n",
    "\n",
    "More specifically, we are looking for high values for the average density and low values for the average variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average density_r: 7.962110575648213e-05, Average density_d: 3.599342497079214e-05, Variance density_r: 7.554281742039276e-09, Variance density_d: 1.632545349816937e-09\n",
      "\n",
      "\n",
      "Average density_r: 8.46827574451412e-05, Average density_d: 3.664885346147574e-05, Variance density_r: 7.747059677670204e-09, Variance density_d: 1.6211171070541367e-09\n",
      "\n",
      "\n",
      "Average density_r: 8.164132830338627e-05, Average density_d: 3.4647549186503005e-05, Variance density_r: 7.052526954162877e-09, Variance density_d: 1.4927145547250588e-09\n",
      "\n",
      "\n",
      "Average density_r: 7.530009483773654e-05, Average density_d: 3.330844689045004e-05, Variance density_r: 5.65147072510877e-09, Variance density_d: 1.2235780221208591e-09\n",
      "\n",
      "\n",
      "Average density_r: 7.0833141687739e-05, Average density_d: 3.124377322263076e-05, Variance density_r: 5.389143411471879e-09, Variance density_d: 1.1658465305734218e-09\n",
      "\n",
      "\n",
      "Average density_r: 6.755260440629955e-05, Average density_d: 2.980317784207617e-05, Variance density_r: 5.035928633263812e-09, Variance density_d: 1.0937616151746232e-09\n",
      "\n",
      "\n",
      "Average density_r: 6.402561239482578e-05, Average density_d: 2.8265457642839695e-05, Variance density_r: 4.831931244307105e-09, Variance density_d: 1.0455839193413751e-09\n",
      "\n",
      "\n",
      "Average density_r: 6.715931272807029e-05, Average density_d: 2.849664609437203e-05, Variance density_r: 4.608068657771316e-09, Variance density_d: 9.499376042002478e-10\n",
      "\n",
      "\n",
      "Average density_r: 6.424857597589042e-05, Average density_d: 2.760008361725085e-05, Variance density_r: 4.389601516147071e-09, Variance density_d: 9.001501276506571e-10\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in metrics_pd_array:\n",
    "    density_r_average = i[\"density_r\"].mean()\n",
    "    density_d_average = i[\"density_d\"].mean()\n",
    "    density_r_variance = i[\"density_r\"].var()\n",
    "    density_d_variance = i[\"density_d\"].var()\n",
    "\n",
    "    print(f\"Average density_r: {density_r_average}, Average density_d: {density_d_average}, Variance density_r: {density_r_variance}, Variance density_d: {density_d_variance}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plotted these averages on a heatmap plot for easier visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_heatmap = np.array([\n",
    "    [\n",
    "        m[\"density_r\"].mean(),\n",
    "        m[\"density_d\"].mean(),\n",
    "        m[\"density_r\"].var(),\n",
    "        m[\"density_d\"].var(),\n",
    "    ]\n",
    "    for m in metrics_pd_array\n",
    "])\n",
    "\n",
    "metrics_heatmap_standardized = (metrics_heatmap - metrics_heatmap.mean(axis=0)) / metrics_heatmap.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_helpers import heatmap, annotate_heatmap\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "im, cbar = heatmap(\n",
    "    data=metrics_heatmap_standardized.T,\n",
    "    row_labels=[\"avg. density ($r^2$)\", \"avg. density ($d^2$)\", \"var. density ($r^2$)\", \"var. density ($d^2$)\"],\n",
    "    col_labels=list(map(str, n_clusters_range)),\n",
    "    cbarlabel=\"Z-score\",\n",
    "    cbar_kw={\"location\": \"bottom\"},\n",
    "    cmap=\"cool\",\n",
    "    ax=ax)\n",
    "texts = annotate_heatmap(im, valfmt=\"{x:.1f}\")\n",
    "\n",
    "ax.set_title(\"Standardized metrics for each of the cluster configurations\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Será que k=11 não deve ser melhor?*\n",
    "\n",
    "From these results, we decided to choose $k=9$ since it provided by far the greatest values for the average density.\n",
    "Although it also provided a lot of variance, we notice this trend of high/low average density being related with high/low average variance for the other configurations as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFR Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: results can be: density of clusters, number of nodes in each cluster, etc., but not strictly necessary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "n_clusters = 9\n",
    "\n",
    "max_memory_used_bytes = int(.1e9)\n",
    "# Assumes all columns are doubles/longs, therefore 8 bytes\n",
    "rows_per_iteration = max_memory_used_bytes // (8 * len(features_df.columns))\n",
    "\n",
    "seed_random = 0\n",
    "\n",
    "# Threshold in terms of standard deviations away from centroid, in each dimension\n",
    "cluster_distance_threshold_standard_deviations = 1\n",
    "cluster_distance_threshold = ((cluster_distance_threshold_standard_deviations**2) * len(features_music_columns)) ** 0.5\n",
    "\n",
    "# TODO: choose threshold and justify decision\n",
    "compression_set_merge_variance_threshold = 1.001\n",
    "\n",
    "# TODO: choose threshold and justify decision\n",
    "#? distance threshold for inserting point in a compressed set???\n",
    "dbscan_eps = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_centroids = cluster_agglomeratively(small_features_pd, n_clusters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass(init=False)\n",
    "class SummarizedCluster:\n",
    "    n:      int                     \n",
    "    sum_:   npt.NDArray[np.float64]\n",
    "    sumsq_: npt.NDArray[np.float64]\n",
    "    id_:    Union[int, None]\n",
    "    tracks: Set[int]\n",
    "\n",
    "    def __init__(self, dimensions: int, id_: int=None):\n",
    "        self.n = 0\n",
    "        self.sum_ = np.zeros((dimensions,), dtype=np.float64)\n",
    "        self.sumsq_ = np.zeros((dimensions,), dtype=np.float64)\n",
    "        self.id_ = id_\n",
    "        self.tracks = set()\n",
    "    \n",
    "    def summarize(self, point: npt.NDArray[np.float64], track_id: int):\n",
    "        self.n += 1\n",
    "        self.sum_ += point\n",
    "        self.sumsq_ += point**2\n",
    "        self.tracks.add(track_id)\n",
    "    \n",
    "    def summarize_points(self, points: npt.NDArray[np.float64], track_ids: Set[int]):\n",
    "        self.n += points.shape[0]\n",
    "        self.sum_ += np.sum(points, axis=0)\n",
    "        self.sumsq_ += np.sum(points**2, axis=0)\n",
    "        self.tracks |= track_ids\n",
    "        self.test = points\n",
    "\n",
    "    def centroid(self) -> npt.NDArray[np.float64]:\n",
    "        return self.sum_ / self.n\n",
    "\n",
    "    def variance(self) -> npt.NDArray[np.float64]:\n",
    "        return (self.sumsq_ / self.n) - (self.sum_ / self.n)**2\n",
    "\n",
    "    def standard_deviation(self) -> npt.NDArray[np.float64]:\n",
    "        return np.sqrt(self.variance())\n",
    "\n",
    "    def __add__(self, other: 'SummarizedCluster') -> 'SummarizedCluster':\n",
    "        if not isinstance(other, SummarizedCluster):\n",
    "            raise ValueError(f\"Addition is not supported between a SummarizedCluster and a '{type(other)}'.\")\n",
    "        if self.id_ is not None and other.id_ is not None and self.id_ != self.other:\n",
    "            raise ValueError(f\"Clusters {self} and {other} have different explicit ids ({self.id_} != {other.id_}).\")\n",
    "        res = SummarizedCluster(self.sum_.size, self.id_ if self.id_ is not None else other.id_)\n",
    "        res.n = self.n + other.n\n",
    "        res.sum_ = self.sum_ + other.sum_\n",
    "        res.sumsq_ = self.sumsq_ + other.sumsq_\n",
    "        res.tracks = self.tracks | other.tracks\n",
    "        return res\n",
    "\n",
    "    def __iadd__(self, other: 'SummarizedCluster') -> 'SummarizedCluster':\n",
    "        if not isinstance(other, SummarizedCluster):\n",
    "            raise ValueError(f\"Addition is not supported between a SummarizedCluster and a '{type(other)}'.\")\n",
    "        if self.id_ is not None and other.id_ is not None and self.id_ != self.other:\n",
    "            raise ValueError(f\"Clusters {self} and {other} have different explicit ids ({self.id_} != {other.id_}).\")\n",
    "        self.n = self.n + other.n\n",
    "        self.sum_ = self.sum_ + other.sum_\n",
    "        self.sumsq_ = self.sumsq_ + other.sumsq_\n",
    "        self.tracks = self.tracks | other.tracks\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_sets: List[SummarizedCluster] = [SummarizedCluster(len(features_music_columns), id_) for id_ in range(n_clusters)]\n",
    "compression_sets: List[SummarizedCluster] = []\n",
    "retained_set: pd.DataFrame = pd.DataFrame(data=[], columns=features_music_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize clusters with the clustering of the small dataset subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_cluster_df(cluster_df: pd.DataFrame) -> None:\n",
    "    cluster_id = cluster_df[\"cluster\"].values[0]\n",
    "    cluster_features_mtx = cluster_df.drop(columns=[\"cluster\", \"track_id\"]).to_numpy()\n",
    "    \n",
    "    track_ids = set(cluster_df[\"track_id\"].values)\n",
    "\n",
    "    discard_set = discard_sets[cluster_id]\n",
    "    discard_set.summarize_points(cluster_features_mtx, track_ids)\n",
    "\n",
    "small_features_pd.groupby(\"cluster\").apply(summarize_cluster_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(k_centroids) == n_clusters, \"The number of clusters does not coincide with the number of random centroids!\"\n",
    "\n",
    "def mahalanobis_distance_pd(x: pd.DataFrame, s: SummarizedCluster) -> pd.Series:\n",
    "    return (((x - s.centroid()) / s.standard_deviation())**2).sum(axis=1) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_without_small_df = (features_df\n",
    "    .join(small_tracks_df, \"track_id\", \"left\")\n",
    "    .filter(F.col(\"set-subset\").isNull())\n",
    "    .select(features_df.columns)\n",
    ")\n",
    "\n",
    "total_rows = features_without_small_df.count()\n",
    "split_weights = [1.0] * (1 + (total_rows // rows_per_iteration))\n",
    "\n",
    "split_dfs = features_without_small_df.randomSplit(split_weights, seed=seed_random)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign discard set id to each point in a set of loaded points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_discard_sets(loaded_points_pd: pd.DataFrame):\n",
    "    prefix = \"cluster_distance_\"\n",
    "    cluster_distance_columns = [f\"{prefix}{i}\" for i in range(n_clusters)]\n",
    "\n",
    "    loaded_points_pd = pd.concat(\n",
    "        objs=[loaded_points_pd, pd.DataFrame(\n",
    "            data=np.zeros((loaded_points_pd.shape[0], len(cluster_distance_columns) + 2)),\n",
    "            columns=cluster_distance_columns + ['min_cluster_distance', 'cluster_id'])],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    for i, discard_set in enumerate(discard_sets):\n",
    "        loaded_points_pd[f\"cluster_distance_{i}\"] = mahalanobis_distance_pd(loaded_points_pd.iloc[:, 1:519], discard_set)\n",
    "    loaded_points_pd[\"min_cluster_distance\"] = loaded_points_pd[cluster_distance_columns].min(axis=1)\n",
    "    loaded_points_pd[\"cluster_id\"] = loaded_points_pd[cluster_distance_columns].idxmin(axis=1).str.slice(start=len(prefix)).astype(np.int32)\n",
    "\n",
    "    loaded_points_pd.drop(columns=cluster_distance_columns, inplace=True)\n",
    "\n",
    "    # Don't consider the points that surpass the threshold for the discard sets\n",
    "    loaded_points_pd.loc[loaded_points_pd[\"min_cluster_distance\"] >= cluster_distance_threshold, \"cluster_id\"] = -1\n",
    "    loaded_points_pd.drop(columns=[\"min_cluster_distance\"], inplace=True)\n",
    "\n",
    "    return loaded_points_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.000000%] Finished one of the splits (compression sets: 10, retained set: 1764)                               \r"
     ]
    }
   ],
   "source": [
    "def print_progress(progress: float, message: str):\n",
    "    print(f\"[{progress:2%}] {message:100}\", end=\"\\r\")\n",
    "\n",
    "print_progress(0, \"Initialized BFR\")\n",
    "\n",
    "# TODO: parallelize with Spark (groupyby + apply), arranjar forma de poderem mexer no mesmo discard set (por exemplo cada um ter a sua cópia e depois juntar tudo, (em termos de resultados pode vir a ser pior ????)\n",
    "for split_idx, loaded_points_df in enumerate(split_dfs):\n",
    "    progress = split_idx / len(split_weights)\n",
    "    \n",
    "    print_progress(progress, \"Collecting split into memory...\")\n",
    "    loaded_points_pd = loaded_points_df.toPandas()\n",
    "\n",
    "    print_progress(progress, \"Clustering with the Mahalanobis distance...\")  \n",
    "    loaded_points_pd = assign_discard_sets(loaded_points_pd)\n",
    "    \n",
    "    print_progress(progress, \"Calculated and collected Mahalanobis distances\")\n",
    "\n",
    "    for cluster_id, cluster_df in loaded_points_pd.groupby(\"cluster_id\"):\n",
    "        track_ids = cluster_df[\"track_id\"]\n",
    "        features_list = cluster_df.iloc[:, 1:519]\n",
    "\n",
    "        print_progress(progress, f\"Evaluating cluster {cluster_id}...\")\n",
    "\n",
    "        # Step 3 - check which points go to the discard sets\n",
    "        if cluster_id != -1:\n",
    "            discard_set = discard_sets[cluster_id]\n",
    "            discard_set.summarize_points(features_list, set(track_ids))\n",
    "        \n",
    "        # Step 4 - check which points go to the compression sets or the retained set\n",
    "        else:\n",
    "            matrix_to_cluster = pd.concat(objs=[features_list, retained_set], axis=0)\n",
    "\n",
    "            # Use same distance as above\n",
    "            clusterer = DBSCAN(eps=dbscan_eps, metric='euclidean')\n",
    "            clusterer.fit(matrix_to_cluster)\n",
    "\n",
    "            retained_set = matrix_to_cluster[clusterer.labels_ == -1]\n",
    "\n",
    "            mini_clusters = set(clusterer.labels_) - {-1}\n",
    "\n",
    "            # Create compression sets\n",
    "            compression_sets_temp = [SummarizedCluster(len(features_music_columns), None) for _ in mini_clusters]\n",
    "            for mini_cluster_id in mini_clusters:\n",
    "                compression_sets_temp[mini_cluster_id].summarize_points(matrix_to_cluster[clusterer.labels_ == mini_cluster_id], set(track_ids))\n",
    "            \n",
    "            compression_sets.extend(compression_sets_temp)\n",
    "        \n",
    "    print_progress(progress, f\"Finished evaluating clusters (compression sets: {len(compression_sets)}, retained set: {len(retained_set)})\")\n",
    "\n",
    "    # Step 5 - merge compression sets\n",
    "    compressing = True\n",
    "    while compressing:\n",
    "        compressing = False\n",
    "        merged_compression_sets = []\n",
    "        compression_set_idxs_to_remove = set()\n",
    "\n",
    "        for (idx_1, compression_set_1), (idx_2, compression_set_2) in combinations(enumerate(compression_sets), 2):\n",
    "            if idx_1 in compression_set_idxs_to_remove or idx_2 in compression_set_idxs_to_remove:\n",
    "                continue\n",
    "\n",
    "            merged_compression_set = compression_set_1 + compression_set_2\n",
    "            if merged_compression_set.variance().mean() < compression_set_merge_variance_threshold * max(compression_set_1.variance().mean(), compression_set_2.variance().mean()):\n",
    "                merged_compression_sets.append(merged_compression_set)\n",
    "                compression_set_idxs_to_remove.add(idx_1)\n",
    "                compression_set_idxs_to_remove.add(idx_2)\n",
    "                compressing = True\n",
    "        \n",
    "        compression_sets: List[SummarizedCluster] = [cs for i, cs in enumerate(compression_sets) if i not in compression_set_idxs_to_remove]\n",
    "        compression_sets.extend(merged_compression_sets)\n",
    "\n",
    "        print_progress(progress, f\"Completed one compression (compression sets: {len(compression_sets)})\")\n",
    "    \n",
    "    print_progress((split_idx + 1) / len(split_weights), f\"Finished one of the splits (compression sets: {len(compression_sets)}, retained set: {len(retained_set)})\")\n",
    "\n",
    "# Step 6 - merge CS into DS, leave RS out for further analysis\n",
    "compression_sets_closest_discard = [\n",
    "    (cs, min(((ds, np.sqrt(np.sum(np.square(cs.centroid() - ds.centroid())))) for ds in discard_sets), key=lambda t: t[1])[0])\n",
    "    for cs in compression_sets\n",
    "]\n",
    "\n",
    "for compression_set, discard_set in compression_sets_closest_discard:\n",
    "    discard_set += compression_set\n",
    "\n",
    "compression_sets.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFR cluster visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignment_df = spark.createDataFrame(\n",
    "    data=[(track, ds.id_) for ds in discard_sets for track in ds.tracks],\n",
    "    schema=StructType([StructField('track_id', StringType(), False), StructField('cluster_id', IntegerType(), False)])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids = [None] + list(range(n_clusters))\n",
    "\n",
    "genres = [row[\"track-genre_top\"] for row in tracks_df.select(\"track-genre_top\").fillna('null', 'track-genre_top').distinct().collect()]\n",
    "\n",
    "genre_counts_for_each_discard_set = {\n",
    "    row['cluster_id']:row['genre_counts']\n",
    "    for row in (tracks_df\n",
    "        .join(cluster_assignment_df, on='track_id', how='left')\n",
    "        .select('track_id', 'cluster_id', 'track-genre_top')\n",
    "        .groupby('cluster_id', 'track-genre_top')\n",
    "        .agg(F.count('track_id').alias('count'))\n",
    "        .fillna('null', 'track-genre_top')\n",
    "        .groupby('cluster_id')\n",
    "        .agg(F.map_from_arrays(F.collect_list('track-genre_top'), F.collect_list('count')).alias('genre_counts'))\n",
    "    ).collect()\n",
    "}\n",
    "\n",
    "genre_counts = {\n",
    "    genre:np.array([(genre_counts_for_each_discard_set[cluster_id][genre] if genre in genre_counts_for_each_discard_set[cluster_id] else 0) for cluster_id in cluster_ids])\n",
    "    for genre in genres\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 0.5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "bottom = np.zeros(n_clusters + 1) # include the outliers\n",
    "\n",
    "cluster_ids_xs = [-1] + list(range(n_clusters))\n",
    "\n",
    "for genre in genres:\n",
    "    p = ax.bar(cluster_ids_xs, genre_counts[genre], width, label=genre, bottom=bottom, color='dimgray' if genre == 'null' else None)\n",
    "    bottom += genre_counts[genre]\n",
    "\n",
    "ax.set_title(\"Number of tracks per genre on each cluster\")\n",
    "ax.legend(loc=\"upper right\", fontsize=10)\n",
    "ax.set_xlabel(\"Cluster\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xticks(ticks=cluster_ids_xs, labels=map(str, cluster_ids))\n",
    "\n",
    "float_to_fname = lambda f: str(f).replace('.', '-')\n",
    "plt.savefig(f'./results/graphs/clustering_c{n_clusters}_std{float_to_fname(cluster_distance_threshold_standard_deviations)}_mvt{float_to_fname(compression_set_merge_variance_threshold)}_eps{float_to_fname(dbscan_eps)}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
