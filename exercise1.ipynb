{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType, DoubleType\n",
    "from itertools import combinations\n",
    "from typing import Iterable, Any, List, Set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 16:10:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('exercise1') \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/22 16:10:42 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "|track_id|album-comments| album-date_created|album-date_released|album-engineer|album-favorites|album-id|   album-information|album-listens|      album-producer|album-tags|         album-title|album-tracks|      album-type|artist-active_year_begin|artist-active_year_end|artist-associated_labels|          artist-bio|artist-comments|artist-date_created|artist-favorites|artist-id|artist-latitude|   artist-location|artist-longitude|      artist-members|         artist-name|artist-related_projects|         artist-tags|      artist-website|artist-wikipedia_page|set-split|set-subset|track-bit_rate|track-comments|track-composer| track-date_created|track-date_recorded|track-duration|track-favorites|track-genre_top|track-genres| track-genres_all|   track-information|track-interest|track-language_code|       track-license|track-listens|track-lyricist|track-number|track-publisher|track-tags|         track-title|\n",
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "|       2|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|     small|        256000|             0|          null|2008-11-26 01:48:12|2008-11-26 00:00:00|           168|              2|        Hip-Hop|        [21]|             [21]|                null|          4656|                 en|Attribution-NonCo...|         1293|          null|           3|           null|        []|                Food|\n",
      "|       3|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|    medium|        256000|             0|          null|2008-11-26 01:48:14|2008-11-26 00:00:00|           237|              1|        Hip-Hop|        [21]|             [21]|                null|          1470|                 en|Attribution-NonCo...|          514|          null|           4|           null|        []|        Electric Ave|\n",
      "|       5|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|     small|        256000|             0|          null|2008-11-26 01:48:20|2008-11-26 00:00:00|           206|              6|        Hip-Hop|        [21]|             [21]|                null|          1933|                 en|Attribution-NonCo...|         1151|          null|           6|           null|        []|          This World|\n",
      "|      10|             0|2008-11-26 01:45:08|2008-02-06 00:00:00|          null|              4|       6|                null|        47632|                null|        []|   Constant Hitmaker|           2|           Album|                    null|                  null|    Mexican Summer, R...|<p><span style=\"f...|              3|2008-11-26 01:42:55|              74|        6|           null|              null|            null|Kurt Vile, the Vi...|           Kurt Vile|                   null|['philly', 'kurt ...| http://kurtvile.com|                 null| training|     small|        192000|             0|     Kurt Vile|2008-11-25 17:49:06|2008-11-26 00:00:00|           161|            178|            Pop|        [10]|             [10]|                null|         54881|                 en|Attribution-NonCo...|        50135|          null|           1|           null|        []|             Freeway|\n",
      "|      20|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:48:56|2008-01-01 00:00:00|           311|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           978|                 en|Attribution-NonCo...|          361|          null|           3|           null|        []|     Spiritual Level|\n",
      "|      26|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:05|2008-01-01 00:00:00|           181|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|          1060|                 en|Attribution-NonCo...|          193|          null|           4|           null|        []| Where is your Love?|\n",
      "|      30|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:11|2008-01-01 00:00:00|           174|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           718|                 en|Attribution-NonCo...|          612|          null|           5|           null|        []|           Too Happy|\n",
      "|      46|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:53|2008-01-01 00:00:00|           104|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           252|                 en|Attribution-NonCo...|          171|          null|           8|           null|        []|            Yosemite|\n",
      "|      48|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:56|2008-01-01 00:00:00|           205|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           247|                 en|Attribution-NonCo...|          173|          null|           9|           null|        []|      Light of Light|\n",
      "|     134|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|    medium|        256000|             0|          null|2008-11-26 01:43:19|2008-11-26 00:00:00|           207|              3|        Hip-Hop|        [21]|             [21]|                null|          1126|                 en|Attribution-NonCo...|          943|          null|           5|           null|        []|        Street Music|\n",
      "|     135|             1|2008-11-26 01:49:19|2009-01-07 00:00:00|          null|              0|      58|<p>A couple of un...|         3331|                null|        []|                 mp3|           4|   Single Tracks|                    null|                  null|                    null|                null|              1|2008-11-26 01:47:07|               0|       52|           null|              null|            null|                null|            Abominog|                   null|        ['abominog']|http://myspace.co...|                 null| training|     large|        256000|             1|          null|2008-11-26 01:43:26|2008-11-26 00:00:00|           837|              0|           Rock|    [45, 58]|     [58, 12, 45]|                null|          2484|                 en|Attribution-NonCo...|         1832|          null|           0|           null|        []|        Father's Day|\n",
      "|     136|             1|2008-11-26 01:49:19|2009-01-07 00:00:00|          null|              0|      58|<p>A couple of un...|         3331|                null|        []|                 mp3|           4|   Single Tracks|                    null|                  null|                    null|                null|              1|2008-11-26 01:47:07|               0|       52|           null|              null|            null|                null|            Abominog|                   null|        ['abominog']|http://myspace.co...|                 null| training|    medium|        256000|             1|          null|2008-11-26 01:43:35|2008-11-26 00:00:00|           509|              0|           Rock|    [45, 58]|     [58, 12, 45]|                null|          1948|                 en|Attribution-NonCo...|         1498|          null|           0|           null|        []|Peel Back The Mou...|\n",
      "|     137|             1|2008-11-26 01:49:35|2006-12-01 00:00:00|          null|              2|      59|<p>Here's the pro...|         1681|                null| ['lafms']|        Live at LACE|           2|Live Performance|     1978-01-01 00:00:00|   1998-01-01 00:00:00|    Los Angeles Free ...|<p>Airway was a m...|              0|2008-11-26 01:47:22|               5|       53|     34.0522342|   Los Angeles, CA|    -118.2436849|Rick Potts, Juan ...|              Airway|   Los Angeles Free ...|          ['airway']|http://www.lafms....| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:43:42|1978-04-27 00:00:00|          1233|              2|   Experimental|     [1, 32]|      [32, 1, 38]|<p>Recorded live ...|          2559|                 en|Attribution-NonCo...|         1278|          null|           1|           null| ['lafms']|              Side A|\n",
      "|     138|             1|2008-11-26 01:49:35|2006-12-01 00:00:00|          null|              2|      59|<p>Here's the pro...|         1681|                null| ['lafms']|        Live at LACE|           2|Live Performance|     1978-01-01 00:00:00|   1998-01-01 00:00:00|    Los Angeles Free ...|<p>Airway was a m...|              0|2008-11-26 01:47:22|               5|       53|     34.0522342|   Los Angeles, CA|    -118.2436849|Rick Potts, Juan ...|              Airway|   Los Angeles Free ...|          ['airway']|http://www.lafms....| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:43:56|1978-04-27 00:00:00|          1231|              2|   Experimental|     [1, 32]|      [32, 1, 38]|<p>Recorded live ...|          1909|                 en|Attribution-NonCo...|          489|          null|           2|           null| ['lafms']|              Side B|\n",
      "|     139|             0|2008-11-26 01:49:57|2009-01-16 00:00:00|          null|              1|      60|<p>A full ensambl...|         1304|                null|        []|Every Man For Him...|           2|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|    medium|        128000|             0|          null|2008-11-26 01:44:05|2008-11-26 00:00:00|           296|              3|           Folk|        [17]|             [17]|                null|           702|                 en|Attribution-Nonco...|          582|          null|           2|           null|        []|            CandyAss|\n",
      "|     140|             1|2008-11-26 01:49:59|2007-05-22 00:00:00|          null|              1|      61|<p>Alec K. Redfea...|         1300|Alec K. Refearn, ...|        []|      The Blind Spot|           1|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     small|        128000|             0|          null|2008-11-26 01:44:07|2008-11-26 00:00:00|           253|              5|           Folk|        [17]|             [17]|                null|          1593|                 en|Attribution-Nonco...|         1299|          null|           2|           null|        []|  Queen Of The Wires|\n",
      "|     141|             0|2008-11-26 01:49:57|2009-01-16 00:00:00|          null|              1|      60|<p>A full ensambl...|         1304|                null|        []|Every Man For Him...|           2|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     small|        128000|             0|          null|2008-11-26 01:44:10|2008-11-26 00:00:00|           182|              1|           Folk|        [17]|             [17]|                null|           839|                 en|Attribution-Nonco...|          725|          null|           4|           null|        []|                Ohio|\n",
      "|     142|             0|2008-11-26 01:50:03|2005-01-25 00:00:00|          null|              1|      62|<p>Recorded at So...|          845|                null|        []|      The Quiet Room|           1|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     large|        128000|             0|          null|2008-11-26 01:44:11|2008-11-26 00:00:00|           470|              6|           Folk|        [17]|             [17]|                null|          1223|                 en|Attribution-Nonco...|          848|          null|           5|           null|        []|Punjabi Watery Grave|\n",
      "|     144|             0|2008-11-26 01:50:07|2009-01-06 00:00:00|          null|              0|      64|<p><em>A</em>ltho...|         2014|        Tom Buckland|        []|          Amoebiasis|           0|           Album|     1992-01-01 00:00:00|   1999-01-01 00:00:00|                    null|<p>The obscure, b...|              1|2008-11-26 01:47:54|               7|       56|     41.8239891|    Providence, RI|     -71.4128343|Alec K. Redfearn\\...|    Amoebic Ensemble|             Septimania|['providence', 'a...|http://www.myspac...| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:44:15|1998-11-26 00:00:00|            82|              1|           Jazz|         [4]|              [4]|                null|          1146|                 en|Attribution-Nonco...|         1143|          null|           1|           null|        []|             Wire Up|\n",
      "|     145|             0|2008-11-26 01:50:07|2009-01-06 00:00:00|          null|              0|      64|<p><em>A</em>ltho...|         2014|        Tom Buckland|        []|          Amoebiasis|           0|           Album|     1992-01-01 00:00:00|   1999-01-01 00:00:00|                    null|<p>The obscure, b...|              1|2008-11-26 01:47:54|               7|       56|     41.8239891|    Providence, RI|     -71.4128343|Alec K. Redfearn\\...|    Amoebic Ensemble|             Septimania|['providence', 'a...|http://www.myspac...| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:44:18|1998-11-26 00:00:00|           326|              1|           Jazz|         [4]|              [4]|                null|           968|                 en|Attribution-Nonco...|          883|          null|           3|           null|        []|          Amoebiasis|\n",
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tracks_df = (spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"quote\", '\"')\n",
    "    .option(\"escape\", '\"')\n",
    "    .csv('data/tracks.csv')\n",
    ")\n",
    "\n",
    "# rename columns with row values from first row to second row\n",
    "column_categories = list(zip(*tracks_df.take(2)))\n",
    "columns = tracks_df.columns\n",
    "tracks_df = tracks_df.select(F.col(columns[0]).alias('track_id'),\n",
    "    *(F.col(column).alias(\"-\".join(map(str, categories)))\n",
    "    for column, categories in zip(columns[1:], column_categories[1:]))\n",
    ")\n",
    "\n",
    "tracks_df = (tracks_df\n",
    "    .filter(F.col(\"track_id\").isNotNull()) \n",
    "    .filter(F.col(\"track_id\") != \"track_id\")\n",
    ")\n",
    "\n",
    "tracks_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: feature selection? not needed necessarily but...\n",
    "features_df = (spark.read\n",
    "    .csv('data/features.csv')\n",
    ")\n",
    "\n",
    "# rename columns with row values from first row to second row\n",
    "column_categories = list(zip(*features_df.take(3)))\n",
    "columns = features_df.columns\n",
    "# TODO: DoubleType instead of FloatType?\n",
    "features_df = features_df.select(F.col(columns[0]).alias('track_id'),\n",
    "    *(F.col(column).cast(DoubleType()).alias(\"-\".join(map(str, categories)))\n",
    "    for column, categories in zip(columns[1:], column_categories[1:]))\n",
    ")\n",
    "\n",
    "features_df = (features_df\n",
    "    .filter(F.col(\"track_id\") != \"feature\")\n",
    "    .filter(F.col(\"track_id\") != \"statistics\")\n",
    "    .filter(F.col(\"track_id\") != \"number\")\n",
    "    .filter(F.col(\"track_id\") != \"track_id\")\n",
    ")\n",
    "\n",
    "features_music_columns = features_df.columns.copy()\n",
    "features_music_columns.remove('track_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative clustering (in-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_subset_name = \"small\"\n",
    "\n",
    "small_tracks_df = tracks_df.filter(F.col(\"set-subset\") == dataset_subset_name)\n",
    "small_features_df = (features_df\n",
    "    .join(small_tracks_df, \"track_id\", \"left\")\n",
    "    .filter(F.col(\"set-subset\").isNotNull())\n",
    "    .select(features_df.columns)\n",
    ")\n",
    "\n",
    "small_features_pd = small_features_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the metrics (radius, diameter, density_r, density_d) for each cluster\n",
    "def calculate_metrics(pd_df, centroids):\n",
    "    \n",
    "    cluster = pd_df[\"cluster\"].values[0]\n",
    "    metrics = pd.DataFrame({'radius': [0], 'diameter': [0],'density_r': [0],'density_d': [0]}, columns=['radius', 'diameter','density_r','density_d'])\n",
    "    centroid = centroids[cluster].reshape(1,-1)\n",
    "\n",
    "    matrix = pd_df.drop(columns=[\"cluster\", \"track_id\"]).to_numpy()\n",
    "    \n",
    "    matrix_radius = np.sqrt(np.sum((matrix - centroid)**2, axis=1))\n",
    "    metrics.loc[0,'radius'] = np.max(matrix_radius)\n",
    "    # calculate density with radius\n",
    "    metrics.loc[0,'density_r'] = len(pd_df) / metrics.loc[0,'radius']**2\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        matrix_diameter = np.sqrt(np.sum((matrix[i:,:] - matrix[i,:])**2, axis=1))\n",
    "\n",
    "        max_diameter = np.max(matrix_diameter)\n",
    "        if max_diameter > metrics.loc[0,'diameter']:\n",
    "            metrics.loc[0,'diameter'] = max_diameter\n",
    "        \n",
    "    # calculate density with diameter\n",
    "    metrics.loc[0,'density_d'] = len(pd_df) / metrics.loc[0,'diameter']**2\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4186/1666919071.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  small_features_pd[\"cluster\"] = np.zeros((len(small_features_pd), 1), dtype=np.int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters: 8\n",
      "n_clusters: 9\n",
      "n_clusters: 10\n",
      "n_clusters: 11\n",
      "n_clusters: 12\n",
      "n_clusters: 13\n",
      "n_clusters: 14\n",
      "n_clusters: 15\n",
      "n_clusters: 16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "import numpy.typing as npt\n",
    "\n",
    "metrics_pd_array = []\n",
    "\n",
    "small_features_pd[\"cluster\"] = np.zeros((len(small_features_pd), 1), dtype=np.int32)\n",
    "\n",
    "def cluster_agglomeratively(data: pd.DataFrame, n_clusters: int) -> npt.NDArray[np.float32]:\n",
    "    if \"cluster\" not in data.columns:\n",
    "        raise ValueError(\"The 'cluster' column should be present in the dataframe!\")\n",
    "    \n",
    "    data_features_only = data.drop(columns=[\"cluster\", \"track_id\"])\n",
    "\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    clusterer.fit(data_features_only)\n",
    "    \n",
    "    centroid_calculator = NearestCentroid()\n",
    "    centroid_calculator.fit(data_features_only, clusterer.labels_)\n",
    "\n",
    "    data[\"cluster\"] = clusterer.labels_\n",
    "    return centroid_calculator.centroids_\n",
    "\n",
    "if os.path.exists(\"./results/metrics_pd_array_pickle.pkl\"):\n",
    "    with open(\"./results/metrics_pd_array_pickle.pkl\",'rb') as f:\n",
    "        metrics_pd_array = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    # i = 8 until 16\n",
    "    for n_clusters in range(8, 17):\n",
    "        print(f\"n_clusters: {n_clusters}\")\n",
    "        centroids = cluster_agglomeratively(small_features_pd, n_clusters)\n",
    "        metrics_pd_array.append(small_features_pd.groupby(\"cluster\").apply(calculate_metrics, centroids))\n",
    "\n",
    "    # TODO: weird column at 0s without name??\n",
    "    with open(\"./results/metrics_pd_array_pickle.pkl\",'wb') as f:\n",
    "        pickle.dump(metrics_pd_array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average density_r: 7.962116015409432e-05, Average density_d: 3.5993421010529105e-05, Variance density_r: 7.554301594511152e-09, Variance density_d: 1.6325448959405526e-09\n",
      "\n",
      "\n",
      "Average density_r: 8.468280174708501e-05, Average density_d: 3.664884862339534e-05, Variance density_r: 7.747077822050173e-09, Variance density_d: 1.6211166463782024e-09\n",
      "\n",
      "\n",
      "Average density_r: 8.164135739606747e-05, Average density_d: 3.464754500675633e-05, Variance density_r: 7.052543814624157e-09, Variance density_d: 1.4927141165628917e-09\n",
      "\n",
      "\n",
      "Average density_r: 7.530015533838525e-05, Average density_d: 3.330844178763551e-05, Variance density_r: 5.651489897428292e-09, Variance density_d: 1.2235775469777811e-09\n",
      "\n",
      "\n",
      "Average density_r: 7.083319800111053e-05, Average density_d: 3.12437683959014e-05, Variance density_r: 5.38916128542342e-09, Variance density_d: 1.1658460841988244e-09\n",
      "\n",
      "\n",
      "Average density_r: 6.755265585371375e-05, Average density_d: 2.9803173044205747e-05, Variance density_r: 5.035945257788826e-09, Variance density_d: 1.0937612033572456e-09\n",
      "\n",
      "\n",
      "Average density_r: 6.402566164660506e-05, Average density_d: 2.8265452838229816e-05, Variance density_r: 4.831946821616402e-09, Variance density_d: 1.045583537428686e-09\n",
      "\n",
      "\n",
      "Average density_r: 6.715938168255902e-05, Average density_d: 2.8496640725964807e-05, Variance density_r: 4.608083603538709e-09, Variance density_d: 9.499372285379279e-10\n",
      "\n",
      "\n",
      "Average density_r: 6.424864539260667e-05, Average density_d: 2.7600078751335965e-05, Variance density_r: 4.38961579641494e-09, Variance density_d: 9.001497688649123e-10\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: justify N-clusters choice, maybe create a colored matrix?\n",
    "for i in metrics_pd_array:\n",
    "    density_r_average = i[\"density_r\"].mean()\n",
    "    density_d_average = i[\"density_d\"].mean()\n",
    "    density_r_variance = i[\"density_r\"].var()\n",
    "    density_d_variance = i[\"density_d\"].var()\n",
    "\n",
    "    print(f\"Average density_r: {density_r_average}, Average density_d: {density_d_average}, Variance density_r: {density_r_variance}, Variance density_d: {density_d_variance}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFR Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: results can be: density of clusters, number of nodes in each cluster, etc., but not strictly necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 9\n",
    "\n",
    "max_memory_used_bytes = int(.1e9)\n",
    "# Assumes all columns are doubles/longs, and so therefore 8 bytes\n",
    "rows_per_iteration = max_memory_used_bytes // (8 * len(features_df.columns))\n",
    "\n",
    "seed_random = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_centroids = cluster_agglomeratively(small_features_pd, n_clusters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass(init=False)\n",
    "class SummarizedCluster:\n",
    "    n:      int                     \n",
    "    sum_:   npt.NDArray[np.float64]\n",
    "    sumsq_: npt.NDArray[np.float64]\n",
    "    id_:    Union[int, None]\n",
    "    tracks: Set[int]\n",
    "\n",
    "    def __init__(self, dimensions: int, id_: int=None):\n",
    "        self.n = 0\n",
    "        self.sum_ = np.zeros((dimensions,), dtype=np.float64)\n",
    "        self.sumsq_ = np.zeros((dimensions,), dtype=np.float64)\n",
    "        self.id_ = id_\n",
    "        self.tracks = set()\n",
    "    \n",
    "    def summarize(self, point: npt.NDArray[np.float64], track_id: int):\n",
    "        self.n += 1\n",
    "        self.sum_ += point\n",
    "        self.sumsq_ += point**2\n",
    "        self.tracks.add(track_id)\n",
    "    \n",
    "    def summarize_points(self, points: npt.NDArray[np.float64], track_ids: Set[int]):\n",
    "        self.n += points.shape[0]\n",
    "        self.sum_ += np.sum(points, axis=0)\n",
    "        self.sumsq_ += np.sum(points**2, axis=0)\n",
    "        self.tracks |= track_ids\n",
    "        self.test = points\n",
    "\n",
    "    def centroid(self) -> npt.NDArray[np.float64]:\n",
    "        return self.sum_ / self.n\n",
    "\n",
    "    def variance(self) -> npt.NDArray[np.float64]:\n",
    "        return (self.sumsq_ / self.n) - (self.sum_ / self.n)**2\n",
    "\n",
    "    def standard_deviation(self) -> npt.NDArray[np.float64]:\n",
    "        return np.sqrt(self.variance())\n",
    "\n",
    "    def __add__(self, other) -> 'SummarizedCluster':\n",
    "        if self.id_ is not None and self.other is not None and self.id_ != self.other:\n",
    "            raise ValueError(f\"Clusters {self} and {other} have different explicit ids ({self.id_} != {other.id_}).\")\n",
    "        res = SummarizedCluster(self.sum_.size, self.id_ if self.id_ is not None else other.id_)\n",
    "        res.n = self.n + other.n\n",
    "        res.sum_ = self.sum_ + other.sum_\n",
    "        res.sumsq_ = self.sumsq_ + other.sumsq_\n",
    "        res.tracks = self.tracks | other.tracks\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_sets: List[SummarizedCluster] = [SummarizedCluster(len(features_music_columns), id_) for id_ in range(n_clusters)]\n",
    "compression_sets: List[SummarizedCluster] = []\n",
    "retained_set: pd.DataFrame = pd.DataFrame(data=[], columns=features_music_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize clusters with the clustering of the small dataset subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_cluster_df(cluster_df: pd.DataFrame) -> None:\n",
    "    cluster_id = cluster_df[\"cluster\"].values[0]\n",
    "    cluster_features_mtx = cluster_df.drop(columns=[\"cluster\", \"track_id\"]).to_numpy()\n",
    "    \n",
    "    track_ids = set(cluster_df[\"track_id\"].values)\n",
    "\n",
    "    discard_set = discard_sets[cluster_id]\n",
    "    discard_set.summarize_points(cluster_features_mtx, track_ids)\n",
    "\n",
    "small_features_pd.groupby(\"cluster\").apply(summarize_cluster_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup functions and thresholds necessary for BFR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold in terms of standard deviations away from centroid, in each dimension\n",
    "cluster_distance_threshold_standard_deviations = 1\n",
    "cluster_distance_threshold = ((cluster_distance_threshold_standard_deviations**2) * len(features_music_columns)) ** 0.5\n",
    "\n",
    "# TODO: choose threshold and justify decision\n",
    "compression_set_merge_variance_threshold = float('-inf')\n",
    "\n",
    "dbscan_eps = 1000\n",
    "\n",
    "assert len(k_centroids) == n_clusters, \"The number of clusters does not coincide with the number of random centroids!\"\n",
    "\n",
    "# def mahalanobis_distance_np(x: npt.NDArray[np.float32], s: SummarizedCluster) -> float:\n",
    "#     return np.sqrt(np.sum(((x - s.centroid()) / s.standard_deviation())**2))\n",
    "\n",
    "def mahalanobis_distance_pd(x: pd.DataFrame, s: SummarizedCluster) -> pd.Series:\n",
    "    return (((x - s.centroid()) / s.standard_deviation())**2).sum(axis=1) ** 0.5\n",
    "\n",
    "# @F.udf(returnType=FloatType())\n",
    "# def closest_mahalanobis_distance_cluster(*features: float):\n",
    "#     x = np.array(features)\n",
    "#     closest_cluster_distance, closest_cluster = min((mahalanobis_distance_np(x, d), d.id_) for d in discard_sets)\n",
    "#     return closest_cluster if closest_cluster_distance < cluster_distance_threshold else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "features_without_small_df = (features_df\n",
    "    .join(small_tracks_df, \"track_id\", \"left\")\n",
    "    .filter(F.col(\"set-subset\").isNull())\n",
    "    .select(features_df.columns)\n",
    ")\n",
    "\n",
    "total_rows = features_without_small_df.count()\n",
    "split_weights = [1.0] * (1 + (total_rows // rows_per_iteration))\n",
    "\n",
    "split_dfs = features_without_small_df.randomSplit(split_weights, seed=seed_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.000000%] Finished one of the splits (compression sets: 60, retained set: 1762)                               \n"
     ]
    }
   ],
   "source": [
    "prefix = \"cluster_distance_\"\n",
    "cluster_distance_columns = [f\"{prefix}{i}\" for i in range(n_clusters)]\n",
    "\n",
    "def print_progress(progress: float, message: str):\n",
    "    print(f\"[{progress:3%}] {message:100}\", end=\"\\r\")\n",
    "\n",
    "print_progress(0, \"Initialized BFR\")\n",
    "\n",
    "# TODO: parallelize with Spark (groupyby + apply), arranjar forma de poderem mexer no mesmo discard set (por exemplo cada um ter a sua cópia e depois juntar tudo, (em termos de resultados pode vir a ser pior ????)\n",
    "for split_idx, loaded_points_df in enumerate(split_dfs):\n",
    "    progress = split_idx / len(split_weights)\n",
    "    \n",
    "    print_progress(progress, \"Collecting split into memory...\")\n",
    "    loaded_points_pd = loaded_points_df.toPandas()\n",
    "\n",
    "    print_progress(progress, \"Clustering with the Mahalanobis distance...\")\n",
    "    loaded_points_pd = pd.concat(\n",
    "        objs=[loaded_points_pd, pd.DataFrame(\n",
    "            data=np.zeros((loaded_points_pd.shape[0], len(cluster_distance_columns) + 2)),\n",
    "            columns=cluster_distance_columns + ['min_cluster_distance', 'cluster_id'])],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    for i, discard_set in enumerate(discard_sets):\n",
    "        loaded_points_pd[f\"cluster_distance_{i}\"] = mahalanobis_distance_pd(loaded_points_pd.iloc[:, 1:519], discard_set)\n",
    "    loaded_points_pd[\"min_cluster_distance\"] = loaded_points_pd[cluster_distance_columns].min(axis=1)\n",
    "    loaded_points_pd[\"cluster_id\"] = loaded_points_pd[cluster_distance_columns].idxmin(axis=1).str.slice(start=len(prefix)).astype(np.int32)\n",
    "\n",
    "    loaded_points_pd.drop(columns=cluster_distance_columns, inplace=True)\n",
    "\n",
    "    # Don't consider the points that surpass the threshold for the discard sets\n",
    "    loaded_points_pd.loc[loaded_points_pd[\"min_cluster_distance\"] >= cluster_distance_threshold, \"cluster_id\"] = -1\n",
    "    loaded_points_pd.drop(columns=[\"min_cluster_distance\"], inplace=True)\n",
    "\n",
    "    # cluster_mapping = loaded_points_df \\\n",
    "    #     .withColumn(\"cluster\", closest_mahalanobis_distance_cluster(*features_columns)) \\\n",
    "    #     .groupby(\"cluster\") \\\n",
    "    #     .agg({\n",
    "    #         \"track_ids\": F.collect_list(\"track_id\"),\n",
    "    #         \"features_list\": F.collect_list(F.array(*features_columns))\n",
    "    #     }).collect()\n",
    "    \n",
    "    print_progress(progress, \"Calculated and collected Mahalanobis distances\")\n",
    "\n",
    "    for cluster_id, cluster_df in loaded_points_pd.groupby(\"cluster_id\"):\n",
    "        track_ids = cluster_df[\"track_id\"]\n",
    "        features_list = cluster_df.iloc[:, 1:519]\n",
    "\n",
    "        print_progress(progress, f\"Evaluating cluster {cluster_id}...\")\n",
    "\n",
    "        # Step 3 - check which points go to the discard sets\n",
    "        if cluster_id != -1:\n",
    "            discard_set = discard_sets[cluster_id]\n",
    "            discard_set.summarize_points(features_list, set(track_ids))\n",
    "        \n",
    "        # Step 4 - check which points go to the compression sets or the retained set\n",
    "        else:\n",
    "            matrix_to_cluster = pd.concat(objs=[features_list, retained_set], axis=0)\n",
    "\n",
    "            # Use same distance as above\n",
    "            clusterer = DBSCAN(eps=dbscan_eps, metric='euclidean')\n",
    "            clusterer.fit(matrix_to_cluster)\n",
    "\n",
    "            retained_set = matrix_to_cluster[clusterer.labels_ == -1]\n",
    "\n",
    "            mini_clusters = set(clusterer.labels_) - {-1}\n",
    "\n",
    "            # Create compression sets\n",
    "            compression_sets_temp = [SummarizedCluster(len(features_music_columns), None) for _ in mini_clusters]\n",
    "            for mini_cluster_id in mini_clusters:\n",
    "                compression_sets_temp[mini_cluster_id].summarize_points(matrix_to_cluster[clusterer.labels_ == mini_cluster_id], set(track_ids))\n",
    "            \n",
    "            compression_sets.extend(compression_sets_temp)\n",
    "        \n",
    "    print_progress(progress, f\"Finished evaluating clusters (compression sets: {len(compression_sets)}, retained set: {len(retained_set)})\")\n",
    "\n",
    "    # Step 5 - merge compression sets\n",
    "    compressing = True\n",
    "    while compressing:\n",
    "        compressing = False\n",
    "        merged_compression_sets = []\n",
    "        compression_set_idxs_to_remove = set()\n",
    "\n",
    "        for (idx_1, compression_set_1), (idx_2, compression_set_2) in combinations(enumerate(compression_sets), 2):\n",
    "            if idx_1 in compression_set_idxs_to_remove or idx_2 in compression_set_idxs_to_remove:\n",
    "                continue\n",
    "\n",
    "            merged_compression_set = compression_set_1 + compression_set_2\n",
    "            if (merged_compression_set.variance() < compression_set_merge_variance_threshold).any():\n",
    "                merged_compression_sets.append(merged_compression_set)\n",
    "                compression_set_idxs_to_remove.add(idx_1)\n",
    "                compression_set_idxs_to_remove.add(idx_2)\n",
    "                compressing = True\n",
    "        \n",
    "        compression_sets = [cs for i, cs in enumerate(compression_sets) if i not in compression_set_idxs_to_remove]\n",
    "        compression_sets.extend(merged_compression_sets)\n",
    "\n",
    "        print_progress(progress, f\"Completed one compression (compression sets: {len(compression_sets)})\")\n",
    "    \n",
    "    print_progress((split_idx + 1) / len(split_weights), f\"Finished one of the splits (compression sets: {len(compression_sets)}, retained set: {len(retained_set)})\")\n",
    "\n",
    "# Step 6 - merge CS and RS into DS (but we won't yet)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFR cluster visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(track-genre_top='International'),\n",
       " Row(track-genre_top='Soul-RnB'),\n",
       " Row(track-genre_top='Instrumental'),\n",
       " Row(track-genre_top='Rock'),\n",
       " Row(track-genre_top='Jazz'),\n",
       " Row(track-genre_top=None),\n",
       " Row(track-genre_top='Folk'),\n",
       " Row(track-genre_top='Old-Time / Historic'),\n",
       " Row(track-genre_top='Classical'),\n",
       " Row(track-genre_top='Blues'),\n",
       " Row(track-genre_top='Experimental'),\n",
       " Row(track-genre_top='Spoken'),\n",
       " Row(track-genre_top='Pop'),\n",
       " Row(track-genre_top='Easy Listening'),\n",
       " Row(track-genre_top='Electronic'),\n",
       " Row(track-genre_top='Country'),\n",
       " Row(track-genre_top='Hip-Hop')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#discard_sets\n",
    "#compression_sets\n",
    "#retained_set\n",
    "\n",
    "tracks_df.select(\"track-genre_top\").distinct().collect()\n",
    "t_counts = {\n",
    "    \"Below\": np.array([70, 31, 58]),\n",
    "    \"Above\": np.array([82, 37, 66]),\n",
    "}\n",
    "width = 0.5\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(3)\n",
    "\n",
    "for boolean, weight_count in weight_counts.items():\n",
    "    p = ax.bar(specieht_count, width, label=boolean, bottom=bottom)\n",
    "    bottom += weight_count\n",
    "\n",
    "ax.set_title(\"Number of penguins with above average body mass\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [row[\"track-genre_top\"] for row in tracks_df.select(\"track-genre_top\").distinct().collect()]\n",
    "\n",
    "t_counts = {\n",
    "    \"Below\": np.array([70, 31, 58]),\n",
    "    \"Above\": np.array([82, 37, 66]),\n",
    "}\n",
    "width = 0.5\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bottom = np.zeros(3)\n",
    "\n",
    "for boolean, weight_count in weight_counts.items():\n",
    "    p = ax.bar(specieht_count, width, label=boolean, bottom=bottom)\n",
    "    bottom += weight_count\n",
    "\n",
    "ax.set_title(\"Number of penguins with above average body mass\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "genre_counts = pd.DataFrame(\n",
    "    data=[\n",
    "        tracks_df.filter(F.col(\"track_id\").isin(discard_set.tracks)).groupby(\"track-genre_top\").count()\n",
    "        for discard_set in discard_sets\n",
    "    ],\n",
    "    columns=genres\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pedro/Desktop/MEI-4ano/2semestre/MDLE/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/pedro/Desktop/MEI-4ano/2semestre/MDLE/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tracks_df\u001b[39m.\u001b[39;49mfilter(F\u001b[39m.\u001b[39;49mcol(\u001b[39m\"\u001b[39;49m\u001b[39mtrack_id\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49misin(discard_sets[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mtracks))\u001b[39m.\u001b[39;49mgroupby(\u001b[39m\"\u001b[39;49m\u001b[39mtrack-genre_top\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mcount()\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[0;32m~/Desktop/MEI-4ano/2semestre/MDLE/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[1;32m    607\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/MEI-4ano/2semestre/MDLE/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Desktop/MEI-4ano/2semestre/MDLE/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/Desktop/MEI-4ano/2semestre/MDLE/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracks_df.filter(F.col(\"track_id\").isin(discard_sets[1].tracks)).groupby(\"track-genre_top\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  c\n",
       "0  1  2  3\n",
       "1  4  5  6\n",
       "2  7  8  9"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=[[1,2,3],[4,5,6],[7,8,9]], columns=['a','b','c'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
