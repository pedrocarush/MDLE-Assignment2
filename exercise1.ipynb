{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType, DoubleType\n",
    "from itertools import combinations\n",
    "from typing import Iterable, Any, List, Set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('exercise1') \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "|track_id|album-comments| album-date_created|album-date_released|album-engineer|album-favorites|album-id|   album-information|album-listens|      album-producer|album-tags|         album-title|album-tracks|      album-type|artist-active_year_begin|artist-active_year_end|artist-associated_labels|          artist-bio|artist-comments|artist-date_created|artist-favorites|artist-id|artist-latitude|   artist-location|artist-longitude|      artist-members|         artist-name|artist-related_projects|         artist-tags|      artist-website|artist-wikipedia_page|set-split|set-subset|track-bit_rate|track-comments|track-composer| track-date_created|track-date_recorded|track-duration|track-favorites|track-genre_top|track-genres| track-genres_all|   track-information|track-interest|track-language_code|       track-license|track-listens|track-lyricist|track-number|track-publisher|track-tags|         track-title|\n",
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "|       2|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|     small|        256000|             0|          null|2008-11-26 01:48:12|2008-11-26 00:00:00|           168|              2|        Hip-Hop|        [21]|             [21]|                null|          4656|                 en|Attribution-NonCo...|         1293|          null|           3|           null|        []|                Food|\n",
      "|       3|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|    medium|        256000|             0|          null|2008-11-26 01:48:14|2008-11-26 00:00:00|           237|              1|        Hip-Hop|        [21]|             [21]|                null|          1470|                 en|Attribution-NonCo...|          514|          null|           4|           null|        []|        Electric Ave|\n",
      "|       5|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|     small|        256000|             0|          null|2008-11-26 01:48:20|2008-11-26 00:00:00|           206|              6|        Hip-Hop|        [21]|             [21]|                null|          1933|                 en|Attribution-NonCo...|         1151|          null|           6|           null|        []|          This World|\n",
      "|      10|             0|2008-11-26 01:45:08|2008-02-06 00:00:00|          null|              4|       6|                null|        47632|                null|        []|   Constant Hitmaker|           2|           Album|                    null|                  null|    Mexican Summer, R...|<p><span style=\"f...|              3|2008-11-26 01:42:55|              74|        6|           null|              null|            null|Kurt Vile, the Vi...|           Kurt Vile|                   null|['philly', 'kurt ...| http://kurtvile.com|                 null| training|     small|        192000|             0|     Kurt Vile|2008-11-25 17:49:06|2008-11-26 00:00:00|           161|            178|            Pop|        [10]|             [10]|                null|         54881|                 en|Attribution-NonCo...|        50135|          null|           1|           null|        []|             Freeway|\n",
      "|      20|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:48:56|2008-01-01 00:00:00|           311|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           978|                 en|Attribution-NonCo...|          361|          null|           3|           null|        []|     Spiritual Level|\n",
      "|      26|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:05|2008-01-01 00:00:00|           181|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|          1060|                 en|Attribution-NonCo...|          193|          null|           4|           null|        []| Where is your Love?|\n",
      "|      30|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:11|2008-01-01 00:00:00|           174|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           718|                 en|Attribution-NonCo...|          612|          null|           5|           null|        []|           Too Happy|\n",
      "|      46|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:53|2008-01-01 00:00:00|           104|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           252|                 en|Attribution-NonCo...|          171|          null|           8|           null|        []|            Yosemite|\n",
      "|      48|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:56|2008-01-01 00:00:00|           205|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           247|                 en|Attribution-NonCo...|          173|          null|           9|           null|        []|      Light of Light|\n",
      "|     134|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|    medium|        256000|             0|          null|2008-11-26 01:43:19|2008-11-26 00:00:00|           207|              3|        Hip-Hop|        [21]|             [21]|                null|          1126|                 en|Attribution-NonCo...|          943|          null|           5|           null|        []|        Street Music|\n",
      "|     135|             1|2008-11-26 01:49:19|2009-01-07 00:00:00|          null|              0|      58|<p>A couple of un...|         3331|                null|        []|                 mp3|           4|   Single Tracks|                    null|                  null|                    null|                null|              1|2008-11-26 01:47:07|               0|       52|           null|              null|            null|                null|            Abominog|                   null|        ['abominog']|http://myspace.co...|                 null| training|     large|        256000|             1|          null|2008-11-26 01:43:26|2008-11-26 00:00:00|           837|              0|           Rock|    [45, 58]|     [58, 12, 45]|                null|          2484|                 en|Attribution-NonCo...|         1832|          null|           0|           null|        []|        Father's Day|\n",
      "|     136|             1|2008-11-26 01:49:19|2009-01-07 00:00:00|          null|              0|      58|<p>A couple of un...|         3331|                null|        []|                 mp3|           4|   Single Tracks|                    null|                  null|                    null|                null|              1|2008-11-26 01:47:07|               0|       52|           null|              null|            null|                null|            Abominog|                   null|        ['abominog']|http://myspace.co...|                 null| training|    medium|        256000|             1|          null|2008-11-26 01:43:35|2008-11-26 00:00:00|           509|              0|           Rock|    [45, 58]|     [58, 12, 45]|                null|          1948|                 en|Attribution-NonCo...|         1498|          null|           0|           null|        []|Peel Back The Mou...|\n",
      "|     137|             1|2008-11-26 01:49:35|2006-12-01 00:00:00|          null|              2|      59|<p>Here's the pro...|         1681|                null| ['lafms']|        Live at LACE|           2|Live Performance|     1978-01-01 00:00:00|   1998-01-01 00:00:00|    Los Angeles Free ...|<p>Airway was a m...|              0|2008-11-26 01:47:22|               5|       53|     34.0522342|   Los Angeles, CA|    -118.2436849|Rick Potts, Juan ...|              Airway|   Los Angeles Free ...|          ['airway']|http://www.lafms....| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:43:42|1978-04-27 00:00:00|          1233|              2|   Experimental|     [1, 32]|      [32, 1, 38]|<p>Recorded live ...|          2559|                 en|Attribution-NonCo...|         1278|          null|           1|           null| ['lafms']|              Side A|\n",
      "|     138|             1|2008-11-26 01:49:35|2006-12-01 00:00:00|          null|              2|      59|<p>Here's the pro...|         1681|                null| ['lafms']|        Live at LACE|           2|Live Performance|     1978-01-01 00:00:00|   1998-01-01 00:00:00|    Los Angeles Free ...|<p>Airway was a m...|              0|2008-11-26 01:47:22|               5|       53|     34.0522342|   Los Angeles, CA|    -118.2436849|Rick Potts, Juan ...|              Airway|   Los Angeles Free ...|          ['airway']|http://www.lafms....| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:43:56|1978-04-27 00:00:00|          1231|              2|   Experimental|     [1, 32]|      [32, 1, 38]|<p>Recorded live ...|          1909|                 en|Attribution-NonCo...|          489|          null|           2|           null| ['lafms']|              Side B|\n",
      "|     139|             0|2008-11-26 01:49:57|2009-01-16 00:00:00|          null|              1|      60|<p>A full ensambl...|         1304|                null|        []|Every Man For Him...|           2|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|    medium|        128000|             0|          null|2008-11-26 01:44:05|2008-11-26 00:00:00|           296|              3|           Folk|        [17]|             [17]|                null|           702|                 en|Attribution-Nonco...|          582|          null|           2|           null|        []|            CandyAss|\n",
      "|     140|             1|2008-11-26 01:49:59|2007-05-22 00:00:00|          null|              1|      61|<p>Alec K. Redfea...|         1300|Alec K. Refearn, ...|        []|      The Blind Spot|           1|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     small|        128000|             0|          null|2008-11-26 01:44:07|2008-11-26 00:00:00|           253|              5|           Folk|        [17]|             [17]|                null|          1593|                 en|Attribution-Nonco...|         1299|          null|           2|           null|        []|  Queen Of The Wires|\n",
      "|     141|             0|2008-11-26 01:49:57|2009-01-16 00:00:00|          null|              1|      60|<p>A full ensambl...|         1304|                null|        []|Every Man For Him...|           2|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     small|        128000|             0|          null|2008-11-26 01:44:10|2008-11-26 00:00:00|           182|              1|           Folk|        [17]|             [17]|                null|           839|                 en|Attribution-Nonco...|          725|          null|           4|           null|        []|                Ohio|\n",
      "|     142|             0|2008-11-26 01:50:03|2005-01-25 00:00:00|          null|              1|      62|<p>Recorded at So...|          845|                null|        []|      The Quiet Room|           1|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     large|        128000|             0|          null|2008-11-26 01:44:11|2008-11-26 00:00:00|           470|              6|           Folk|        [17]|             [17]|                null|          1223|                 en|Attribution-Nonco...|          848|          null|           5|           null|        []|Punjabi Watery Grave|\n",
      "|     144|             0|2008-11-26 01:50:07|2009-01-06 00:00:00|          null|              0|      64|<p><em>A</em>ltho...|         2014|        Tom Buckland|        []|          Amoebiasis|           0|           Album|     1992-01-01 00:00:00|   1999-01-01 00:00:00|                    null|<p>The obscure, b...|              1|2008-11-26 01:47:54|               7|       56|     41.8239891|    Providence, RI|     -71.4128343|Alec K. Redfearn\\...|    Amoebic Ensemble|             Septimania|['providence', 'a...|http://www.myspac...| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:44:15|1998-11-26 00:00:00|            82|              1|           Jazz|         [4]|              [4]|                null|          1146|                 en|Attribution-Nonco...|         1143|          null|           1|           null|        []|             Wire Up|\n",
      "|     145|             0|2008-11-26 01:50:07|2009-01-06 00:00:00|          null|              0|      64|<p><em>A</em>ltho...|         2014|        Tom Buckland|        []|          Amoebiasis|           0|           Album|     1992-01-01 00:00:00|   1999-01-01 00:00:00|                    null|<p>The obscure, b...|              1|2008-11-26 01:47:54|               7|       56|     41.8239891|    Providence, RI|     -71.4128343|Alec K. Redfearn\\...|    Amoebic Ensemble|             Septimania|['providence', 'a...|http://www.myspac...| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:44:18|1998-11-26 00:00:00|           326|              1|           Jazz|         [4]|              [4]|                null|           968|                 en|Attribution-Nonco...|          883|          null|           3|           null|        []|          Amoebiasis|\n",
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tracks_df = (spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"quote\", '\"')\n",
    "    .option(\"escape\", '\"')\n",
    "    .csv('data/tracks.csv')\n",
    ")\n",
    "\n",
    "# rename columns with row values from first row to second row\n",
    "column_categories = list(zip(*tracks_df.take(2)))\n",
    "columns = tracks_df.columns\n",
    "tracks_df = tracks_df.select(F.col(columns[0]).alias('track_id'),\n",
    "    *(F.col(column).alias(\"-\".join(map(str, categories)))\n",
    "    for column, categories in zip(columns[1:], column_categories[1:]))\n",
    ")\n",
    "\n",
    "tracks_df = (tracks_df\n",
    "    .filter(F.col(\"track_id\").isNotNull()) \n",
    "    .filter(F.col(\"track_id\") != \"track_id\")\n",
    ")\n",
    "\n",
    "tracks_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: feature selection? not needed necessarily but...\n",
    "features_df = (spark.read\n",
    "    .csv('data/features.csv')\n",
    ")\n",
    "\n",
    "# rename columns with row values from first row to second row\n",
    "column_categories = list(zip(*features_df.take(3)))\n",
    "columns = features_df.columns\n",
    "# TODO: DoubleType instead of FloatType?\n",
    "features_df = features_df.select(F.col(columns[0]).alias('track_id'),\n",
    "    *(F.col(column).cast(DoubleType()).alias(\"-\".join(map(str, categories)))\n",
    "    for column, categories in zip(columns[1:], column_categories[1:]))\n",
    ")\n",
    "\n",
    "features_df = (features_df\n",
    "    .filter(F.col(\"track_id\") != \"feature\")\n",
    "    .filter(F.col(\"track_id\") != \"statistics\")\n",
    "    .filter(F.col(\"track_id\") != \"number\")\n",
    "    .filter(F.col(\"track_id\") != \"track_id\")\n",
    ")\n",
    "\n",
    "features_music_columns = features_df.columns.copy()\n",
    "features_music_columns.remove('track_id')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative clustering (in-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_subset_name = \"small\"\n",
    "\n",
    "small_tracks_df = tracks_df.filter(F.col(\"set-subset\") == dataset_subset_name)\n",
    "small_features_df = (features_df\n",
    "    .join(small_tracks_df, \"track_id\", \"left\")\n",
    "    .filter(F.col(\"set-subset\").isNotNull())\n",
    "    .select(features_df.columns)\n",
    ")\n",
    "\n",
    "small_features_pd = small_features_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the metrics (radius, diameter, density_r, density_d) for each cluster\n",
    "def calculate_metrics(pd_df, centroids):\n",
    "    \n",
    "    cluster = pd_df[\"cluster\"].values[0]\n",
    "    metrics = pd.DataFrame({'radius': [0], 'diameter': [0],'density_r': [0],'density_d': [0]}, columns=['radius', 'diameter','density_r','density_d'])\n",
    "    centroid = centroids[cluster].reshape(1,-1)\n",
    "\n",
    "    matrix = pd_df.drop(columns=[\"cluster\", \"track_id\"]).to_numpy()\n",
    "    \n",
    "    matrix_radius = np.sqrt(np.sum((matrix - centroid)**2, axis=1))\n",
    "    metrics.loc[0,'radius'] = np.max(matrix_radius)\n",
    "    # calculate density with radius\n",
    "    metrics.loc[0,'density_r'] = len(pd_df) / metrics.loc[0,'radius']**2\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        matrix_diameter = np.sqrt(np.sum((matrix[i:,:] - matrix[i,:])**2, axis=1))\n",
    "\n",
    "        max_diameter = np.max(matrix_diameter)\n",
    "        if max_diameter > metrics.loc[0,'diameter']:\n",
    "            metrics.loc[0,'diameter'] = max_diameter\n",
    "        \n",
    "    # calculate density with diameter\n",
    "    metrics.loc[0,'density_d'] = len(pd_df) / metrics.loc[0,'diameter']**2\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4186/1666919071.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  small_features_pd[\"cluster\"] = np.zeros((len(small_features_pd), 1), dtype=np.int32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "import numpy.typing as npt\n",
    "\n",
    "metrics_pd_array = []\n",
    "\n",
    "small_features_pd[\"cluster\"] = np.zeros((len(small_features_pd), 1), dtype=np.int32)\n",
    "\n",
    "def cluster_agglomeratively(data: pd.DataFrame, n_clusters: int) -> npt.NDArray[np.float32]:\n",
    "    if \"cluster\" not in data.columns:\n",
    "        raise ValueError(\"The 'cluster' column should be present in the dataframe!\")\n",
    "    \n",
    "    data_features_only = data.drop(columns=[\"cluster\", \"track_id\"])\n",
    "\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    clusterer.fit(data_features_only)\n",
    "    \n",
    "    centroid_calculator = NearestCentroid()\n",
    "    centroid_calculator.fit(data_features_only, clusterer.labels_)\n",
    "\n",
    "    data[\"cluster\"] = clusterer.labels_\n",
    "    return centroid_calculator.centroids_\n",
    "\n",
    "if os.path.exists(\"./results/metrics_pd_array_pickle.pkl\"):\n",
    "    with open(\"./results/metrics_pd_array_pickle.pkl\",'rb') as f:\n",
    "        metrics_pd_array = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    # i = 8 until 16\n",
    "    for n_clusters in range(8, 17):\n",
    "        print(f\"n_clusters: {n_clusters}\")\n",
    "        centroids = cluster_agglomeratively(small_features_pd, n_clusters)\n",
    "        metrics_pd_array.append(small_features_pd.groupby(\"cluster\").apply(calculate_metrics, centroids))\n",
    "\n",
    "    # TODO: weird column at 0s without name??\n",
    "    with open(\"./results/metrics_pd_array_pickle.pkl\",'wb') as f:\n",
    "        pickle.dump(metrics_pd_array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average density_r: 7.962116015409432e-05, Average density_d: 3.5993421010529105e-05, Variance density_r: 7.554301594511152e-09, Variance density_d: 1.6325448959405526e-09\n",
      "\n",
      "\n",
      "Average density_r: 8.468280174708501e-05, Average density_d: 3.664884862339534e-05, Variance density_r: 7.747077822050173e-09, Variance density_d: 1.6211166463782024e-09\n",
      "\n",
      "\n",
      "Average density_r: 8.164135739606747e-05, Average density_d: 3.464754500675633e-05, Variance density_r: 7.052543814624157e-09, Variance density_d: 1.4927141165628917e-09\n",
      "\n",
      "\n",
      "Average density_r: 7.530015533838525e-05, Average density_d: 3.330844178763551e-05, Variance density_r: 5.651489897428292e-09, Variance density_d: 1.2235775469777811e-09\n",
      "\n",
      "\n",
      "Average density_r: 7.083319800111053e-05, Average density_d: 3.12437683959014e-05, Variance density_r: 5.38916128542342e-09, Variance density_d: 1.1658460841988244e-09\n",
      "\n",
      "\n",
      "Average density_r: 6.755265585371375e-05, Average density_d: 2.9803173044205747e-05, Variance density_r: 5.035945257788826e-09, Variance density_d: 1.0937612033572456e-09\n",
      "\n",
      "\n",
      "Average density_r: 6.402566164660506e-05, Average density_d: 2.8265452838229816e-05, Variance density_r: 4.831946821616402e-09, Variance density_d: 1.045583537428686e-09\n",
      "\n",
      "\n",
      "Average density_r: 6.715938168255902e-05, Average density_d: 2.8496640725964807e-05, Variance density_r: 4.608083603538709e-09, Variance density_d: 9.499372285379279e-10\n",
      "\n",
      "\n",
      "Average density_r: 6.424864539260667e-05, Average density_d: 2.7600078751335965e-05, Variance density_r: 4.38961579641494e-09, Variance density_d: 9.001497688649123e-10\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: justify N-clusters choice, maybe create a colored matrix?\n",
    "for i in metrics_pd_array:\n",
    "    density_r_average = i[\"density_r\"].mean()\n",
    "    density_d_average = i[\"density_d\"].mean()\n",
    "    density_r_variance = i[\"density_r\"].var()\n",
    "    density_d_variance = i[\"density_d\"].var()\n",
    "\n",
    "    print(f\"Average density_r: {density_r_average}, Average density_d: {density_d_average}, Variance density_r: {density_r_variance}, Variance density_d: {density_d_variance}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFR Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: results can be: density of clusters, number of nodes in each cluster, etc., but not strictly necessary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "n_clusters = 9\n",
    "\n",
    "max_memory_used_bytes = int(.1e9)\n",
    "# Assumes all columns are doubles/longs, and so therefore 8 bytes\n",
    "rows_per_iteration = max_memory_used_bytes // (8 * len(features_df.columns))\n",
    "\n",
    "seed_random = 0\n",
    "\n",
    "# Threshold in terms of standard deviations away from centroid, in each dimension\n",
    "cluster_distance_threshold_standard_deviations = 1\n",
    "cluster_distance_threshold = ((cluster_distance_threshold_standard_deviations**2) * len(features_music_columns)) ** 0.5\n",
    "\n",
    "# TODO: choose threshold and justify decision\n",
    "compression_set_merge_variance_threshold = float('-inf')\n",
    "\n",
    "# TODO: choose threshold and justify decision\n",
    "#? distance threshold for inserting point in a compressed set???\n",
    "dbscan_eps = 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_centroids = cluster_agglomeratively(small_features_pd, n_clusters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass(init=False)\n",
    "class SummarizedCluster:\n",
    "    n:      int                     \n",
    "    sum_:   npt.NDArray[np.float64]\n",
    "    sumsq_: npt.NDArray[np.float64]\n",
    "    id_:    Union[int, None]\n",
    "    tracks: Set[int]\n",
    "\n",
    "    def __init__(self, dimensions: int, id_: int=None):\n",
    "        self.n = 0\n",
    "        self.sum_ = np.zeros((dimensions,), dtype=np.float64)\n",
    "        self.sumsq_ = np.zeros((dimensions,), dtype=np.float64)\n",
    "        self.id_ = id_\n",
    "        self.tracks = set()\n",
    "    \n",
    "    def summarize(self, point: npt.NDArray[np.float64], track_id: int):\n",
    "        self.n += 1\n",
    "        self.sum_ += point\n",
    "        self.sumsq_ += point**2\n",
    "        self.tracks.add(track_id)\n",
    "    \n",
    "    def summarize_points(self, points: npt.NDArray[np.float64], track_ids: Set[int]):\n",
    "        self.n += points.shape[0]\n",
    "        self.sum_ += np.sum(points, axis=0)\n",
    "        self.sumsq_ += np.sum(points**2, axis=0)\n",
    "        self.tracks |= track_ids\n",
    "        self.test = points\n",
    "\n",
    "    def centroid(self) -> npt.NDArray[np.float64]:\n",
    "        return self.sum_ / self.n\n",
    "\n",
    "    def variance(self) -> npt.NDArray[np.float64]:\n",
    "        return (self.sumsq_ / self.n) - (self.sum_ / self.n)**2\n",
    "\n",
    "    def standard_deviation(self) -> npt.NDArray[np.float64]:\n",
    "        return np.sqrt(self.variance())\n",
    "\n",
    "    def __add__(self, other) -> 'SummarizedCluster':\n",
    "        if self.id_ is not None and self.other is not None and self.id_ != self.other:\n",
    "            raise ValueError(f\"Clusters {self} and {other} have different explicit ids ({self.id_} != {other.id_}).\")\n",
    "        res = SummarizedCluster(self.sum_.size, self.id_ if self.id_ is not None else other.id_)\n",
    "        res.n = self.n + other.n\n",
    "        res.sum_ = self.sum_ + other.sum_\n",
    "        res.sumsq_ = self.sumsq_ + other.sumsq_\n",
    "        res.tracks = self.tracks | other.tracks\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_sets: List[SummarizedCluster] = [SummarizedCluster(len(features_music_columns), id_) for id_ in range(n_clusters)]\n",
    "compression_sets: List[SummarizedCluster] = []\n",
    "retained_set: pd.DataFrame = pd.DataFrame(data=[], columns=features_music_columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize clusters with the clustering of the small dataset subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_cluster_df(cluster_df: pd.DataFrame) -> None:\n",
    "    cluster_id = cluster_df[\"cluster\"].values[0]\n",
    "    cluster_features_mtx = cluster_df.drop(columns=[\"cluster\", \"track_id\"]).to_numpy()\n",
    "    \n",
    "    track_ids = set(cluster_df[\"track_id\"].values)\n",
    "\n",
    "    discard_set = discard_sets[cluster_id]\n",
    "    discard_set.summarize_points(cluster_features_mtx, track_ids)\n",
    "\n",
    "small_features_pd.groupby(\"cluster\").apply(summarize_cluster_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(k_centroids) == n_clusters, \"The number of clusters does not coincide with the number of random centroids!\"\n",
    "\n",
    "def mahalanobis_distance_pd(x: pd.DataFrame, s: SummarizedCluster) -> pd.Series:\n",
    "    return (((x - s.centroid()) / s.standard_deviation())**2).sum(axis=1) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_without_small_df = (features_df\n",
    "    .join(small_tracks_df, \"track_id\", \"left\")\n",
    "    .filter(F.col(\"set-subset\").isNull())\n",
    "    .select(features_df.columns)\n",
    ")\n",
    "\n",
    "total_rows = features_without_small_df.count()\n",
    "split_weights = [1.0] * (1 + (total_rows // rows_per_iteration))\n",
    "\n",
    "split_dfs = features_without_small_df.randomSplit(split_weights, seed=seed_random)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign discard set id to each point in a set of loaded points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_discard_sets(loaded_points_pd: pd.DataFrame):\n",
    "    loaded_points_pd = pd.concat(\n",
    "        objs=[loaded_points_pd, pd.DataFrame(\n",
    "            data=np.zeros((loaded_points_pd.shape[0], len(cluster_distance_columns) + 2)),\n",
    "            columns=cluster_distance_columns + ['min_cluster_distance', 'cluster_id'])],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    for i, discard_set in enumerate(discard_sets):\n",
    "        loaded_points_pd[f\"cluster_distance_{i}\"] = mahalanobis_distance_pd(loaded_points_pd.iloc[:, 1:519], discard_set)\n",
    "    loaded_points_pd[\"min_cluster_distance\"] = loaded_points_pd[cluster_distance_columns].min(axis=1)\n",
    "    loaded_points_pd[\"cluster_id\"] = loaded_points_pd[cluster_distance_columns].idxmin(axis=1).str.slice(start=len(prefix)).astype(np.int32)\n",
    "\n",
    "    loaded_points_pd.drop(columns=cluster_distance_columns, inplace=True)\n",
    "\n",
    "    # Don't consider the points that surpass the threshold for the discard sets\n",
    "    loaded_points_pd.loc[loaded_points_pd[\"min_cluster_distance\"] >= cluster_distance_threshold, \"cluster_id\"] = -1\n",
    "    loaded_points_pd.drop(columns=[\"min_cluster_distance\"], inplace=True)\n",
    "\n",
    "    return loaded_points_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80.000000%] Collecting split into memory...                                                                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.000000%] Finished one of the splits (compression sets: 60, retained set: 1762)                               \n"
     ]
    }
   ],
   "source": [
    "prefix = \"cluster_distance_\"\n",
    "cluster_distance_columns = [f\"{prefix}{i}\" for i in range(n_clusters)]\n",
    "\n",
    "def print_progress(progress: float, message: str):\n",
    "    print(f\"[{progress:3%}] {message:100}\", end=\"\\r\")\n",
    "\n",
    "print_progress(0, \"Initialized BFR\")\n",
    "\n",
    "# TODO: parallelize with Spark (groupyby + apply), arranjar forma de poderem mexer no mesmo discard set (por exemplo cada um ter a sua cópia e depois juntar tudo, (em termos de resultados pode vir a ser pior ????)\n",
    "for split_idx, loaded_points_df in enumerate(split_dfs):\n",
    "    progress = split_idx / len(split_weights)\n",
    "    \n",
    "    print_progress(progress, \"Collecting split into memory...\")\n",
    "    loaded_points_pd = loaded_points_df.toPandas()\n",
    "\n",
    "    print_progress(progress, \"Clustering with the Mahalanobis distance...\")\n",
    "    \n",
    "    loaded_points_pd = assign_discard_sets(loaded_points_pd)\n",
    "    \n",
    "    print_progress(progress, \"Calculated and collected Mahalanobis distances\")\n",
    "\n",
    "    for cluster_id, cluster_df in loaded_points_pd.groupby(\"cluster_id\"):\n",
    "        track_ids = cluster_df[\"track_id\"]\n",
    "        features_list = cluster_df.iloc[:, 1:519]\n",
    "\n",
    "        print_progress(progress, f\"Evaluating cluster {cluster_id}...\")\n",
    "\n",
    "        # Step 3 - check which points go to the discard sets\n",
    "        if cluster_id != -1:\n",
    "            discard_set = discard_sets[cluster_id]\n",
    "            discard_set.summarize_points(features_list, set(track_ids))\n",
    "        \n",
    "        # Step 4 - check which points go to the compression sets or the retained set\n",
    "        else:\n",
    "            matrix_to_cluster = pd.concat(objs=[features_list, retained_set], axis=0)\n",
    "\n",
    "            # Use same distance as above\n",
    "            clusterer = DBSCAN(eps=dbscan_eps, metric='euclidean')\n",
    "            clusterer.fit(matrix_to_cluster)\n",
    "\n",
    "            retained_set = matrix_to_cluster[clusterer.labels_ == -1]\n",
    "\n",
    "            mini_clusters = set(clusterer.labels_) - {-1}\n",
    "\n",
    "            # Create compression sets\n",
    "            compression_sets_temp = [SummarizedCluster(len(features_music_columns), None) for _ in mini_clusters]\n",
    "            for mini_cluster_id in mini_clusters:\n",
    "                compression_sets_temp[mini_cluster_id].summarize_points(matrix_to_cluster[clusterer.labels_ == mini_cluster_id], set(track_ids))\n",
    "            \n",
    "            compression_sets.extend(compression_sets_temp)\n",
    "        \n",
    "    print_progress(progress, f\"Finished evaluating clusters (compression sets: {len(compression_sets)}, retained set: {len(retained_set)})\")\n",
    "\n",
    "    # Step 5 - merge compression sets\n",
    "    compressing = True\n",
    "    while compressing:\n",
    "        compressing = False\n",
    "        merged_compression_sets = []\n",
    "        compression_set_idxs_to_remove = set()\n",
    "\n",
    "        for (idx_1, compression_set_1), (idx_2, compression_set_2) in combinations(enumerate(compression_sets), 2):\n",
    "            if idx_1 in compression_set_idxs_to_remove or idx_2 in compression_set_idxs_to_remove:\n",
    "                continue\n",
    "\n",
    "            merged_compression_set = compression_set_1 + compression_set_2\n",
    "            if (merged_compression_set.variance() < compression_set_merge_variance_threshold).any():\n",
    "                merged_compression_sets.append(merged_compression_set)\n",
    "                compression_set_idxs_to_remove.add(idx_1)\n",
    "                compression_set_idxs_to_remove.add(idx_2)\n",
    "                compressing = True\n",
    "        \n",
    "        compression_sets = [cs for i, cs in enumerate(compression_sets) if i not in compression_set_idxs_to_remove]\n",
    "        compression_sets.extend(merged_compression_sets)\n",
    "\n",
    "        print_progress(progress, f\"Completed one compression (compression sets: {len(compression_sets)})\")\n",
    "    \n",
    "    print_progress((split_idx + 1) / len(split_weights), f\"Finished one of the splits (compression sets: {len(compression_sets)}, retained set: {len(retained_set)})\")\n",
    "\n",
    "# Step 6 - merge CS and RS into DS (but we won't yet)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFR cluster visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Old-Time / Historic', 'Blues', 'Easy Listening'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m cluster_idxs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(n_clusters))\n\u001b[1;32m      3\u001b[0m genres \u001b[39m=\u001b[39m [row[\u001b[39m\"\u001b[39m\u001b[39mtrack-genre_top\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m tracks_df\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39mtrack-genre_top\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdistinct()\u001b[39m.\u001b[39mcollect()]\n\u001b[1;32m      5\u001b[0m genre_counts \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[0;32m----> 6\u001b[0m     data\u001b[39m=\u001b[39m[\n\u001b[1;32m      7\u001b[0m         (tracks_df\n\u001b[1;32m      8\u001b[0m          \u001b[39m.\u001b[39mfilter(F\u001b[39m.\u001b[39mcol(\u001b[39m\"\u001b[39m\u001b[39mtrack_id\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39misin(discard_set\u001b[39m.\u001b[39mtracks))\n\u001b[1;32m      9\u001b[0m          \u001b[39m.\u001b[39mgroupby(\u001b[39m\"\u001b[39m\u001b[39mtrack-genre_top\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m          \u001b[39m.\u001b[39mcount()\n\u001b[1;32m     11\u001b[0m          \u001b[39m.\u001b[39mtoPandas()\n\u001b[1;32m     12\u001b[0m          \u001b[39m.\u001b[39mset_index(\u001b[39m\"\u001b[39m\u001b[39mtrack-genre_top\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m          \u001b[39m.\u001b[39mloc[genres, :]\n\u001b[1;32m     14\u001b[0m         )\n\u001b[1;32m     15\u001b[0m         \u001b[39mfor\u001b[39;00m discard_set \u001b[39min\u001b[39;00m discard_sets\n\u001b[1;32m     16\u001b[0m     ],\n\u001b[1;32m     17\u001b[0m     columns\u001b[39m=\u001b[39mgenres\n\u001b[1;32m     18\u001b[0m )\n",
      "Cell \u001b[0;32mIn[74], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m cluster_idxs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(n_clusters))\n\u001b[1;32m      3\u001b[0m genres \u001b[39m=\u001b[39m [row[\u001b[39m\"\u001b[39m\u001b[39mtrack-genre_top\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m tracks_df\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39mtrack-genre_top\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdistinct()\u001b[39m.\u001b[39mcollect()]\n\u001b[1;32m      5\u001b[0m genre_counts \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[1;32m      6\u001b[0m     data\u001b[39m=\u001b[39m[\n\u001b[0;32m----> 7\u001b[0m         (tracks_df\n\u001b[1;32m      8\u001b[0m          \u001b[39m.\u001b[39;49mfilter(F\u001b[39m.\u001b[39;49mcol(\u001b[39m\"\u001b[39;49m\u001b[39mtrack_id\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49misin(discard_set\u001b[39m.\u001b[39;49mtracks))\n\u001b[1;32m      9\u001b[0m          \u001b[39m.\u001b[39;49mgroupby(\u001b[39m\"\u001b[39;49m\u001b[39mtrack-genre_top\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m          \u001b[39m.\u001b[39;49mcount()\n\u001b[1;32m     11\u001b[0m          \u001b[39m.\u001b[39;49mtoPandas()\n\u001b[1;32m     12\u001b[0m          \u001b[39m.\u001b[39;49mset_index(\u001b[39m\"\u001b[39;49m\u001b[39mtrack-genre_top\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     13\u001b[0m          \u001b[39m.\u001b[39;49mloc[genres, :]\n\u001b[1;32m     14\u001b[0m         )\n\u001b[1;32m     15\u001b[0m         \u001b[39mfor\u001b[39;00m discard_set \u001b[39min\u001b[39;00m discard_sets\n\u001b[1;32m     16\u001b[0m     ],\n\u001b[1;32m     17\u001b[0m     columns\u001b[39m=\u001b[39mgenres\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:1067\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1066\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[0;32m-> 1067\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[1;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1069\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:1256\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[1;32m   1254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multi_take(tup)\n\u001b[0;32m-> 1256\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:924\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_null_slice(key):\n\u001b[1;32m    922\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 924\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(retval, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\u001b[39m.\u001b[39;49m_getitem_axis(key, axis\u001b[39m=\u001b[39;49mi)\n\u001b[1;32m    925\u001b[0m \u001b[39m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[39m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39massert\u001b[39;00m retval\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:1301\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(key, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1299\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index with multidimensional key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1301\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_iterable(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   1303\u001b[0m \u001b[39m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m \u001b[39mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:1239\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1238\u001b[0m \u001b[39m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1239\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1240\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1241\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py:1432\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1429\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1430\u001b[0m axis_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1432\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49m_get_indexer_strict(key, axis_name)\n\u001b[1;32m   1434\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 6133\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Old-Time / Historic', 'Blues', 'Easy Listening'] not in index\""
     ]
    }
   ],
   "source": [
    "cluster_idxs = list(range(n_clusters))\n",
    "\n",
    "genres = [row[\"track-genre_top\"] for row in tracks_df.select(\"track-genre_top\").distinct().collect()]\n",
    "\n",
    "genre_counts_for_each_discard_set = [\n",
    "    {\n",
    "        row[\"track-genre_top\"]:row[\"count\"]\n",
    "        for row in tracks_df\n",
    "            .filter(F.col(\"track_id\").isin(discard_set.tracks))\n",
    "            .groupby(\"track-genre_top\")\n",
    "            .count()\n",
    "            .collect()\n",
    "    }\n",
    "    for discard_set in discard_sets\n",
    "]\n",
    "\n",
    "genre_counts = {\n",
    "    genre:np.array([(genre_counts_for_each_discard_set[discard_set_idx][genre] if genre in genre_counts_for_each_discard_set[discard_set_idx] else 0) for discard_set_idx in range(n_clusters)])\n",
    "    for genre in genres\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 0.5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "bottom = np.zeros(n_clusters)\n",
    "\n",
    "for genre in genres:\n",
    "    p = ax.bar(cluster_idxs, genre_counts[genre], width, label=genre, bottom=bottom)\n",
    "    bottom += genre_counts[genre]\n",
    "\n",
    "ax.set_title(\"Number of tracks per genre on each cluster (discard set)\")\n",
    "ax.legend(loc=\"upper right\", fontsize=10)\n",
    "ax.set_xlabel(\"Cluster\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
