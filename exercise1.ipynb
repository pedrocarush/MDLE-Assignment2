{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType, DoubleType\n",
    "from itertools import combinations\n",
    "from typing import Iterable, Any, List, Set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/20 21:52:34 WARN Utils: Your hostname, martinho-MS-7B86 resolves to a loopback address: 127.0.1.1; using 192.168.1.67 instead (on interface enp34s0)\n",
      "23/04/20 21:52:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/20 21:52:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('exercise1') \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/20 21:52:38 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "|track_id|album-comments| album-date_created|album-date_released|album-engineer|album-favorites|album-id|   album-information|album-listens|      album-producer|album-tags|         album-title|album-tracks|      album-type|artist-active_year_begin|artist-active_year_end|artist-associated_labels|          artist-bio|artist-comments|artist-date_created|artist-favorites|artist-id|artist-latitude|   artist-location|artist-longitude|      artist-members|         artist-name|artist-related_projects|         artist-tags|      artist-website|artist-wikipedia_page|set-split|set-subset|track-bit_rate|track-comments|track-composer| track-date_created|track-date_recorded|track-duration|track-favorites|track-genre_top|track-genres| track-genres_all|   track-information|track-interest|track-language_code|       track-license|track-listens|track-lyricist|track-number|track-publisher|track-tags|         track-title|\n",
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "|       2|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|     small|        256000|             0|          null|2008-11-26 01:48:12|2008-11-26 00:00:00|           168|              2|        Hip-Hop|        [21]|             [21]|                null|          4656|                 en|Attribution-NonCo...|         1293|          null|           3|           null|        []|                Food|\n",
      "|       3|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|    medium|        256000|             0|          null|2008-11-26 01:48:14|2008-11-26 00:00:00|           237|              1|        Hip-Hop|        [21]|             [21]|                null|          1470|                 en|Attribution-NonCo...|          514|          null|           4|           null|        []|        Electric Ave|\n",
      "|       5|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|     small|        256000|             0|          null|2008-11-26 01:48:20|2008-11-26 00:00:00|           206|              6|        Hip-Hop|        [21]|             [21]|                null|          1933|                 en|Attribution-NonCo...|         1151|          null|           6|           null|        []|          This World|\n",
      "|      10|             0|2008-11-26 01:45:08|2008-02-06 00:00:00|          null|              4|       6|                null|        47632|                null|        []|   Constant Hitmaker|           2|           Album|                    null|                  null|    Mexican Summer, R...|<p><span style=\"f...|              3|2008-11-26 01:42:55|              74|        6|           null|              null|            null|Kurt Vile, the Vi...|           Kurt Vile|                   null|['philly', 'kurt ...| http://kurtvile.com|                 null| training|     small|        192000|             0|     Kurt Vile|2008-11-25 17:49:06|2008-11-26 00:00:00|           161|            178|            Pop|        [10]|             [10]|                null|         54881|                 en|Attribution-NonCo...|        50135|          null|           1|           null|        []|             Freeway|\n",
      "|      20|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:48:56|2008-01-01 00:00:00|           311|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           978|                 en|Attribution-NonCo...|          361|          null|           3|           null|        []|     Spiritual Level|\n",
      "|      26|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:05|2008-01-01 00:00:00|           181|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|          1060|                 en|Attribution-NonCo...|          193|          null|           4|           null|        []| Where is your Love?|\n",
      "|      30|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:11|2008-01-01 00:00:00|           174|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           718|                 en|Attribution-NonCo...|          612|          null|           5|           null|        []|           Too Happy|\n",
      "|      46|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:53|2008-01-01 00:00:00|           104|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           252|                 en|Attribution-NonCo...|          171|          null|           8|           null|        []|            Yosemite|\n",
      "|      48|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:56|2008-01-01 00:00:00|           205|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           247|                 en|Attribution-NonCo...|          173|          null|           9|           null|        []|      Light of Light|\n",
      "|     134|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|    medium|        256000|             0|          null|2008-11-26 01:43:19|2008-11-26 00:00:00|           207|              3|        Hip-Hop|        [21]|             [21]|                null|          1126|                 en|Attribution-NonCo...|          943|          null|           5|           null|        []|        Street Music|\n",
      "|     135|             1|2008-11-26 01:49:19|2009-01-07 00:00:00|          null|              0|      58|<p>A couple of un...|         3331|                null|        []|                 mp3|           4|   Single Tracks|                    null|                  null|                    null|                null|              1|2008-11-26 01:47:07|               0|       52|           null|              null|            null|                null|            Abominog|                   null|        ['abominog']|http://myspace.co...|                 null| training|     large|        256000|             1|          null|2008-11-26 01:43:26|2008-11-26 00:00:00|           837|              0|           Rock|    [45, 58]|     [58, 12, 45]|                null|          2484|                 en|Attribution-NonCo...|         1832|          null|           0|           null|        []|        Father's Day|\n",
      "|     136|             1|2008-11-26 01:49:19|2009-01-07 00:00:00|          null|              0|      58|<p>A couple of un...|         3331|                null|        []|                 mp3|           4|   Single Tracks|                    null|                  null|                    null|                null|              1|2008-11-26 01:47:07|               0|       52|           null|              null|            null|                null|            Abominog|                   null|        ['abominog']|http://myspace.co...|                 null| training|    medium|        256000|             1|          null|2008-11-26 01:43:35|2008-11-26 00:00:00|           509|              0|           Rock|    [45, 58]|     [58, 12, 45]|                null|          1948|                 en|Attribution-NonCo...|         1498|          null|           0|           null|        []|Peel Back The Mou...|\n",
      "|     137|             1|2008-11-26 01:49:35|2006-12-01 00:00:00|          null|              2|      59|<p>Here's the pro...|         1681|                null| ['lafms']|        Live at LACE|           2|Live Performance|     1978-01-01 00:00:00|   1998-01-01 00:00:00|    Los Angeles Free ...|<p>Airway was a m...|              0|2008-11-26 01:47:22|               5|       53|     34.0522342|   Los Angeles, CA|    -118.2436849|Rick Potts, Juan ...|              Airway|   Los Angeles Free ...|          ['airway']|http://www.lafms....| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:43:42|1978-04-27 00:00:00|          1233|              2|   Experimental|     [1, 32]|      [32, 1, 38]|<p>Recorded live ...|          2559|                 en|Attribution-NonCo...|         1278|          null|           1|           null| ['lafms']|              Side A|\n",
      "|     138|             1|2008-11-26 01:49:35|2006-12-01 00:00:00|          null|              2|      59|<p>Here's the pro...|         1681|                null| ['lafms']|        Live at LACE|           2|Live Performance|     1978-01-01 00:00:00|   1998-01-01 00:00:00|    Los Angeles Free ...|<p>Airway was a m...|              0|2008-11-26 01:47:22|               5|       53|     34.0522342|   Los Angeles, CA|    -118.2436849|Rick Potts, Juan ...|              Airway|   Los Angeles Free ...|          ['airway']|http://www.lafms....| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:43:56|1978-04-27 00:00:00|          1231|              2|   Experimental|     [1, 32]|      [32, 1, 38]|<p>Recorded live ...|          1909|                 en|Attribution-NonCo...|          489|          null|           2|           null| ['lafms']|              Side B|\n",
      "|     139|             0|2008-11-26 01:49:57|2009-01-16 00:00:00|          null|              1|      60|<p>A full ensambl...|         1304|                null|        []|Every Man For Him...|           2|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|    medium|        128000|             0|          null|2008-11-26 01:44:05|2008-11-26 00:00:00|           296|              3|           Folk|        [17]|             [17]|                null|           702|                 en|Attribution-Nonco...|          582|          null|           2|           null|        []|            CandyAss|\n",
      "|     140|             1|2008-11-26 01:49:59|2007-05-22 00:00:00|          null|              1|      61|<p>Alec K. Redfea...|         1300|Alec K. Refearn, ...|        []|      The Blind Spot|           1|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     small|        128000|             0|          null|2008-11-26 01:44:07|2008-11-26 00:00:00|           253|              5|           Folk|        [17]|             [17]|                null|          1593|                 en|Attribution-Nonco...|         1299|          null|           2|           null|        []|  Queen Of The Wires|\n",
      "|     141|             0|2008-11-26 01:49:57|2009-01-16 00:00:00|          null|              1|      60|<p>A full ensambl...|         1304|                null|        []|Every Man For Him...|           2|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     small|        128000|             0|          null|2008-11-26 01:44:10|2008-11-26 00:00:00|           182|              1|           Folk|        [17]|             [17]|                null|           839|                 en|Attribution-Nonco...|          725|          null|           4|           null|        []|                Ohio|\n",
      "|     142|             0|2008-11-26 01:50:03|2005-01-25 00:00:00|          null|              1|      62|<p>Recorded at So...|          845|                null|        []|      The Quiet Room|           1|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     large|        128000|             0|          null|2008-11-26 01:44:11|2008-11-26 00:00:00|           470|              6|           Folk|        [17]|             [17]|                null|          1223|                 en|Attribution-Nonco...|          848|          null|           5|           null|        []|Punjabi Watery Grave|\n",
      "|     144|             0|2008-11-26 01:50:07|2009-01-06 00:00:00|          null|              0|      64|<p><em>A</em>ltho...|         2014|        Tom Buckland|        []|          Amoebiasis|           0|           Album|     1992-01-01 00:00:00|   1999-01-01 00:00:00|                    null|<p>The obscure, b...|              1|2008-11-26 01:47:54|               7|       56|     41.8239891|    Providence, RI|     -71.4128343|Alec K. Redfearn\\...|    Amoebic Ensemble|             Septimania|['providence', 'a...|http://www.myspac...| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:44:15|1998-11-26 00:00:00|            82|              1|           Jazz|         [4]|              [4]|                null|          1146|                 en|Attribution-Nonco...|         1143|          null|           1|           null|        []|             Wire Up|\n",
      "|     145|             0|2008-11-26 01:50:07|2009-01-06 00:00:00|          null|              0|      64|<p><em>A</em>ltho...|         2014|        Tom Buckland|        []|          Amoebiasis|           0|           Album|     1992-01-01 00:00:00|   1999-01-01 00:00:00|                    null|<p>The obscure, b...|              1|2008-11-26 01:47:54|               7|       56|     41.8239891|    Providence, RI|     -71.4128343|Alec K. Redfearn\\...|    Amoebic Ensemble|             Septimania|['providence', 'a...|http://www.myspac...| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:44:18|1998-11-26 00:00:00|           326|              1|           Jazz|         [4]|              [4]|                null|           968|                 en|Attribution-Nonco...|          883|          null|           3|           null|        []|          Amoebiasis|\n",
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tracks_df = (spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"quote\", '\"')\n",
    "    .option(\"escape\", '\"')\n",
    "    .csv('data/tracks.csv')\n",
    ")\n",
    "\n",
    "# rename columns with row values from first row to second row\n",
    "column_categories = list(zip(*tracks_df.take(2)))\n",
    "columns = tracks_df.columns\n",
    "tracks_df = tracks_df.select(F.col(columns[0]).alias('track_id'),\n",
    "    *(F.col(column).alias(\"-\".join(map(str, categories)))\n",
    "    for column, categories in zip(columns[1:], column_categories[1:]))\n",
    ")\n",
    "\n",
    "tracks_df = (tracks_df\n",
    "    .filter(F.col(\"track_id\").isNotNull()) \n",
    "    .filter(F.col(\"track_id\") != \"track_id\")\n",
    ")\n",
    "\n",
    "tracks_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: feature selection? not needed necessarily but...\n",
    "features_df = (spark.read\n",
    "    .csv('data/features.csv')\n",
    ")\n",
    "\n",
    "# rename columns with row values from first row to second row\n",
    "column_categories = list(zip(*features_df.take(3)))\n",
    "columns = features_df.columns\n",
    "# TODO: DoubleType instead of FloatType?\n",
    "features_df = features_df.select(F.col(columns[0]).alias('track_id'),\n",
    "    *(F.col(column).cast(DoubleType()).alias(\"-\".join(map(str, categories)))\n",
    "    for column, categories in zip(columns[1:], column_categories[1:]))\n",
    ")\n",
    "\n",
    "features_df = (features_df\n",
    "    .filter(F.col(\"track_id\") != \"feature\")\n",
    "    .filter(F.col(\"track_id\") != \"statistics\")\n",
    "    .filter(F.col(\"track_id\") != \"number\")\n",
    "    .filter(F.col(\"track_id\") != \"track_id\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative clustering (in-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_subset_name = \"small\"\n",
    "\n",
    "small_tracks_df = tracks_df.filter(F.col(\"set-subset\") == dataset_subset_name)\n",
    "small_features_df = (features_df\n",
    "    .join(small_tracks_df, \"track_id\", \"left\")\n",
    "    .filter(F.col(\"set-subset\").isNotNull())\n",
    "    .select(features_df.columns)\n",
    ")\n",
    "\n",
    "small_features_pd = small_features_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the metrics (radius, diameter, density_r, density_d) for each cluster\n",
    "def calculate_metrics(pd_df, centroids):\n",
    "    \n",
    "    cluster = pd_df[\"cluster\"].values[0]\n",
    "    metrics = pd.DataFrame({'radius': [0], 'diameter': [0],'density_r': [0],'density_d': [0]}, columns=['radius', 'diameter','density_r','density_d'])\n",
    "    centroid = centroids[cluster].reshape(1,-1)\n",
    "\n",
    "    matrix = pd_df.drop(columns=[\"cluster\", \"track_id\"]).to_numpy()\n",
    "    \n",
    "    matrix_radius = np.sqrt(np.sum((matrix - centroid)**2, axis=1))\n",
    "    metrics.loc[0,'radius'] = np.max(matrix_radius)\n",
    "    # calculate density with radius\n",
    "    metrics.loc[0,'density_r'] = len(pd_df) / metrics.loc[0,'radius']**2\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        matrix_diameter = np.sqrt(np.sum((matrix[i:,:] - matrix[i,:])**2, axis=1))\n",
    "\n",
    "        max_diameter = np.max(matrix_diameter)\n",
    "        if max_diameter > metrics.loc[0,'diameter']:\n",
    "            metrics.loc[0,'diameter'] = max_diameter\n",
    "        \n",
    "    # calculate density with diameter\n",
    "    metrics.loc[0,'density_d'] = len(pd_df) / metrics.loc[0,'diameter']**2\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66721/1666919071.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  small_features_pd[\"cluster\"] = np.zeros((len(small_features_pd), 1), dtype=np.int32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "import numpy.typing as npt\n",
    "\n",
    "metrics_pd_array = []\n",
    "\n",
    "small_features_pd[\"cluster\"] = np.zeros((len(small_features_pd), 1), dtype=np.int32)\n",
    "\n",
    "def cluster_agglomeratively(data: pd.DataFrame, n_clusters: int) -> npt.NDArray[np.float32]:\n",
    "    if \"cluster\" not in data.columns:\n",
    "        raise ValueError(\"The 'cluster' column should be present in the dataframe!\")\n",
    "    \n",
    "    data_features_only = data.drop(columns=[\"cluster\", \"track_id\"])\n",
    "\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    clusterer.fit(data_features_only)\n",
    "    \n",
    "    centroid_calculator = NearestCentroid()\n",
    "    centroid_calculator.fit(data_features_only, clusterer.labels_)\n",
    "\n",
    "    data[\"cluster\"] = clusterer.labels_\n",
    "    return centroid_calculator.centroids_\n",
    "\n",
    "if os.path.exists(\"./results/metrics_pd_array_pickle.pkl\"):\n",
    "    with open(\"./results/metrics_pd_array_pickle.pkl\",'rb') as f:\n",
    "        metrics_pd_array = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    # i = 8 until 16\n",
    "    for n_clusters in range(8, 17):\n",
    "        print(f\"n_clusters: {n_clusters}\")\n",
    "        centroids = cluster_agglomeratively(small_features_pd, n_clusters)\n",
    "        metrics_pd_array.append(small_features_pd.groupby(\"cluster\").apply(calculate_metrics, centroids))\n",
    "\n",
    "    # TODO: weird column at 0s without name??\n",
    "    with open(\"./results/metrics_pd_array_pickle.pkl\",'wb') as f:\n",
    "        pickle.dump(metrics_pd_array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average density_r: 7.962110575648213e-05, Average density_d: 3.599342497079214e-05, Variance density_r: 7.554281742039276e-09, Variance density_d: 1.632545349816937e-09\n",
      "\n",
      "\n",
      "Average density_r: 8.46827574451412e-05, Average density_d: 3.664885346147574e-05, Variance density_r: 7.747059677670204e-09, Variance density_d: 1.6211171070541367e-09\n",
      "\n",
      "\n",
      "Average density_r: 8.164132830338627e-05, Average density_d: 3.4647549186503005e-05, Variance density_r: 7.052526954162877e-09, Variance density_d: 1.4927145547250588e-09\n",
      "\n",
      "\n",
      "Average density_r: 7.530009483773654e-05, Average density_d: 3.330844689045004e-05, Variance density_r: 5.65147072510877e-09, Variance density_d: 1.2235780221208591e-09\n",
      "\n",
      "\n",
      "Average density_r: 7.0833141687739e-05, Average density_d: 3.124377322263076e-05, Variance density_r: 5.389143411471879e-09, Variance density_d: 1.1658465305734218e-09\n",
      "\n",
      "\n",
      "Average density_r: 6.755260440629955e-05, Average density_d: 2.980317784207617e-05, Variance density_r: 5.035928633263812e-09, Variance density_d: 1.0937616151746232e-09\n",
      "\n",
      "\n",
      "Average density_r: 6.402561239482578e-05, Average density_d: 2.8265457642839695e-05, Variance density_r: 4.831931244307105e-09, Variance density_d: 1.0455839193413751e-09\n",
      "\n",
      "\n",
      "Average density_r: 6.715931272807029e-05, Average density_d: 2.849664609437203e-05, Variance density_r: 4.608068657771316e-09, Variance density_d: 9.499376042002478e-10\n",
      "\n",
      "\n",
      "Average density_r: 6.424857597589042e-05, Average density_d: 2.760008361725085e-05, Variance density_r: 4.389601516147071e-09, Variance density_d: 9.001501276506571e-10\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: justify N-clusters choice, maybe create a colored matrix?\n",
    "for i in metrics_pd_array:\n",
    "    density_r_average = i[\"density_r\"].mean()\n",
    "    density_d_average = i[\"density_d\"].mean()\n",
    "    density_r_variance = i[\"density_r\"].var()\n",
    "    density_d_variance = i[\"density_d\"].var()\n",
    "\n",
    "    print(f\"Average density_r: {density_r_average}, Average density_d: {density_d_average}, Variance density_r: {density_r_variance}, Variance density_d: {density_d_variance}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFR Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: results can be: density of clusters, number of nodes in each cluster, etc., but not strictly necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# TODO: why were 9 clusters chosen?\n",
    "n_clusters = 9\n",
    "dimensions = len(features_df.columns) - 1   # don't consider 'track_id'\n",
    "\n",
    "max_memory_used_megabytes = 4000\n",
    "# Assumes all columns are floats/integers, and so therefore 4 bytes\n",
    "rows_per_iteration = max_memory_used_megabytes // (4 * len(features_df.columns))\n",
    "total_rows = features_df.count()\n",
    "\n",
    "seed_random = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_centroids = cluster_agglomeratively(small_features_pd, n_clusters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass(init=False)\n",
    "class SummarizedCluster:\n",
    "    n:      int                     \n",
    "    sum_:   npt.NDArray[np.float64]\n",
    "    sumsq_: npt.NDArray[np.float64]\n",
    "    id_:    Union[int, None]\n",
    "    tracks: Set[int]\n",
    "\n",
    "    def __init__(self, dimensions: int, id_: int=None):\n",
    "        self.n = 0\n",
    "        self.sum_ = np.zeros((dimensions,), dtype=np.float64)\n",
    "        self.sumsq_ = np.zeros((dimensions,), dtype=np.float64)\n",
    "        self.id_ = id_\n",
    "        self.tracks = set()\n",
    "    \n",
    "    def summarize(self, point: npt.NDArray[np.float64], track_id: int):\n",
    "        self.n += 1\n",
    "        self.sum_ += point\n",
    "        self.sumsq_ += point**2\n",
    "        self.tracks.add(track_id)\n",
    "    \n",
    "    def summarize_points(self, points: npt.NDArray[np.float64], track_ids: Set[int]):\n",
    "        self.n += points.shape[0]\n",
    "        self.sum_ += np.sum(points, axis=0)\n",
    "        self.sumsq_ += np.sum(points**2, axis=0)\n",
    "        self.tracks |= track_ids\n",
    "        self.test = points\n",
    "\n",
    "    def centroid(self) -> npt.NDArray[np.float64]:\n",
    "        return self.sum_ / self.n\n",
    "\n",
    "    def variance(self) -> npt.NDArray[np.float64]:\n",
    "        return (self.sumsq_ / self.n) - (self.sum_ / self.n)**2\n",
    "\n",
    "    def standard_deviation(self) -> npt.NDArray[np.float64]:\n",
    "        return np.sqrt(self.variance())\n",
    "\n",
    "    def __add__(self, other) -> 'SummarizedCluster':\n",
    "        if self.id_ is not None and self.other is not None and self.id_ != self.other:\n",
    "            raise ValueError(f\"Clusters {self} and {other} have different explicit ids ({self.id_} != {other.id_}).\")\n",
    "        res = SummarizedCluster(self.dimensions, self.id_ if self.id_ is not None else other.id_)\n",
    "        res.n = self.n + other.n\n",
    "        res.sum_ = self.sum_ + other.sum_\n",
    "        res.sumsq_ = self.sumsq_ + other.sumsq_\n",
    "        res.tracks = self.tracks | other.tracks\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_sets: List[SummarizedCluster] = [SummarizedCluster(dimensions, id_) for id_ in range(n_clusters)]\n",
    "compression_sets: List[SummarizedCluster] = []\n",
    "retained_set: List[npt.NDArray[np.float32]] = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize clusters with the clustering of the small dataset subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize_cluster_df(cluster_df: pd.DataFrame) -> None:\n",
    "    cluster_id = cluster_df[\"cluster\"].values[0]\n",
    "    cluster_features_mtx = cluster_df.drop(columns=[\"cluster\", \"track_id\"]).to_numpy()\n",
    "    \n",
    "    track_ids = set(cluster_df[\"track_id\"].values)\n",
    "\n",
    "    discard_set = discard_sets[cluster_id]\n",
    "    discard_set.summarize_points(cluster_features_mtx, track_ids)\n",
    "\n",
    "small_features_pd.groupby(\"cluster\").apply(summarize_cluster_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup functions and thresholds necessary for BFR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold in terms of standard deviations away from centroid, in each dimension\n",
    "cluster_distance_threshold_standard_deviations = 1\n",
    "cluster_distance_threshold = ((cluster_distance_threshold_standard_deviations**2) * dimensions) ** 0.5\n",
    "\n",
    "# TODO: choose threshold and justify decision\n",
    "compression_set_merge_variance_threshold = float('-inf')\n",
    "\n",
    "dbscan_eps = 0.5    # TODO: parameter?\n",
    "\n",
    "assert len(k_centroids) == n_clusters, \"The number of clusters does not coincide with the number of random centroids!\"\n",
    "\n",
    "# def mahalanobis_distance_np(x: npt.NDArray[np.float32], s: SummarizedCluster) -> float:\n",
    "#     return np.sqrt(np.sum(((x - s.centroid()) / s.standard_deviation())**2))\n",
    "\n",
    "def mahalanobis_distance_pd(x: pd.DataFrame, s: SummarizedCluster) -> pd.Series:\n",
    "    return (((x - s.centroid()) / s.standard_deviation())**2).sum(axis=1) ** 0.5\n",
    "\n",
    "# @F.udf(returnType=FloatType())\n",
    "# def closest_mahalanobis_distance_cluster(*features: float):\n",
    "#     x = np.array(features)\n",
    "#     closest_cluster_distance, closest_cluster = min((mahalanobis_distance_np(x, d), d.id_) for d in discard_sets)\n",
    "#     return closest_cluster if closest_cluster_distance < cluster_distance_threshold else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "features_columns = set(features_df.columns) - {\"track_id\"}\n",
    "split_weights = [1.0] * (total_rows // rows_per_iteration)\n",
    "\n",
    "features_without_small_df = (features_df\n",
    "    .join(small_tracks_df, \"track_id\", \"left\")\n",
    "    .filter(F.col(\"set-subset\").isNull())\n",
    "    .select(features_df.columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(small_features_pd.drop(columns=[\"track_id\", \"cluster\"]) + np.arange(518).reshape((518,)) * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_points_pd = small_features_pd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = discard_sets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(d.variance() < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "526.9983**2 / 527**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "526.9966 / 527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d.sum_[104], d.sumsq_[104])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 518)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_points_pd.iloc[:,1:519].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_points_pd: pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"cluster_distance_\"\n",
    "cluster_distance_columns = [f\"{prefix}{i}\" for i in range(n_clusters)]\n",
    "loaded_points_pd[cluster_distance_columns] = np.ones((loaded_points_pd.shape[0], n_clusters))\n",
    "\n",
    "for i, discard_set in enumerate(discard_sets):\n",
    "    loaded_points_pd[f\"cluster_distance_{i}\"] = mahalanobis_distance_pd(loaded_points_pd.iloc[:, 1:519], discard_set)\n",
    "loaded_points_pd[\"min_cluster_distance\"] = loaded_points_pd[cluster_distance_columns].min(axis=1)\n",
    "loaded_points_pd[\"cluster_id\"] = loaded_points_pd[cluster_distance_columns].idxmin(axis=1).str.slice(start=len(prefix)).astype(np.int32)\n",
    "\n",
    "loaded_points_pd.drop(columns=cluster_distance_columns, inplace=True)\n",
    "\n",
    "# Don't consider the points that surpass the threshold for the discard sets\n",
    "loaded_points_pd.loc[loaded_points_pd[\"min_cluster_distance\"] >= cluster_distance_threshold - 4, \"cluster_id\"] = -1\n",
    "loaded_points_pd.drop(columns=[\"min_cluster_distance\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>chroma_cens-kurtosis-01</th>\n",
       "      <th>chroma_cens-kurtosis-02</th>\n",
       "      <th>chroma_cens-kurtosis-03</th>\n",
       "      <th>chroma_cens-kurtosis-04</th>\n",
       "      <th>chroma_cens-kurtosis-05</th>\n",
       "      <th>chroma_cens-kurtosis-06</th>\n",
       "      <th>chroma_cens-kurtosis-07</th>\n",
       "      <th>chroma_cens-kurtosis-08</th>\n",
       "      <th>chroma_cens-kurtosis-09</th>\n",
       "      <th>...</th>\n",
       "      <th>zcr-kurtosis-01</th>\n",
       "      <th>zcr-max-01</th>\n",
       "      <th>zcr-mean-01</th>\n",
       "      <th>zcr-median-01</th>\n",
       "      <th>zcr-min-01</th>\n",
       "      <th>zcr-skew-01</th>\n",
       "      <th>zcr-std-01</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_tmp</th>\n",
       "      <th>cluster_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>7.180653</td>\n",
       "      <td>5.230309</td>\n",
       "      <td>0.249321</td>\n",
       "      <td>1.347620</td>\n",
       "      <td>1.482478</td>\n",
       "      <td>0.531371</td>\n",
       "      <td>1.481593</td>\n",
       "      <td>2.691455</td>\n",
       "      <td>0.866868</td>\n",
       "      <td>...</td>\n",
       "      <td>5.758890</td>\n",
       "      <td>0.459473</td>\n",
       "      <td>0.085629</td>\n",
       "      <td>0.071289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.089872</td>\n",
       "      <td>0.061448</td>\n",
       "      <td>20.204036</td>\n",
       "      <td>21.174570</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.527563</td>\n",
       "      <td>-0.077654</td>\n",
       "      <td>-0.279610</td>\n",
       "      <td>0.685883</td>\n",
       "      <td>1.937570</td>\n",
       "      <td>0.880839</td>\n",
       "      <td>-0.923192</td>\n",
       "      <td>-0.927232</td>\n",
       "      <td>0.666617</td>\n",
       "      <td>...</td>\n",
       "      <td>6.808415</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.053114</td>\n",
       "      <td>0.041504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.193303</td>\n",
       "      <td>0.044861</td>\n",
       "      <td>17.233454</td>\n",
       "      <td>17.233454</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>3.702245</td>\n",
       "      <td>-0.291193</td>\n",
       "      <td>2.196742</td>\n",
       "      <td>-0.234449</td>\n",
       "      <td>1.367364</td>\n",
       "      <td>0.998411</td>\n",
       "      <td>1.770694</td>\n",
       "      <td>1.604566</td>\n",
       "      <td>0.521217</td>\n",
       "      <td>...</td>\n",
       "      <td>21.434212</td>\n",
       "      <td>0.452148</td>\n",
       "      <td>0.077515</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.542325</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>21.125072</td>\n",
       "      <td>22.857894</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140</td>\n",
       "      <td>0.533579</td>\n",
       "      <td>-0.623885</td>\n",
       "      <td>-1.086205</td>\n",
       "      <td>-1.081079</td>\n",
       "      <td>-0.765151</td>\n",
       "      <td>-0.072282</td>\n",
       "      <td>-0.882913</td>\n",
       "      <td>-0.582376</td>\n",
       "      <td>-0.884749</td>\n",
       "      <td>...</td>\n",
       "      <td>11.052547</td>\n",
       "      <td>0.379395</td>\n",
       "      <td>0.052379</td>\n",
       "      <td>0.036621</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>3.143968</td>\n",
       "      <td>0.057712</td>\n",
       "      <td>15.630419</td>\n",
       "      <td>18.973435</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141</td>\n",
       "      <td>0.172898</td>\n",
       "      <td>-0.284804</td>\n",
       "      <td>-1.169662</td>\n",
       "      <td>-1.062855</td>\n",
       "      <td>-0.706868</td>\n",
       "      <td>-0.708281</td>\n",
       "      <td>-0.204884</td>\n",
       "      <td>0.023624</td>\n",
       "      <td>-0.642770</td>\n",
       "      <td>...</td>\n",
       "      <td>32.994659</td>\n",
       "      <td>0.415527</td>\n",
       "      <td>0.040267</td>\n",
       "      <td>0.034668</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>4.204097</td>\n",
       "      <td>0.028665</td>\n",
       "      <td>15.125494</td>\n",
       "      <td>24.417967</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>154413</td>\n",
       "      <td>-0.214509</td>\n",
       "      <td>-1.130469</td>\n",
       "      <td>0.718534</td>\n",
       "      <td>-0.368448</td>\n",
       "      <td>-0.147830</td>\n",
       "      <td>-0.099409</td>\n",
       "      <td>-1.325709</td>\n",
       "      <td>-0.105248</td>\n",
       "      <td>-1.363881</td>\n",
       "      <td>...</td>\n",
       "      <td>25.368595</td>\n",
       "      <td>0.323242</td>\n",
       "      <td>0.024532</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>3.736646</td>\n",
       "      <td>0.023821</td>\n",
       "      <td>22.169065</td>\n",
       "      <td>27.847208</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>154414</td>\n",
       "      <td>-0.487371</td>\n",
       "      <td>-0.923754</td>\n",
       "      <td>-0.283099</td>\n",
       "      <td>-0.435221</td>\n",
       "      <td>-1.137329</td>\n",
       "      <td>-0.798039</td>\n",
       "      <td>-0.258168</td>\n",
       "      <td>1.004049</td>\n",
       "      <td>-0.499121</td>\n",
       "      <td>...</td>\n",
       "      <td>21.276468</td>\n",
       "      <td>0.511230</td>\n",
       "      <td>0.046116</td>\n",
       "      <td>0.033691</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>3.997052</td>\n",
       "      <td>0.045733</td>\n",
       "      <td>17.251695</td>\n",
       "      <td>24.705431</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>155066</td>\n",
       "      <td>0.044216</td>\n",
       "      <td>-0.300441</td>\n",
       "      <td>-0.217022</td>\n",
       "      <td>-0.356106</td>\n",
       "      <td>-1.085789</td>\n",
       "      <td>-1.185135</td>\n",
       "      <td>-0.655948</td>\n",
       "      <td>-1.517006</td>\n",
       "      <td>-0.490595</td>\n",
       "      <td>...</td>\n",
       "      <td>116.044044</td>\n",
       "      <td>0.554199</td>\n",
       "      <td>0.016058</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.688635</td>\n",
       "      <td>0.030787</td>\n",
       "      <td>21.273305</td>\n",
       "      <td>33.545784</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8001 rows × 522 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     track_id  chroma_cens-kurtosis-01  chroma_cens-kurtosis-02   \n",
       "0           2                 7.180653                 5.230309  \\\n",
       "1           5                 0.527563                -0.077654   \n",
       "2          10                 3.702245                -0.291193   \n",
       "3         140                 0.533579                -0.623885   \n",
       "4         141                 0.172898                -0.284804   \n",
       "...       ...                      ...                      ...   \n",
       "7996     None                      NaN                      NaN   \n",
       "7997   154413                -0.214509                -1.130469   \n",
       "7998   154414                -0.487371                -0.923754   \n",
       "7999   155066                 0.044216                -0.300441   \n",
       "8000      0.0                 0.000000                 0.000000   \n",
       "\n",
       "      chroma_cens-kurtosis-03  chroma_cens-kurtosis-04   \n",
       "0                    0.249321                 1.347620  \\\n",
       "1                   -0.279610                 0.685883   \n",
       "2                    2.196742                -0.234449   \n",
       "3                   -1.086205                -1.081079   \n",
       "4                   -1.169662                -1.062855   \n",
       "...                       ...                      ...   \n",
       "7996                      NaN                      NaN   \n",
       "7997                 0.718534                -0.368448   \n",
       "7998                -0.283099                -0.435221   \n",
       "7999                -0.217022                -0.356106   \n",
       "8000                 0.000000                 0.000000   \n",
       "\n",
       "      chroma_cens-kurtosis-05  chroma_cens-kurtosis-06   \n",
       "0                    1.482478                 0.531371  \\\n",
       "1                    1.937570                 0.880839   \n",
       "2                    1.367364                 0.998411   \n",
       "3                   -0.765151                -0.072282   \n",
       "4                   -0.706868                -0.708281   \n",
       "...                       ...                      ...   \n",
       "7996                      NaN                      NaN   \n",
       "7997                -0.147830                -0.099409   \n",
       "7998                -1.137329                -0.798039   \n",
       "7999                -1.085789                -1.185135   \n",
       "8000                 0.000000                 0.000000   \n",
       "\n",
       "      chroma_cens-kurtosis-07  chroma_cens-kurtosis-08   \n",
       "0                    1.481593                 2.691455  \\\n",
       "1                   -0.923192                -0.927232   \n",
       "2                    1.770694                 1.604566   \n",
       "3                   -0.882913                -0.582376   \n",
       "4                   -0.204884                 0.023624   \n",
       "...                       ...                      ...   \n",
       "7996                      NaN                      NaN   \n",
       "7997                -1.325709                -0.105248   \n",
       "7998                -0.258168                 1.004049   \n",
       "7999                -0.655948                -1.517006   \n",
       "8000                 0.000000                 0.000000   \n",
       "\n",
       "      chroma_cens-kurtosis-09  ...  zcr-kurtosis-01  zcr-max-01  zcr-mean-01   \n",
       "0                    0.866868  ...         5.758890    0.459473     0.085629  \\\n",
       "1                    0.666617  ...         6.808415    0.375000     0.053114   \n",
       "2                    0.521217  ...        21.434212    0.452148     0.077515   \n",
       "3                   -0.884749  ...        11.052547    0.379395     0.052379   \n",
       "4                   -0.642770  ...        32.994659    0.415527     0.040267   \n",
       "...                       ...  ...              ...         ...          ...   \n",
       "7996                      NaN  ...              NaN         NaN          NaN   \n",
       "7997                -1.363881  ...        25.368595    0.323242     0.024532   \n",
       "7998                -0.499121  ...        21.276468    0.511230     0.046116   \n",
       "7999                -0.490595  ...       116.044044    0.554199     0.016058   \n",
       "8000                 0.000000  ...         0.000000    0.000000     0.000000   \n",
       "\n",
       "      zcr-median-01  zcr-min-01  zcr-skew-01  zcr-std-01    cluster   \n",
       "0          0.071289    0.000000     2.089872    0.061448  20.204036  \\\n",
       "1          0.041504    0.000000     2.193303    0.044861  17.233454   \n",
       "2          0.071777    0.000000     3.542325    0.040800  21.125072   \n",
       "3          0.036621    0.001953     3.143968    0.057712  15.630419   \n",
       "4          0.034668    0.002930     4.204097    0.028665  15.125494   \n",
       "...             ...         ...          ...         ...        ...   \n",
       "7996            NaN         NaN          NaN         NaN        NaN   \n",
       "7997       0.018066    0.000977     3.736646    0.023821  22.169065   \n",
       "7998       0.033691    0.003418     3.997052    0.045733  17.251695   \n",
       "7999       0.009766    0.000000     9.688635    0.030787  21.273305   \n",
       "8000       0.000000    0.000000     0.000000    0.000000   0.000000   \n",
       "\n",
       "      cluster_tmp  cluster_id  \n",
       "0       21.174570        -1.0  \n",
       "1       17.233454         8.0  \n",
       "2       22.857894        -1.0  \n",
       "3       18.973435         3.0  \n",
       "4       24.417967         1.0  \n",
       "...           ...         ...  \n",
       "7996          NaN         0.0  \n",
       "7997    27.847208        -1.0  \n",
       "7998    24.705431         3.0  \n",
       "7999    33.545784        -1.0  \n",
       "8000     0.000000         0.0  \n",
       "\n",
       "[8001 rows x 522 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([loaded_points_pd, pd.DataFrame(np.zeros((1, 522)), columns=loaded_points_pd.columns)], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"cluster_distance_\"\n",
    "cluster_distance_columns = [f\"{prefix}{i}\" for i in range(n_clusters)]\n",
    "\n",
    "def print_progress(progress: float, message: str):\n",
    "    print(f\"[{progress:3%}] {message:100}\", end=\"\\r\")\n",
    "\n",
    "print_progress(0, \"Initialized BFR\")\n",
    "for split_idx, loaded_points_df in enumerate(features_without_small_df.randomSplit(split_weights, seed=seed_random)):\n",
    "    progress = split_idx / len(split_weights)\n",
    "    \n",
    "    print_progress(progress, \"Clustering with the Mahalanobis distance...\")\n",
    "\n",
    "    loaded_points_pd[cluster_distance_columns] = np.ones((loaded_points_pd.shape[0], n_clusters))\n",
    "\n",
    "    for i, discard_set in enumerate(discard_sets):\n",
    "        loaded_points_pd[f\"cluster_distance_{i}\"] = mahalanobis_distance_pd(loaded_points_pd.iloc[:, 1:519], discard_set)\n",
    "    loaded_points_pd[\"min_cluster_distance\"] = loaded_points_pd[cluster_distance_columns].min(axis=1)\n",
    "    loaded_points_pd[\"cluster_id\"] = loaded_points_pd[cluster_distance_columns].idxmin(axis=1).str.slice(start=len(prefix)).astype(np.int32)\n",
    "\n",
    "    loaded_points_pd.drop(columns=cluster_distance_columns, inplace=True)\n",
    "\n",
    "    # Don't consider the points that surpass the threshold for the discard sets\n",
    "    loaded_points_pd.loc[loaded_points_pd[\"min_cluster_distance\"] >= cluster_distance_threshold - 4, \"cluster_id\"] = -1\n",
    "    loaded_points_pd.drop(columns=[\"min_cluster_distance\"], inplace=True)\n",
    "\n",
    "    # cluster_mapping = loaded_points_df \\\n",
    "    #     .withColumn(\"cluster\", closest_mahalanobis_distance_cluster(*features_columns)) \\\n",
    "    #     .groupby(\"cluster\") \\\n",
    "    #     .agg({\n",
    "    #         \"track_ids\": F.collect_list(\"track_id\"),\n",
    "    #         \"features_list\": F.collect_list(F.array(*features_columns))\n",
    "    #     }).collect()\n",
    "    \n",
    "    print_progress(progress, \"Calculated and collected Mahalanobis distances\")\n",
    "\n",
    "    for cluster_id, cluster_df in loaded_points_pd.groupby(\"cluster\"):\n",
    "        track_ids = cluster_df[\"track_id\"]\n",
    "        features_list = cluster_df[:, 1:519]\n",
    "\n",
    "        print_progress(progress, f\"Evaluated cluster {cluster_id}...\")\n",
    "\n",
    "        # Step 3 - check which points go to the discard sets\n",
    "        if cluster_id != -1:\n",
    "            discard_set = discard_sets[cluster_id]\n",
    "            for track_id, features in zip(track_ids, features_list):\n",
    "                discard_set.summarize(np.array(features), track_id)\n",
    "        \n",
    "        # Step 4 - check which points go to the compression sets or the retained set\n",
    "        else:\n",
    "            matrix_to_cluster = np.array(features_list + retained_set)\n",
    "\n",
    "            # Use same distance as above\n",
    "            clusterer = DBSCAN(eps=dbscan_eps, metric='euclidean')\n",
    "            clusterer.fit(matrix_to_cluster)\n",
    "\n",
    "            centroid_calculator = NearestCentroid()\n",
    "            centroid_calculator.fit(matrix_to_cluster, clusterer.labels_)\n",
    "\n",
    "            retained_set.clear()\n",
    "\n",
    "            # Create compression sets\n",
    "            compression_sets_temp = [SummarizedCluster(dimensions, None) for _ in centroid_calculator.centroids_]\n",
    "\n",
    "            for point_idx in range(matrix_to_cluster.shape[0]):\n",
    "                cluster_id = clusterer.labels_[point_idx]\n",
    "                point = matrix_to_cluster[point_idx, :]\n",
    "                \n",
    "                if cluster_id == -1:\n",
    "                    retained_set.append(point)\n",
    "                else:\n",
    "                    compression_sets_temp[cluster_id].summarize(point)\n",
    "            \n",
    "            compression_sets.extend(compression_sets_temp)\n",
    "        \n",
    "    print_progress(progress, f\"Finished evaluating clusters (compression sets: {len(compression_sets)}, retained set: {len(retained_set)})\")\n",
    "    \n",
    "    # Step 5 - merge compression sets\n",
    "    compressing = True\n",
    "    while compressing:\n",
    "        compressing = False\n",
    "        merged_compression_sets = []\n",
    "        compression_set_idxs_to_remove = set()\n",
    "\n",
    "        for (idx_1, compression_set_1), (idx_2, compression_set_2) in combinations(enumerate(compression_sets), 2):\n",
    "            if idx_1 in compression_set_idxs_to_remove or idx_2 in compression_set_idxs_to_remove:\n",
    "                continue\n",
    "\n",
    "            merged_compression_set = compression_set_1 + compression_set_2\n",
    "            if merged_compression_set.variance() < compression_set_merge_variance_threshold:\n",
    "                merged_compression_sets.append(merged_compression_set)\n",
    "                compression_set_idxs_to_remove.add(idx_1)\n",
    "                compression_set_idxs_to_remove.add(idx_2)\n",
    "                compressing = True\n",
    "        \n",
    "        compression_sets = [cs for i, cs in enumerate(compression_sets) if i not in compression_set_idxs_to_remove]\n",
    "        compression_sets.extend(merged_compression_sets)\n",
    "\n",
    "        print_progress(progress, f\"Completed one compression (compression sets: {len(compression_sets)})\")\n",
    "    \n",
    "    print_progress((split_idx + 1) / len(split_weights), f\"Finished one of the splits (compression sets: {len(compression_sets)}, retained set: {len(retained_set)})\")\n",
    "\n",
    "# Step 6 - merge CS and RS into DS (but we won't yet)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFR cluster visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
