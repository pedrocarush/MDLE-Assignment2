{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import Broadcast\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType\n",
    "from itertools import combinations\n",
    "from typing import Iterable, Any, List, Set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from typing import Union"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/17 12:08:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('exercise1') \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/17 12:08:59 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "|track_id|album-comments| album-date_created|album-date_released|album-engineer|album-favorites|album-id|   album-information|album-listens|      album-producer|album-tags|         album-title|album-tracks|      album-type|artist-active_year_begin|artist-active_year_end|artist-associated_labels|          artist-bio|artist-comments|artist-date_created|artist-favorites|artist-id|artist-latitude|   artist-location|artist-longitude|      artist-members|         artist-name|artist-related_projects|         artist-tags|      artist-website|artist-wikipedia_page|set-split|set-subset|track-bit_rate|track-comments|track-composer| track-date_created|track-date_recorded|track-duration|track-favorites|track-genre_top|track-genres| track-genres_all|   track-information|track-interest|track-language_code|       track-license|track-listens|track-lyricist|track-number|track-publisher|track-tags|         track-title|\n",
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "|       2|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|     small|        256000|             0|          null|2008-11-26 01:48:12|2008-11-26 00:00:00|           168|              2|        Hip-Hop|        [21]|             [21]|                null|          4656|                 en|Attribution-NonCo...|         1293|          null|           3|           null|        []|                Food|\n",
      "|       3|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|    medium|        256000|             0|          null|2008-11-26 01:48:14|2008-11-26 00:00:00|           237|              1|        Hip-Hop|        [21]|             [21]|                null|          1470|                 en|Attribution-NonCo...|          514|          null|           4|           null|        []|        Electric Ave|\n",
      "|       5|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|     small|        256000|             0|          null|2008-11-26 01:48:20|2008-11-26 00:00:00|           206|              6|        Hip-Hop|        [21]|             [21]|                null|          1933|                 en|Attribution-NonCo...|         1151|          null|           6|           null|        []|          This World|\n",
      "|      10|             0|2008-11-26 01:45:08|2008-02-06 00:00:00|          null|              4|       6|                null|        47632|                null|        []|   Constant Hitmaker|           2|           Album|                    null|                  null|    Mexican Summer, R...|<p><span style=\"f...|              3|2008-11-26 01:42:55|              74|        6|           null|              null|            null|Kurt Vile, the Vi...|           Kurt Vile|                   null|['philly', 'kurt ...| http://kurtvile.com|                 null| training|     small|        192000|             0|     Kurt Vile|2008-11-25 17:49:06|2008-11-26 00:00:00|           161|            178|            Pop|        [10]|             [10]|                null|         54881|                 en|Attribution-NonCo...|        50135|          null|           1|           null|        []|             Freeway|\n",
      "|      20|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:48:56|2008-01-01 00:00:00|           311|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           978|                 en|Attribution-NonCo...|          361|          null|           3|           null|        []|     Spiritual Level|\n",
      "|      26|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:05|2008-01-01 00:00:00|           181|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|          1060|                 en|Attribution-NonCo...|          193|          null|           4|           null|        []| Where is your Love?|\n",
      "|      30|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:11|2008-01-01 00:00:00|           174|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           718|                 en|Attribution-NonCo...|          612|          null|           5|           null|        []|           Too Happy|\n",
      "|      46|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:53|2008-01-01 00:00:00|           104|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           252|                 en|Attribution-NonCo...|          171|          null|           8|           null|        []|            Yosemite|\n",
      "|      48|             0|2008-11-26 01:45:05|2009-01-06 00:00:00|          null|              2|       4|<p> \"spiritual so...|         2710|                null|        []|               Niris|          13|           Album|     1990-01-01 00:00:00|   2011-01-01 00:00:00|                    null|<p>Songs written ...|              2|2008-11-26 01:42:52|              10|        4|      51.895927|Colchester England|        0.891874|        Nicky Cook\\n|          Nicky Cook|                   null|['instrumentals',...|                null|                 null| training|     large|        256000|             0|          null|2008-11-26 01:49:56|2008-01-01 00:00:00|           205|              0|           null|   [76, 103]|[17, 10, 76, 103]|                null|           247|                 en|Attribution-NonCo...|          173|          null|           9|           null|        []|      Light of Light|\n",
      "|     134|             0|2008-11-26 01:44:45|2009-01-05 00:00:00|          null|              4|       1|             <p></p>|         6073|                null|        []|AWOL - A Way Of Life|           7|           Album|     2006-01-01 00:00:00|                  null|                    null|<p>A Way Of Life,...|              0|2008-11-26 01:42:32|               9|        1|     40.0583238|        New Jersey|     -74.4056612|Sajje Morocco,Bro...|                AWOL|   The list of past ...|            ['awol']|http://www.Azilli...|                 null| training|    medium|        256000|             0|          null|2008-11-26 01:43:19|2008-11-26 00:00:00|           207|              3|        Hip-Hop|        [21]|             [21]|                null|          1126|                 en|Attribution-NonCo...|          943|          null|           5|           null|        []|        Street Music|\n",
      "|     135|             1|2008-11-26 01:49:19|2009-01-07 00:00:00|          null|              0|      58|<p>A couple of un...|         3331|                null|        []|                 mp3|           4|   Single Tracks|                    null|                  null|                    null|                null|              1|2008-11-26 01:47:07|               0|       52|           null|              null|            null|                null|            Abominog|                   null|        ['abominog']|http://myspace.co...|                 null| training|     large|        256000|             1|          null|2008-11-26 01:43:26|2008-11-26 00:00:00|           837|              0|           Rock|    [45, 58]|     [58, 12, 45]|                null|          2484|                 en|Attribution-NonCo...|         1832|          null|           0|           null|        []|        Father's Day|\n",
      "|     136|             1|2008-11-26 01:49:19|2009-01-07 00:00:00|          null|              0|      58|<p>A couple of un...|         3331|                null|        []|                 mp3|           4|   Single Tracks|                    null|                  null|                    null|                null|              1|2008-11-26 01:47:07|               0|       52|           null|              null|            null|                null|            Abominog|                   null|        ['abominog']|http://myspace.co...|                 null| training|    medium|        256000|             1|          null|2008-11-26 01:43:35|2008-11-26 00:00:00|           509|              0|           Rock|    [45, 58]|     [58, 12, 45]|                null|          1948|                 en|Attribution-NonCo...|         1498|          null|           0|           null|        []|Peel Back The Mou...|\n",
      "|     137|             1|2008-11-26 01:49:35|2006-12-01 00:00:00|          null|              2|      59|<p>Here's the pro...|         1681|                null| ['lafms']|        Live at LACE|           2|Live Performance|     1978-01-01 00:00:00|   1998-01-01 00:00:00|    Los Angeles Free ...|<p>Airway was a m...|              0|2008-11-26 01:47:22|               5|       53|     34.0522342|   Los Angeles, CA|    -118.2436849|Rick Potts, Juan ...|              Airway|   Los Angeles Free ...|          ['airway']|http://www.lafms....| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:43:42|1978-04-27 00:00:00|          1233|              2|   Experimental|     [1, 32]|      [32, 1, 38]|<p>Recorded live ...|          2559|                 en|Attribution-NonCo...|         1278|          null|           1|           null| ['lafms']|              Side A|\n",
      "|     138|             1|2008-11-26 01:49:35|2006-12-01 00:00:00|          null|              2|      59|<p>Here's the pro...|         1681|                null| ['lafms']|        Live at LACE|           2|Live Performance|     1978-01-01 00:00:00|   1998-01-01 00:00:00|    Los Angeles Free ...|<p>Airway was a m...|              0|2008-11-26 01:47:22|               5|       53|     34.0522342|   Los Angeles, CA|    -118.2436849|Rick Potts, Juan ...|              Airway|   Los Angeles Free ...|          ['airway']|http://www.lafms....| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:43:56|1978-04-27 00:00:00|          1231|              2|   Experimental|     [1, 32]|      [32, 1, 38]|<p>Recorded live ...|          1909|                 en|Attribution-NonCo...|          489|          null|           2|           null| ['lafms']|              Side B|\n",
      "|     139|             0|2008-11-26 01:49:57|2009-01-16 00:00:00|          null|              1|      60|<p>A full ensambl...|         1304|                null|        []|Every Man For Him...|           2|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|    medium|        128000|             0|          null|2008-11-26 01:44:05|2008-11-26 00:00:00|           296|              3|           Folk|        [17]|             [17]|                null|           702|                 en|Attribution-Nonco...|          582|          null|           2|           null|        []|            CandyAss|\n",
      "|     140|             1|2008-11-26 01:49:59|2007-05-22 00:00:00|          null|              1|      61|<p>Alec K. Redfea...|         1300|Alec K. Refearn, ...|        []|      The Blind Spot|           1|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     small|        128000|             0|          null|2008-11-26 01:44:07|2008-11-26 00:00:00|           253|              5|           Folk|        [17]|             [17]|                null|          1593|                 en|Attribution-Nonco...|         1299|          null|           2|           null|        []|  Queen Of The Wires|\n",
      "|     141|             0|2008-11-26 01:49:57|2009-01-16 00:00:00|          null|              1|      60|<p>A full ensambl...|         1304|                null|        []|Every Man For Him...|           2|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     small|        128000|             0|          null|2008-11-26 01:44:10|2008-11-26 00:00:00|           182|              1|           Folk|        [17]|             [17]|                null|           839|                 en|Attribution-Nonco...|          725|          null|           4|           null|        []|                Ohio|\n",
      "|     142|             0|2008-11-26 01:50:03|2005-01-25 00:00:00|          null|              1|      62|<p>Recorded at So...|          845|                null|        []|      The Quiet Room|           1|           Album|     1999-01-01 00:00:00|                  null|                    null|<p>The Eyesores o...|              0|2008-11-26 01:47:44|              11|       54|     41.8239891|    Providence, RI|     -71.4128343|                null|Alec K. Redfearn ...|   Haldols, Amoebic ...|['alec k redfearn...|http://www.aleckr...| http://en.wikiped...| training|     large|        128000|             0|          null|2008-11-26 01:44:11|2008-11-26 00:00:00|           470|              6|           Folk|        [17]|             [17]|                null|          1223|                 en|Attribution-Nonco...|          848|          null|           5|           null|        []|Punjabi Watery Grave|\n",
      "|     144|             0|2008-11-26 01:50:07|2009-01-06 00:00:00|          null|              0|      64|<p><em>A</em>ltho...|         2014|        Tom Buckland|        []|          Amoebiasis|           0|           Album|     1992-01-01 00:00:00|   1999-01-01 00:00:00|                    null|<p>The obscure, b...|              1|2008-11-26 01:47:54|               7|       56|     41.8239891|    Providence, RI|     -71.4128343|Alec K. Redfearn\\...|    Amoebic Ensemble|             Septimania|['providence', 'a...|http://www.myspac...| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:44:15|1998-11-26 00:00:00|            82|              1|           Jazz|         [4]|              [4]|                null|          1146|                 en|Attribution-Nonco...|         1143|          null|           1|           null|        []|             Wire Up|\n",
      "|     145|             0|2008-11-26 01:50:07|2009-01-06 00:00:00|          null|              0|      64|<p><em>A</em>ltho...|         2014|        Tom Buckland|        []|          Amoebiasis|           0|           Album|     1992-01-01 00:00:00|   1999-01-01 00:00:00|                    null|<p>The obscure, b...|              1|2008-11-26 01:47:54|               7|       56|     41.8239891|    Providence, RI|     -71.4128343|Alec K. Redfearn\\...|    Amoebic Ensemble|             Septimania|['providence', 'a...|http://www.myspac...| http://en.wikiped...| training|     large|        256000|             0|          null|2008-11-26 01:44:18|1998-11-26 00:00:00|           326|              1|           Jazz|         [4]|              [4]|                null|           968|                 en|Attribution-Nonco...|          883|          null|           3|           null|        []|          Amoebiasis|\n",
      "+--------+--------------+-------------------+-------------------+--------------+---------------+--------+--------------------+-------------+--------------------+----------+--------------------+------------+----------------+------------------------+----------------------+------------------------+--------------------+---------------+-------------------+----------------+---------+---------------+------------------+----------------+--------------------+--------------------+-----------------------+--------------------+--------------------+---------------------+---------+----------+--------------+--------------+--------------+-------------------+-------------------+--------------+---------------+---------------+------------+-----------------+--------------------+--------------+-------------------+--------------------+-------------+--------------+------------+---------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tracks_df = (spark.read\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"quote\", '\"')\n",
    "    .option(\"escape\", '\"')\n",
    "    .csv('data/tracks.csv')threshold\n",
    ")\n",
    "\n",
    "# rename columns with row values from first row to second row\n",
    "column_categories = list(zip(*tracks_df.take(2)))\n",
    "columns = tracks_df.columns\n",
    "tracks_df = tracks_df.select(F.col(columns[0]).alias('track_id'),\n",
    "    *(F.col(column).alias(\"-\".join(map(str, categories)))\n",
    "    for column, categories in zip(columns[1:], column_categories[1:]))\n",
    ")\n",
    "\n",
    "tracks_df = (tracks_df\n",
    "    .filter(F.col(\"track_id\").isNotNull()) \n",
    "    .filter(F.col(\"track_id\") != \"track_id\")\n",
    ")\n",
    "\n",
    "tracks_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: feature selection? not needed necessarily but...\n",
    "features_df = (spark.read\n",
    "    .csv('data/features.csv')\n",
    ")\n",
    "\n",
    "# rename columns with row values from first row to second row\n",
    "column_categories = list(zip(*features_df.take(3)))\n",
    "columns = features_df.columns\n",
    "# TODO: DoubleType instead of FloatType?\n",
    "features_df = features_df.select(F.col(columns[0]).alias('track_id'),\n",
    "    *(F.col(column).cast(FloatType()).alias(\"-\".join(map(str, categories)))\n",
    "    for column, categories in zip(columns[1:], column_categories[1:]))\n",
    ")\n",
    "\n",
    "features_df = (features_df\n",
    "    .filter(F.col(\"track_id\") != \"feature\")\n",
    "    .filter(F.col(\"track_id\") != \"statistics\")\n",
    "    .filter(F.col(\"track_id\") != \"number\")\n",
    "    .filter(F.col(\"track_id\") != \"track_id\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative clustering (in-memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "small_tracks_df = tracks_df.filter(F.col(\"set-subset\") == \"small\")\n",
    "small_features_df = (features_df\n",
    "    .join(small_tracks_df, \"track_id\", \"left\")\n",
    "    .filter(F.col(\"set-subset\").isNotNull())\n",
    "    .select(features_df.columns)\n",
    ")\n",
    "\n",
    "music_features_pd = (small_features_df\n",
    "    .drop(\"track_id\")\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the metrics (radius, diameter, density_r, density_d) for each cluster\n",
    "def calculate_metrics(pd_df,centroids):\n",
    "    \n",
    "    cluster = pd_df[\"cluster\"].values[0]\n",
    "    \n",
    "    metrics = pd.DataFrame({'radius': [0], 'diameter': [0],'density_r': [0],'density_d': [0]}, columns=['radius', 'diameter','density_r','density_d'])\n",
    "\n",
    "    centroid = centroids[cluster].reshape(1,-1)\n",
    "\n",
    "    matrix = pd_df.drop(columns=['cluster']).to_numpy()\n",
    "    \n",
    "    matrix_radius = np.sqrt(np.sum((matrix - centroid)**2, axis=1))\n",
    "\n",
    "    metrics.loc[0,'radius'] = np.max(matrix_radius)\n",
    "\n",
    "    # calculater density with radius\n",
    "    metrics.loc[0,'density_r'] = len(pd_df) / metrics.loc[0,'radius']**2\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "\n",
    "        matrix_diameter = np.sqrt(np.sum((matrix[i:,:] - matrix[i,:])**2, axis=1))\n",
    "\n",
    "        max_diameter = np.max(matrix_diameter)\n",
    "        if max_diameter > metrics.loc[0,'diameter']:\n",
    "            metrics.loc[0,'diameter'] = max_diameter\n",
    "        \n",
    "    # calculater density with diameter\n",
    "    metrics.loc[0,'density_d'] = len(pd_df) / metrics.loc[0,'diameter']**2\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_clusters: 8, cluster_labels: [0 0 5 ... 1 3 3]\n",
      "n_clusters: 9, cluster_labels: [8 8 5 ... 1 3 3]\n",
      "n_clusters: 10, cluster_labels: [3 3 5 ... 0 1 1]\n",
      "n_clusters: 11, cluster_labels: [3 3 2 ... 5 1 1]\n",
      "n_clusters: 12, cluster_labels: [3 3 2 ... 5 0 0]\n",
      "n_clusters: 13, cluster_labels: [ 1  1  2 ...  5  3 12]\n",
      "n_clusters: 14, cluster_labels: [ 0  0  2 ...  5  1 12]\n",
      "n_clusters: 15, cluster_labels: [14 14  2 ...  5  0 12]\n",
      "n_clusters: 16, cluster_labels: [14 14  2 ...  5 15 12]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "metrics_pd_array = []\n",
    "\n",
    "music_features_pd[\"cluster\"] = np.zeros((len(music_features_pd), 1), dtype=np.int32)\n",
    "\n",
    "if os.path.exists(\"./results/metrics_pd_array_pickle.pkl\"):\n",
    "    with open(\"metrics_pd_array_pickle.pkl\",'rb') as f:\n",
    "        metrics_pd_array = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    # i = 8 until 16\n",
    "    for i in range(8, 17):\n",
    "        n_clusters = i\n",
    "        clusterer = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "        clusterer.fit(music_features_pd.drop(\"cluster\", axis=1))\n",
    "        print(f\"n_clusters: {n_clusters}, cluster_labels: {clusterer.labels_}\")\n",
    "\n",
    "        # calculate centroids\n",
    "\n",
    "        centroid_calculator = NearestCentroid()\n",
    "\n",
    "        centroid_calculator.fit(music_features_pd.drop(\"cluster\",axis=1), clusterer.labels_)\n",
    "\n",
    "        music_features_pd[\"cluster\"] = clusterer.labels_\n",
    "        \n",
    "        metrics_pd_array.append(music_features_pd.groupby(\"cluster\").apply(calculate_metrics,centroid_calculator.centroids_))\n",
    "\n",
    "    # TODO: weird column at 0s without name??\n",
    "    with open(\"./results/metrics_pd_array_pickle.pkl\",'wb') as f:\n",
    "        pickle.dump(metrics_pd_array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: justify N-clusters choice, maybe create a colored matrix?\n",
    "for i in metrics_pd_array:\n",
    "    density_r_average = i[\"density_r\"].mean()\n",
    "    density_d_average = i[\"density_d\"].mean()\n",
    "    density_r_variance = i[\"density_r\"].var()\n",
    "    density_d_variance = i[\"density_d\"].var()\n",
    "\n",
    "    print(f\"Average density_r: {density_r_average}, Average density_d: {density_d_average}, Variance density_r: {density_r_variance}, Variance density_d: {density_d_variance}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFR Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: results can be: density of clusters, number of nodes in each cluster, etc., but not strictly necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "n_clusters = 9\n",
    "dimensions = len(features_df.columns) - 1   # don't consider 'track_id'\n",
    "\n",
    "max_memory_used_megabytes = 4000\n",
    "# Assumes all columns are floats/integers, and so therefore 4 bytes\n",
    "rows_per_iteration = max_memory_used_megabytes // (4 * len(features_df.columns))\n",
    "total_rows = features_df.count()\n",
    "\n",
    "seed_random = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# TODO: initialize K clusters/centroids with the small dataset that was processed\n",
    "\n",
    "k_centroids_ids = random.choices(range(0, total_rows), k=n_clusters)\n",
    "\n",
    "k_centroids = features_df.filter(F.col(\"track_id\").isin(k_centroids_ids)).drop(\"track_id\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k_centroids não é igual ao numero de clusters, ISTO DEVE SER PORQUE HÁ IDS QUE NÃO EXISTEM NO DATASET\n",
    "# !!! RESOLVER ISTO !!!\n",
    "len(k_centroids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import numpy.typing as npt\n",
    "\n",
    "@dataclasses.dataclass(init=False)\n",
    "class SummarizedCluster:\n",
    "    n:      int                     \n",
    "    sum_:   npt.NDArray[np.float32]\n",
    "    sumsq_: npt.NDArray[np.float32]\n",
    "    id_:    Union[int, None]\n",
    "\n",
    "    def __init__(self, dimensions: int, id_: int=None):\n",
    "        self.n = 0\n",
    "        self.sum_ = np.zeros((dimensions,), dtype=np.float32)\n",
    "        self.sumsq = np.zeros((dimensions,), dtype=np.float32)\n",
    "        self.id_ = id_\n",
    "    \n",
    "    def summarize(self, point: npt.NDArray[np.float32], track_id: int):\n",
    "        self.n += 1\n",
    "        self.sum_ += point\n",
    "        self.sumsq_ += np.pow(point, 2)\n",
    "\n",
    "    def centroid(self) -> npt.NDArray[np.float32]:\n",
    "        return self.sum_ / self.n\n",
    "\n",
    "    def variance(self) -> npt.NDArray[np.float32]:\n",
    "        return (self.sumsq_ / self.n) - np.pow(self.sum_ / self.n, 2)\n",
    "\n",
    "    def standard_deviation(self) -> npt.NDArray[np.float32]:\n",
    "        return np.sqrt(self.variance())\n",
    "\n",
    "    def __add__(self, other) -> 'SummarizedCluster':\n",
    "        if self.id_ is not None and self.other is not None and self.id_ != self.other:\n",
    "            raise ValueError(f\"Clusters {self} and {other} have different explicit ids ({self.id_} != {other.id_}).\")\n",
    "        res = SummarizedCluster(self.dimensions, )\n",
    "        res.n = self.n + other.n\n",
    "        res.sum_ = self.sum_ + other.sum_\n",
    "        res.sumsq_ = self.sumsq_ + other.sumsq_\n",
    "        res.id_ = self.id_ if self.id_ is not None else other.id_\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The number of clusters does not coincide with the number of random centroids!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m cluster_distance_threshold_standard_deviations \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      7\u001b[0m cluster_distance_threshold \u001b[39m=\u001b[39m ((cluster_distance_threshold_standard_deviations\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m dimensions) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(k_centroids) \u001b[39m==\u001b[39m n_clusters, \u001b[39m\"\u001b[39m\u001b[39mThe number of clusters does not coincide with the number of random centroids!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m centroid, discard_set \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(k_centroids, discard_sets):\n\u001b[1;32m     12\u001b[0m     discard_set\u001b[39m.\u001b[39msummarize(np\u001b[39m.\u001b[39marray(centroid))\n",
      "\u001b[0;31mAssertionError\u001b[0m: The number of clusters does not coincide with the number of random centroids!"
     ]
    }
   ],
   "source": [
    "discard_sets: List[SummarizedCluster] = [SummarizedCluster(dimensions, id_) for id_ in range(n_clusters)]\n",
    "compression_sets: List[SummarizedCluster] = []\n",
    "retained_set: List[npt.NDArray[np.float32]] = []\n",
    "\n",
    "# Threshold in terms of standard deviations away from centroid, in each dimension\n",
    "cluster_distance_threshold_standard_deviations = 1\n",
    "cluster_distance_threshold = ((cluster_distance_threshold_standard_deviations**2) * dimensions) ** 0.5\n",
    "\n",
    "assert len(k_centroids) == n_clusters, \"The number of clusters does not coincide with the number of random centroids!\"\n",
    "\n",
    "for centroid, discard_set in zip(k_centroids, discard_sets):\n",
    "    discard_set.summarize(np.array(centroid))\n",
    "\n",
    "def mahalanobis_distance(x: npt.NDArray[np.float32], s: SummarizedCluster) -> float:\n",
    "    return np.sqrt(np.sum(np.pow((x - s.centroid()) / s.standard_deviation(), 2)))\n",
    "\n",
    "@F.udf(returnType=FloatType())\n",
    "def mahalanobis_distance_column(*features: float):\n",
    "    x = np.array(features)\n",
    "    closest_cluster_distance, closest_cluster = min((mahalanobis_distance(x, d), d.id_) for d in discard_sets)\n",
    "    return closest_cluster if closest_cluster_distance < cluster_distance_threshold else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import DBSCAN\n",
    "\n",
    "features_columns = set(features_df.columns) - {\"track_id\"}\n",
    "split_weights = [1.0] * (total_rows // rows_per_iteration)\n",
    "dbscan_eps = 0.5    # TODO: parameter?\n",
    "# TODO: choose threshold and justify decision\n",
    "compression_set_merge_variance_threshold = float('-inf')\n",
    "\n",
    "for loaded_points_df in features_df.filter(~F.col(\"track_id\").isin(k_centroids_ids)).randomSplit(split_weights, seed=seed_random):\n",
    "    cluster_mapping = loaded_points_df \\\n",
    "        .withColumn(\"cluster\", mahalanobis_distance_column(*features_columns)) \\\n",
    "        .groupby(\"cluster\") \\\n",
    "        .agg({\n",
    "            \"track_ids\": F.collect_list(\"track_id\"),\n",
    "            \"features_list\": F.collect_list(F.array(*features_columns))\n",
    "        }).collect()\n",
    "    \n",
    "    for row in cluster_mapping:\n",
    "        cluster = row[\"cluster\"]\n",
    "        track_ids = row[\"track_ids\"]\n",
    "        features_list = row[\"features_list\"]\n",
    "\n",
    "        # Step 3 - check which points go to the discard sets\n",
    "        if cluster is not None:\n",
    "            discard_set = discard_sets[cluster]\n",
    "            for track_id, features in zip(track_ids, features_list):\n",
    "                discard_set.summarize(np.array(features), track_id)\n",
    "        \n",
    "        else:\n",
    "            # Step 4 - check which points go to the compression sets or the retained set\n",
    "            matrix_to_cluster = np.array(features_list + retained_set)\n",
    "\n",
    "            # Use same distance as above\n",
    "            clusterer = DBSCAN(eps=dbscan_eps, metric='euclidean')\n",
    "            clusterer.fit(matrix_to_cluster)\n",
    "\n",
    "            centroid_calculator = NearestCentroid()\n",
    "            centroid_calculator.fit(matrix_to_cluster, clusterer.labels_)\n",
    "\n",
    "            retained_set.clear()\n",
    "\n",
    "            # Create compression sets\n",
    "            compression_sets_temp = [SummarizedCluster(dimensions, None) for _ in centroid_calculator.centroids_]\n",
    "\n",
    "            for point_idx in range(matrix_to_cluster.shape[0]):\n",
    "                cluster_id = clusterer.labels_[point_idx]\n",
    "                point = matrix_to_cluster[point_idx, :]\n",
    "                \n",
    "                if cluster_id == -1:\n",
    "                    retained_set.append(point)\n",
    "                else:\n",
    "                    compression_sets_temp[cluster_id].summarize(point)\n",
    "            \n",
    "            compression_sets.extend(compression_sets_temp)\n",
    "    \n",
    "    # Step 5 - merge compression sets\n",
    "    compressing = True\n",
    "    while compressing:\n",
    "        merged_compression_sets = []\n",
    "        compression_sets_to_remove = []\n",
    "        for (idx_1, compression_set_1), (idx_2, compression_set_2) in combinations(enumerate(compression_sets), 2):\n",
    "            merged_compression_set = compression_set_1 + compression_set_2\n",
    "            if merged_compression_set.variance() < compression_set_merge_variance_threshold:\n",
    "                merged_compression_sets.append(merged_compression_set)\n",
    "                compression_sets_to_remove\n",
    "\n",
    "# Step 6 - merge CS and RS into DS (but we won't)\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
