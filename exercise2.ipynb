{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType, DoubleType, IntegerType\n",
    "from itertools import combinations\n",
    "from typing import Iterable, Any, List, Set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.clustering import BisectingKMeans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/25 21:09:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('exercise2') \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Start with the small dataset first.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "    .option('header', 'false')\n",
    "    .option('sep', '\\t')\n",
    "    .csv('data/ml-100k/u.data')\n",
    ")\n",
    "\n",
    "df = (df.withColumnRenamed('_c0', 'user_id') \n",
    "    .withColumnRenamed('_c1', 'item_id')\n",
    "    .withColumnRenamed('_c2', 'rating')\n",
    "    .drop('_c3')\n",
    "    .select(F.col('user_id').cast(IntegerType()), F.col('item_id').cast(IntegerType()), F.col('rating').cast(IntegerType()))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_baseLine = {\n",
    "    row['user_id']:row['user_avg']\n",
    "    for row in df.groupBy('user_id')\n",
    "        .agg(F.avg('rating').alias('user_avg'))\n",
    "        .collect()\n",
    "}\n",
    "item_baseLine = {\n",
    "    row['item_id']:row['item_avg']\n",
    "    for row in df.groupBy('item_id')\n",
    "        .agg(F.avg('rating').alias('item_avg'))\n",
    "        .collect()\n",
    "}\n",
    "mu = df.agg(F.avg('rating')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_user_item_baseline(user_id: int, item_id: int) -> float:\n",
    "    user_avg = user_baseLine[user_id]\n",
    "    item_avg = item_baseLine[item_id]\n",
    "    return -mu + user_avg + item_avg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_number = df.select('user_id').distinct().count()\n",
    "item_number = df.select('item_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=ArrayType(IntegerType(),False))\n",
    "def utility_matrix_row(elems: Iterable[Any]):\n",
    "\n",
    "    lista = [0] * user_number\n",
    "    \n",
    "    for (user_id,rating) in elems:\n",
    "        lista[user_id-1] = rating\n",
    "\n",
    "    return lista"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "@F.udf(returnType=StructType([\n",
    "    StructField('items_in_cluster', ArrayType(IntegerType(), False)),\n",
    "    StructField('ratings_in_cluster', ArrayType(ArrayType(DoubleType(), False), False)),\n",
    "]))\n",
    "def find_most_similar(user_id: int, items_in_cluster: List[int], ratings_in_cluster: List[List[float]]):\n",
    "    return tuple(zip(*(\n",
    "        (item_id, ratings)\n",
    "        for item_id, ratings in zip(items_in_cluster, ratings_in_cluster)\n",
    "        if ratings[user_id-1] != 0\n",
    "    )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=DoubleType())\n",
    "def predict_rating_no_numpy(user_id: int, item_id: int, ratings: List[float], items_in_cluster: List[int], ratings_in_cluster: List[List[float]]) -> float:\n",
    "\n",
    "    # TODO: maybe remove item_id itself?\n",
    "\n",
    "    ratings_mean = sum(r for r in ratings if r != 0) / sum(1 for r in ratings if r != 0)\n",
    "    ratings_without_mean = [r - ratings_mean for r in ratings]\n",
    "\n",
    "    ratings_in_cluster_mean = [sum(r for r in ratings if r != 0) / sum(1 for r in ratings if r != 0) for ratings in ratings_in_cluster]  \n",
    "    ratings_in_cluster_without_mean = [[r - mean for r in ratings] for ratings, mean in zip(ratings_in_cluster, ratings_in_cluster_mean)]\n",
    "    \n",
    "    pearson_correlation_similarities = [\n",
    "        sum(r1 * r2 for r1, r2 in zip(ratings_without_mean, rs)) / (sum(r**2 for r in ratings_without_mean)**.5 * sum(r**2 for r in rs)**.5)\n",
    "        for rs in ratings_in_cluster_without_mean\n",
    "    ]\n",
    "\n",
    "    base_line = calculate_user_item_baseline(user_id, item_id)\n",
    "\n",
    "    other_ratings = [r[user_id-1] for r in ratings_in_cluster ]\n",
    "\n",
    "    other_baselines = [calculate_user_item_baseline(user_id, item) for item in items_in_cluster]\n",
    "\n",
    "    return base_line + sum(pcs * (ort - obl) for pcs, ort, obl in zip(pearson_correlation_similarities, other_ratings, other_baselines)) / sum(pearson_correlation_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=DoubleType())\n",
    "def predict_rating(user_id: int, item_id: int, ratings: List[float], items_in_cluster: List[int], ratings_in_cluster: List[List[float]]) -> float:\n",
    "\n",
    "    # TODO: maybe remove item_id itself?\n",
    "\n",
    "    # TODO: with numpy or not?\n",
    "    ratings_np = np.array(ratings).reshape((1, -1))\n",
    "    ratings_in_cluster_np = np.array(ratings_in_cluster)\n",
    "\n",
    "    ratings_np_pc = ratings_np - np.average(ratings_np[ratings_np.nonzero()])\n",
    "    non_zero = np.zeros(ratings_in_cluster_np.shape)\n",
    "    non_zero[ratings_in_cluster_np.nonzero()] = 1\n",
    "    ratings_in_cluster_np_pc = ratings_in_cluster_np - np.average(ratings_in_cluster_np, axis=1, weights=non_zero, keepdims=True)\n",
    "\n",
    "    pearson_correlation_similarities = np.sum(ratings_np_pc * ratings_in_cluster_np_pc, axis=1) / (np.sqrt(np.sum(ratings_in_cluster_np_pc**2, axis=1)) * np.sqrt(np.sum(ratings_np_pc**2, axis=1)))\n",
    "\n",
    "    base_line = calculate_user_item_baseline(user_id, item_id)\n",
    "\n",
    "    other_ratings = ratings_in_cluster_np[:, user_id-1]\n",
    "\n",
    "    other_baselines = np.array([calculate_user_item_baseline(user_id, item) for item in items_in_cluster])\n",
    "\n",
    "    return base_line + float(np.sum(pearson_correlation_similarities * (other_ratings - other_baselines)) / np.sum(pearson_correlation_similarities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave out 10% of the ratings for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_random = 1\n",
    "# TODO: maybe instead add random column and filter by that? the randomSplit seems too contiguous\n",
    "train_set, test_set = df.randomSplit([9.0, 1.0], seed=seed_random)\n",
    "\n",
    "train_matrix = (train_set.groupBy('item_id')\n",
    "    .agg(F.collect_list(F.array('user_id','rating')).alias('ratings'))\n",
    "    .withColumn('ratings', utility_matrix_row(F.col('ratings')).cast(ArrayType(DoubleType(),False)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(progress: float, message: str):\n",
    "    print(f\"[{progress:3%}] {message:100}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/25 21:09:35 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/04/25 21:09:35 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "neighbours = 10\n",
    "bkm = BisectingKMeans(featuresCol='ratings',minDivisibleClusterSize = neighbours,predictionCol = \"cluster_id\").setK(10).setSeed(1)\n",
    "model = bkm.fit(train_matrix)\n",
    "\n",
    "train_matrix_with_clusters = model.transform(train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: show how many of the items in the test set could not be predicted because they were not in the train set\n",
    "predictions = (test_set\n",
    "    .withColumnRenamed('rating', 'true_rating')\n",
    "    .join(train_matrix_with_clusters.select('item_id', 'ratings', 'cluster_id'), on='item_id', how='inner') # obtain ratings and cluster_id of item_id\n",
    "    .join(\n",
    "        train_matrix_with_clusters.groupBy('cluster_id').agg(F.collect_list('item_id').alias('items_in_cluster'), F.collect_list('ratings').alias('ratings_in_cluster')),\n",
    "        on='cluster_id',\n",
    "        how='inner'\n",
    "    )\n",
    "    .withColumn('find_most_similar_results', find_most_similar('user_id', 'items_in_cluster', 'ratings_in_cluster'))\n",
    "    .withColumns({\n",
    "        'items_in_cluster': F.col('find_most_similar_results').items_in_cluster,\n",
    "        'ratings_in_cluster': F.col('find_most_similar_results').ratings_in_cluster,\n",
    "    })\n",
    "    .drop('find_most_similar_results')\n",
    "    .withColumn('predicted_rating', predict_rating_no_numpy('user_id', 'item_id', 'ratings', 'items_in_cluster', 'ratings_in_cluster'))\n",
    "    .select('user_id', 'item_id', 'true_rating', 'predicted_rating')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                  (0 + 1) / 1]\n",
      "  File \"/home/pedro/Desktop/MEI-4ano/2semestre/MDLE/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 187, in manager\n",
      "  File \"/home/pedro/Desktop/MEI-4ano/2semestre/MDLE/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/pedro/Desktop/MEI-4ano/2semestre/MDLE/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 730, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/home/pedro/Desktop/MEI-4ano/2semestre/MDLE/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/25 21:10:07 ERROR Utils: Uncaught exception in thread stdout writer for /usr/bin/python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat java.base/java.io.OutputStream.write(OutputStream.java:122)\n",
      "\tat net.razorvine.pickle.Pickler.put_float(Pickler.java:707)\n",
      "\tat net.razorvine.pickle.Pickler.dispatch(Pickler.java:286)\n",
      "\tat net.razorvine.pickle.Pickler.save(Pickler.java:185)\n",
      "\tat net.razorvine.pickle.Pickler.put_collection(Pickler.java:407)\n",
      "\tat net.razorvine.pickle.Pickler.dispatch(Pickler.java:363)\n",
      "\tat net.razorvine.pickle.Pickler.save(Pickler.java:185)\n",
      "\tat net.razorvine.pickle.Pickler.put_collection(Pickler.java:407)\n",
      "\tat net.razorvine.pickle.Pickler.dispatch(Pickler.java:363)\n",
      "\tat net.razorvine.pickle.Pickler.save(Pickler.java:185)\n",
      "\tat net.razorvine.pickle.Pickler.put_arrayOfObjects(Pickler.java:578)\n",
      "\tat net.razorvine.pickle.Pickler.dispatch(Pickler.java:254)\n",
      "\tat net.razorvine.pickle.Pickler.save(Pickler.java:185)\n",
      "\tat net.razorvine.pickle.Pickler.put_arrayOfObjects(Pickler.java:585)\n",
      "\tat net.razorvine.pickle.Pickler.dispatch(Pickler.java:254)\n",
      "\tat net.razorvine.pickle.Pickler.save(Pickler.java:185)\n",
      "\tat net.razorvine.pickle.Pickler.dump(Pickler.java:155)\n",
      "\tat net.razorvine.pickle.Pickler.dumps(Pickler.java:140)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$4(BatchEvalPythonExec.scala:77)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$3773/0x0000000841560040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$Lambda$3797/0x0000000841569840.apply(Unknown Source)\n",
      "+-------+-------+-----------+------------------+\n",
      "|user_id|item_id|true_rating|  predicted_rating|\n",
      "+-------+-------+-----------+------------------+\n",
      "|    935|      1|          3| 3.733790096369709|\n",
      "|    924|      1|          5| 3.469093520930787|\n",
      "|    885|      1|          5|3.6338398943373003|\n",
      "|    870|      1|          5|3.9995335420862634|\n",
      "|    843|      1|          3| 3.229725198128368|\n",
      "|    829|      1|          4| 3.608088099983241|\n",
      "|    763|      1|          4| 4.047094647941229|\n",
      "|    756|      1|          4|3.4786351909243605|\n",
      "|    714|      1|          3|  3.85311984983874|\n",
      "|    682|      1|          4| 3.947155994825667|\n",
      "|    680|      1|          4| 4.018539687485053|\n",
      "|    622|      1|          3|4.1530411891126775|\n",
      "|    561|      1|          2| 3.232068612114822|\n",
      "|    517|      1|          3| 3.654648947115515|\n",
      "|    512|      1|          4| 3.825404684361647|\n",
      "|    465|      1|          4| 3.873051256190118|\n",
      "|    419|      1|          4|4.1530774267383865|\n",
      "|    407|      1|          4| 3.794121418810268|\n",
      "|    344|      1|          3|3.8240908396667077|\n",
      "|    332|      1|          4| 4.729288415868949|\n",
      "+-------+-------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"stdout writer for /usr/bin/python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:120)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:95)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:156)\n",
      "\tat java.base/java.io.OutputStream.write(OutputStream.java:122)\n",
      "\tat net.razorvine.pickle.Pickler.put_float(Pickler.java:707)\n",
      "\tat net.razorvine.pickle.Pickler.dispatch(Pickler.java:286)\n",
      "\tat net.razorvine.pickle.Pickler.save(Pickler.java:185)\n",
      "\tat net.razorvine.pickle.Pickler.put_collection(Pickler.java:407)\n",
      "\tat net.razorvine.pickle.Pickler.dispatch(Pickler.java:363)\n",
      "\tat net.razorvine.pickle.Pickler.save(Pickler.java:185)\n",
      "\tat net.razorvine.pickle.Pickler.put_collection(Pickler.java:407)\n",
      "\tat net.razorvine.pickle.Pickler.dispatch(Pickler.java:363)\n",
      "\tat net.razorvine.pickle.Pickler.save(Pickler.java:185)\n",
      "\tat net.razorvine.pickle.Pickler.put_arrayOfObjects(Pickler.java:578)\n",
      "\tat net.razorvine.pickle.Pickler.dispatch(Pickler.java:254)\n",
      "\tat net.razorvine.pickle.Pickler.save(Pickler.java:185)\n",
      "\tat net.razorvine.pickle.Pickler.put_arrayOfObjects(Pickler.java:585)\n",
      "\tat net.razorvine.pickle.Pickler.dispatch(Pickler.java:254)\n",
      "\tat net.razorvine.pickle.Pickler.save(Pickler.java:185)\n",
      "\tat net.razorvine.pickle.Pickler.dump(Pickler.java:155)\n",
      "\tat net.razorvine.pickle.Pickler.dumps(Pickler.java:140)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$4(BatchEvalPythonExec.scala:77)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$3773/0x0000000841560040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$Lambda$3797/0x0000000841569840.apply(Unknown Source)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use evaluation metric on the 10% (precision at 10 / rank correlation?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 294:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/25 21:10:11 ERROR Utils: Uncaught exception in thread stdout writer for /usr/bin/python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.toByteArray(ByteArrayOutputStream.java:211)\n",
      "\tat net.razorvine.pickle.Pickler.dumps(Pickler.java:142)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$4(BatchEvalPythonExec.scala:77)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$3773/0x0000000841560040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$Lambda$3797/0x0000000841569840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"stdout writer for /usr/bin/python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n",
      "\tat java.base/java.io.ByteArrayOutputStream.toByteArray(ByteArrayOutputStream.java:211)\n",
      "\tat net.razorvine.pickle.Pickler.dumps(Pickler.java:142)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$4(BatchEvalPythonExec.scala:77)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$Lambda$3773/0x0000000841560040.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:431)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$Lambda$3797/0x0000000841569840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:265)\n"
     ]
    }
   ],
   "source": [
    "(predictions\n",
    "    .withColumn('square_error', (F.col('true_rating') - F.col('predicted_rating'))**2)\n",
    "    .select('square_error')\n",
    "    .groupBy()\n",
    "    .sum('square_error')\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
