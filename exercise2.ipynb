{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import Broadcast\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StringType, ArrayType, FloatType, DoubleType, IntegerType\n",
    "from itertools import combinations\n",
    "from typing import Iterable, Any, List, Set, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.clustering import BisectingKMeans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/26 16:23:20 WARN Utils: Your hostname, martinho-MS-7B86 resolves to a loopback address: 127.0.1.1; using 192.168.1.67 instead (on interface enp34s0)\n",
      "23/04/26 16:23:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/26 16:23:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('exercise2') \\\n",
    "    .config('spark.master', 'local[*]') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Start with the small dataset first.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "    .option('header', 'false')\n",
    "    .option('sep', '\\t')\n",
    "    .csv('data/ml-100k/u.data')\n",
    ")\n",
    "\n",
    "df = (df.withColumnRenamed('_c0', 'user_id') \n",
    "    .withColumnRenamed('_c1', 'item_id')\n",
    "    .withColumnRenamed('_c2', 'rating')\n",
    "    .drop('_c3')\n",
    "    .select(F.col('user_id').cast(IntegerType()), F.col('item_id').cast(IntegerType()), F.col('rating').cast(IntegerType()))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate baselines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicitly broadcast the `user_baseLine` and `item_baseLine` variables beforehand, as they can take up some memory and will be used for prediciting ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_user_item_means(df: DataFrame) -> Tuple[Broadcast, Broadcast, float]:\n",
    "    user_means = {\n",
    "        row['user_id']:row['user_avg']\n",
    "        for row in df.groupBy('user_id')\n",
    "            .agg(F.avg('rating').alias('user_avg'))\n",
    "            .collect()\n",
    "    }\n",
    "    item_means = {\n",
    "        row['item_id']:row['item_avg']\n",
    "        for row in df.groupBy('item_id')\n",
    "            .agg(F.avg('rating').alias('item_avg'))\n",
    "            .collect()\n",
    "    }\n",
    "    mu = df.agg(F.avg('rating')).collect()[0][0]\n",
    "\n",
    "    user_means = spark.sparkContext.broadcast(user_means)\n",
    "    item_means = spark.sparkContext.broadcast(item_means)\n",
    "\n",
    "    return user_means, item_means, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_item_baseline(user_id: int, item_id: int, user_means, item_means, mu) -> float:\n",
    "    user_avg = user_means[user_id]\n",
    "    item_avg = item_means[item_id]\n",
    "    return -mu + user_avg + item_avg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_number = df.select('user_id').distinct().count()\n",
    "item_number = df.select('item_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=ArrayType(IntegerType(),False))\n",
    "def utility_matrix_row(elems: Iterable[Any]):\n",
    "\n",
    "    lista = [0] * user_number\n",
    "    \n",
    "    for (user_id,rating) in elems:\n",
    "        lista[user_id-1] = rating\n",
    "\n",
    "    return lista"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "\n",
    "@F.udf(returnType=StructType([\n",
    "    StructField('items_in_cluster', ArrayType(IntegerType(), False)),\n",
    "    StructField('ratings_in_cluster', ArrayType(ArrayType(DoubleType(), False), False)),\n",
    "]))\n",
    "def find_most_similar(user_id: int, items_in_cluster: List[int], ratings_in_cluster: List[List[float]]):\n",
    "    return tuple(zip(*(\n",
    "        (item_id, ratings)\n",
    "        for item_id, ratings in zip(items_in_cluster, ratings_in_cluster)\n",
    "        if ratings[user_id-1] != 0\n",
    "    )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @F.udf(returnType=DoubleType())\n",
    "# def predict_rating_no_numpy(user_id: int, item_id: int, ratings: List[float], items_in_cluster: List[int], ratings_in_cluster: List[List[float]]) -> float:\n",
    "\n",
    "#     # TODO: maybe remove item_id itself?\n",
    "\n",
    "#     ratings_mean = sum(r for r in ratings if r != 0) / sum(1 for r in ratings if r != 0)\n",
    "#     ratings_without_mean = [r - ratings_mean for r in ratings]\n",
    "\n",
    "#     ratings_in_cluster_mean = [sum(r for r in ratings if r != 0) / sum(1 for r in ratings if r != 0) for ratings in ratings_in_cluster]  \n",
    "#     ratings_in_cluster_without_mean = [[r - mean for r in ratings] for ratings, mean in zip(ratings_in_cluster, ratings_in_cluster_mean)]\n",
    "    \n",
    "#     pearson_correlation_similarities = [\n",
    "#         sum(r1 * r2 for r1, r2 in zip(ratings_without_mean, rs)) / (sum(r**2 for r in ratings_without_mean)**.5 * sum(r**2 for r in rs)**.5)\n",
    "#         for rs in ratings_in_cluster_without_mean\n",
    "#     ]\n",
    "\n",
    "#     base_line = get_user_item_baseline(user_id, item_id)\n",
    "\n",
    "#     other_ratings = [r[user_id-1] for r in ratings_in_cluster]\n",
    "\n",
    "#     other_baselines = [get_user_item_baseline(user_id, item) for item in items_in_cluster]\n",
    "\n",
    "#     return base_line + sum(pcs * (ort - obl) for pcs, ort, obl in zip(pearson_correlation_similarities, other_ratings, other_baselines)) / sum(pearson_correlation_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @F.udf(returnType=DoubleType())\n",
    "# def predict_rating(user_id: int, item_id: int, ratings: List[float], items_in_cluster: List[int], ratings_in_cluster: List[List[float]]) -> float:\n",
    "\n",
    "#     # TODO: maybe remove item_id itself?\n",
    "\n",
    "#     # TODO: use a pandas UDF instead, since it can be vectorized as well?\n",
    "#     # TODO: with numpy or not?\n",
    "#     ratings_np = np.array(ratings).reshape((1, -1))\n",
    "#     ratings_in_cluster_np = np.array(ratings_in_cluster)\n",
    "\n",
    "#     ratings_np_pc = ratings_np - np.average(ratings_np[ratings_np.nonzero()])\n",
    "#     non_zero = np.zeros(ratings_in_cluster_np.shape)\n",
    "#     non_zero[ratings_in_cluster_np.nonzero()] = 1\n",
    "#     ratings_in_cluster_np_pc = ratings_in_cluster_np - np.average(ratings_in_cluster_np, axis=1, weights=non_zero, keepdims=True)\n",
    "\n",
    "#     pearson_correlation_similarities = np.sum(ratings_np_pc * ratings_in_cluster_np_pc, axis=1) / (np.sqrt(np.sum(ratings_in_cluster_np_pc**2, axis=1)) * np.sqrt(np.sum(ratings_np_pc**2, axis=1)))\n",
    "\n",
    "#     base_line = get_user_item_baseline(user_id, item_id)\n",
    "\n",
    "#     other_ratings = ratings_in_cluster_np[:, user_id-1]\n",
    "\n",
    "#     other_baselines = np.array([get_user_item_baseline(user_id, item) for item in items_in_cluster])\n",
    "\n",
    "#     return base_line + float(np.sum(pearson_correlation_similarities * (other_ratings - other_baselines)) / np.sum(pearson_correlation_similarities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave out 10% of the ratings for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_random = 1\n",
    "# TODO: maybe instead add random column and filter by that? the randomSplit seems too contiguous\n",
    "train_set, test_set = df.randomSplit([9.0, 1.0], seed=seed_random)\n",
    "\n",
    "train_matrix = (train_set.groupBy('item_id')\n",
    "    .agg(F.collect_list(F.array('user_id','rating')).alias('ratings'))\n",
    "    .withColumn('ratings', utility_matrix_row(F.col('ratings')).cast(ArrayType(DoubleType(),False)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_means, item_means, mu = calculate_user_item_means(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(progress: float, message: str):\n",
    "    print(f\"[{progress:3%}] {message:100}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "neighbours = 10\n",
    "bkm = BisectingKMeans(featuresCol='ratings',minDivisibleClusterSize = neighbours,predictionCol = \"cluster_id\").setK(50).setSeed(1)\n",
    "model = bkm.fit(train_matrix)\n",
    "\n",
    "train_matrix_with_clusters = model.transform(train_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cluster info.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_matrix_with_clusters.select('item_id', 'cluster_id').distinct().groupBy('cluster_id').sum('item_id').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (test_set\n",
    "    .withColumnRenamed('rating', 'true_rating')\n",
    "    .join(train_matrix_with_clusters, on='item_id', how='inner') # get cluster and ratings of test item\n",
    "    # these should be after the join so that we remove items and users that were not trained on (never seen before)\n",
    "    .withColumn('item_mean', F.udf(lambda x: item_means.value[x], returnType=DoubleType())('item_id'))\n",
    "    .withColumn('user_mean', F.udf(lambda x: user_means.value[x], returnType=DoubleType())('user_id'))\n",
    "    .withColumn('ratings_pearson', F.transform('ratings', lambda x: x - F.col('item_mean')))\n",
    "    .join(train_matrix_with_clusters\n",
    "        .withColumnsRenamed({\n",
    "            'item_id': 'other_item_id',\n",
    "            'ratings': 'other_ratings',\n",
    "        })\n",
    "        .withColumn('other_item_mean', F.udf(lambda x: item_means.value[x], returnType=DoubleType())('other_item_id'))\n",
    "        .withColumn('other_ratings_pearson', F.transform('other_ratings', lambda x: x - F.col('other_item_mean'))),\n",
    "        on='cluster_id',\n",
    "        how='inner')\n",
    "    .withColumn('user_other_rating', F.col('other_ratings')[F.col('user_id') - 1])\n",
    "    .filter(F.col('user_other_rating') != 0) # should remove the test item since it never gave got a rating from the test user\n",
    "    .withColumn('similarity',\n",
    "        F.aggregate(\n",
    "            F.zip_with('ratings_pearson', 'other_ratings_pearson', lambda x1, x2: x1 * x2),\n",
    "            initialValue=F.lit(0.0),\n",
    "            merge=lambda acc, x: acc + x\n",
    "        )\n",
    "        /\n",
    "        (\n",
    "            F.sqrt(F.aggregate(\n",
    "                F.transform('ratings_pearson', lambda x: x**2),\n",
    "                initialValue=F.lit(0.0),\n",
    "                merge=lambda acc, x: acc + x\n",
    "            ))\n",
    "            *\n",
    "            F.sqrt(F.aggregate(\n",
    "                F.transform('other_ratings_pearson', lambda x: x**2),\n",
    "                initialValue=F.lit(0.0),\n",
    "                merge=lambda acc, x: acc + x\n",
    "            ))\n",
    "        )\n",
    "    )\n",
    "    .withColumn('baseline', -mu + F.col('user_mean') + F.col('item_mean')) # b_xi\n",
    "    .withColumn('other_baseline', -mu + F.col('user_mean') + F.col('other_item_mean')) # b_xj\n",
    "    .groupBy('user_id', 'item_id', 'baseline', 'true_rating')\n",
    "    .agg((F.col('baseline') + F.sum(F.col('similarity') * (F.col('user_other_rating') - F.col('other_baseline'))) / F.sum('similarity')).alias('predicted_rating'))\n",
    "    .drop('baseline')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 611:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+------------------+\n",
      "|user_id|item_id|true_rating|  predicted_rating|\n",
      "+-------+-------+-----------+------------------+\n",
      "|    218|      5|          3| 2.914431715167286|\n",
      "|    804|     33|          4|3.8720220265457024|\n",
      "|    352|     39|          5|2.1025641025641026|\n",
      "|    354|     60|          5| 4.175480342029669|\n",
      "|    922|     67|          3| 3.221525184744525|\n",
      "|    577|    100|          4| 4.730644829477191|\n",
      "|    479|    154|          3|3.9223404709660836|\n",
      "|    201|    185|          5|3.5395898775773946|\n",
      "|    650|    193|          3| 3.819501276798148|\n",
      "|    532|    210|          5| 4.783328543273073|\n",
      "|    778|    226|          4|2.9201318303701083|\n",
      "|     92|    226|          3|3.1599671967590885|\n",
      "|    332|    255|          4|  4.38444386904236|\n",
      "|      6|    259|          1| 2.213027091654288|\n",
      "|    345|    272|          5| 4.145343824396781|\n",
      "|    385|    273|          2|3.1033294281552393|\n",
      "|    110|    288|          4|3.6212777142150343|\n",
      "|    544|    312|          2| 2.016117216117216|\n",
      "|    416|    323|          3|2.8899115678749565|\n",
      "|    253|    328|          4| 3.815949448088306|\n",
      "+-------+-------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: show how many of the items in the test set could not be predicted because they were not in the train set\n",
    "# TODO: only use users that were in the train set\n",
    "# predictions = (test_set\n",
    "#     .withColumnRenamed('rating', 'true_rating')\n",
    "#     .join(train_matrix_with_clusters.select('item_id', 'ratings', 'cluster_id'), on='item_id', how='inner') # obtain ratings and cluster_id of item_id\n",
    "#     .join(\n",
    "#         train_matrix_with_clusters.groupBy('cluster_id').agg(F.collect_list('item_id').alias('items_in_cluster'), F.collect_list('ratings').alias('ratings_in_cluster')),\n",
    "#         on='cluster_id',\n",
    "#         how='inner'\n",
    "#     )\n",
    "#     .withColumn('find_most_similar_results', find_most_similar('user_id', 'items_in_cluster', 'ratings_in_cluster'))\n",
    "#     .withColumns({\n",
    "#         'items_in_cluster': F.col('find_most_similar_results').items_in_cluster,\n",
    "#         'ratings_in_cluster': F.col('find_most_similar_results').ratings_in_cluster,\n",
    "#     })\n",
    "#     .drop('find_most_similar_results')\n",
    "#     .withColumn('predicted_rating', predict_rating_no_numpy('user_id', 'item_id', 'ratings', 'items_in_cluster', 'ratings_in_cluster'))\n",
    "#     .select('user_id', 'item_id', 'true_rating', 'predicted_rating')\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use evaluation metric on the 10% (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tested = test_set.join(train_matrix_with_clusters, on='item_id', how='inner').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rmse = ((predictions\n",
    "    .withColumn('square_error', (F.col('true_rating') - F.col('predicted_rating'))**2)\n",
    "    .select('square_error')\n",
    "    .groupBy()\n",
    "    .sum('square_error')\n",
    "    .collect()[0]['sum(square_error)']\n",
    "    ) / (total_tested)) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9710793061099181\n"
     ]
    }
   ],
   "source": [
    "print('RMSE:', rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
